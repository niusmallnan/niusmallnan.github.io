<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Niusmallnan</title>
    <link>http://niusmallnan.com/categories/coding/index.xml</link>
    <description>Recent content on Niusmallnan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <copyright>Niusmallnan</copyright>
    <atom:link href="http://niusmallnan.com/categories/coding/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>如何在RancherOS上更新microcode</title>
      <link>http://niusmallnan.com/2018/01/22/update-microcode-on-rancheros</link>
      <pubDate>Mon, 22 Jan 2018 17:16:17 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2018/01/22/update-microcode-on-rancheros</guid>
      <description>&lt;p&gt;RancherOS上更新microcode
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;进来公布Google的Project Zero团队公布的Spectre和Meltdown漏洞，真可谓一石激起千层浪，
业内各个硬件软件厂商开源社区都在积极参与漏洞修复。Meltdown是最早被修复的，在Kernel中开启KPTI就可以缓解该问题，
Spectre则依然在火热进行中。而作为漏洞的始作俑者Intel，Spectre漏洞的Intel官方缓解方案也于近日放出。
当然目前很多更新并非真正的修复，而是在最大程度抵御和缓解相关攻击。&lt;/p&gt;

&lt;h3 id=&#34;更新microcode&#34;&gt;更新microcode&lt;/h3&gt;

&lt;p&gt;通过更新microcode和系统补丁可以缓解Spectre Var. 2漏洞，也就是CVE-2017-5715 branch target injection。
除了在BIOS中可以更新microcode之外，Kernel也提供了更新microcode的机制。本文只关注在如何在RancherOS上更新microcode。&lt;/p&gt;

&lt;p&gt;下载最新的Intel microcode版本，注意microcode一般并不是适用所有的CPU，比如&lt;a href=&#34;https://downloadcenter.intel.com/download/27431/Linux-Processor-Microcode-Data-File&#34;&gt;20180108&lt;/a&gt;这个版本，主要是以下CPU才会起作用：&lt;br /&gt;
&lt;img src=&#34;https://ws1.sinaimg.cn/large/006tNc79ly1fnpsvdzc3rj30wu1byn1l.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;下载并解压缩之后，你会发现一个是intel-ucode目录下一些文件，另外一个单独的microcode.dat文件。
前者是支持热加载方式，也是现在比较推荐的方式；后者是传统更新方式，需要在initrd中加入microcode加载。
对于RancherOS，前者比较合适，使用起来相对简单。&lt;/p&gt;

&lt;p&gt;若要在内核支持更新microcode，需要在编译内核时加入以下配置（RancherOS已经开启）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CONFIG_MICROCODE=y
CONFIG_MICROCODE_INTEL=y
CONFIG_MICROCODE_AMD=y
CONFIG_MICROCODE_OLD_INTERFACE=y #支持传统方式，开启此项
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装过程比较简单，几乎每个版本的microcode都有相应的releasnote，大致如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Make sure /sys/devices/system/cpu/microcode/reload exits:
$ ls -l /sys/devices/system/cpu/microcode/reload

# You must copy all files from intel-ucode to /lib/firmware/intel-ucode/ using the cp command
$ sudo cp -v intel-ucode/* /lib/firmware/intel-ucode/

# You just copied intel-ucode directory to /lib/firmware/. Write the reload interface to 1 to reload the microcode files:
$ echo 1 &amp;gt; /sys/devices/system/cpu/microcode/reload
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;无论更新成功与否，在dmesg中都会查看到相关信息，比如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ dmesg | grep microcode
[   13.659429] microcode: sig=0x306f2, pf=0x1, revision=0x36
[   13.665981] microcode: Microcode Update Driver: v2.01 &amp;lt;tigran@aivazian.fsnet.co.uk&amp;gt;, Peter Oruba
[  510.899733] microcode: updated to revision 0x3b, date = 2017-11-17  # 这条msg很重要
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看cpuinfo，再次确认microcode版本，不同的CPU型号，升级后对应的microcode版本是不同的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat /proc/cpuinfo |grep &amp;quot;model\|microcode\|stepping\|family&amp;quot; |head -n 5
cpu family   : 6
model        : 63
model name   : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping     : 2
microcode    : 0x3b #之前是0x36
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后在cloud-config中在runcmd添加脚本，保证每次启动都加载最新版本的microcode：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;runcmd:
- echo 1 &amp;gt; /sys/devices/system/cpu/microcode/reload
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;关于Spectre Var. 2，我们依然在持续关注，
直接在内核编译中使用Retpoline指令替换技术，可以更简单方便的缓解branch target injection，
现在内核已经支持了Retpoline指令替换的设置，但是也需要最新版本的GCC编译器的特性支持，
而带有GCC的新补丁的正式版本还没有发布，一旦GCC新版本发布，我们会马上更新内核并发布新的RancherOS版本。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>RancherOS上运行RKE</title>
      <link>http://niusmallnan.com/2018/01/21/rke-on-rancheros</link>
      <pubDate>Sun, 21 Jan 2018 16:23:54 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2018/01/21/rke-on-rancheros</guid>
      <description>&lt;p&gt;RancherOS上运行RKE的注意事项
&lt;/p&gt;

&lt;h3 id=&#34;注意事项&#34;&gt;注意事项&lt;/h3&gt;

&lt;p&gt;RancherOS是一个非常精简的操作系统，功能上肯定不如Ubuntu/CentOS之流全面，所以在其上面使用RKE部署Kubernetes时候需要注意一些问题。&lt;/p&gt;

&lt;h4 id=&#34;切换到合适的docker-engine上&#34;&gt;切换到合适的docker engine上&lt;/h4&gt;

&lt;p&gt;一般来说K8s并不会适配所有docker engine，但是RancherOS提供docker engine自由切换的功能，可以先列出RancherOS的支持的docker engine版本:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@ip-172-31-5-104 rancher]# ros engine list
disabled docker-1.12.6
disabled docker-1.13.1
disabled docker-17.03.1-ce
current  docker-17.03.2-ce
disabled docker-17.04.0-ce
disabled docker-17.05.0-ce
disabled docker-17.06.1-ce
disabled docker-17.06.2-ce
disabled docker-17.09.1-ce
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据RKE的文档，选择合适的docker engine，切换docker engine很简单，使用&lt;code&gt;ros engine switch xxx&lt;/code&gt;即可。&lt;/p&gt;

&lt;h4 id=&#34;持久化相关目录&#34;&gt;持久化相关目录&lt;/h4&gt;

&lt;p&gt;RancherOS的默认console中只有部分目录是持久化的，这意味着你重启主机后，非持久化目录的数据会自动清理。
K8s会有一些数据需要持久化的磁盘上，所以针对K8s这些持久化目录，我们需要提前给RancherOS配置额外的持久化目录。&lt;/p&gt;

&lt;p&gt;RancherOS中目录的定义，请参考：&lt;a href=&#34;http://rancher.com/docs/os/v1.1/en/system-services/system-docker-volumes/&#34;&gt;http://rancher.com/docs/os/v1.1/en/system-services/system-docker-volumes/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;RancherOS默认包括三个持久化目录(user-volumes)，&lt;code&gt;/home/&lt;/code&gt; &lt;code&gt;/opt/&lt;/code&gt; &lt;code&gt;/var/lib/kubelet&lt;/code&gt;，如果要添加其他目录，可以使用下面的方式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 增加了 /etc/kubernetes 和 /etc/cni
ros config set rancher.services.user-volumes.volumes  [/home:/home,/opt:/opt,/var/lib/kubelet:/var/lib/kubelet,/etc/kubernetes:/etc/kubernetes,/etc/cni:/etc/cni]

system-docker rm all-volumes

reboot
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;具体那些目录需要持久化，还要看自己的部署需求，相关目录请参考：&lt;a href=&#34;https://github.com/rancher/rke#cluster-remove&#34;&gt;https://github.com/rancher/rke#cluster-remove&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>在阿里云上运行RancherOS</title>
      <link>http://niusmallnan.com/2017/12/05/rancheros-on-aliyun</link>
      <pubDate>Tue, 05 Dec 2017 16:32:52 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2017/12/05/rancheros-on-aliyun</guid>
      <description>&lt;p&gt;阿里云上也可以玩转RancherOS
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;RancherLabs创立之初就立志于做一家纯粹的云产品公司，秉承一切开源的理念，为用户带来方便快捷的体验。我们推出的Rancher深受全球用户的喜爱，
为了构建完整的用户体验丰富我们的产品线，我们考虑把传统的操作系统也引入容器的体验，于是我们大概在2015年正式推出了&lt;a href=&#34;https://github.com/rancher/os&#34;&gt;RancherOS&lt;/a&gt;这个项目，
该项目一经推出凭借其极具创新性的理念，荣登2015年Github10大开源项目，我们致力于打造轻量可靠，一切皆容器的操作系统。今年早些时候Docker发布的Linuxkit其实也从侧面印证了，
我们选择的这条路是顺应发展趋势的，只不过RancherOS先行了一步。&lt;/p&gt;

&lt;p&gt;现在我们在各个公有云平台上给用户免费使用，国外的厂商DigitalOcean甚至把RancherOS放到了Container distributions的UI选项中，越来越多的用户开始使用RancherOS，
参与到RancherOS的社区建设中，给我们反馈问题，协助我们不断改善产品。
而非常遗憾的是，中国用户始终无法在公有云中方便地使用它，其中原因诸多，国内的公有云也是这一两年才真正发展起来，
最初我们尝试在国内引入RancherOS时，很多平台无法提供有效的途径。&lt;/p&gt;

&lt;p&gt;现在，一切已经开始走向成熟，这次可以很开心得向大家宣布，RancherOS登陆阿里云，欢迎大家使用。
作为开源软件，不断积累改进是我们的目标，文章最后给大家提供了各种反馈使用问题的渠道。&lt;/p&gt;

&lt;h3 id=&#34;如何使用&#34;&gt;如何使用&lt;/h3&gt;

&lt;p&gt;首先声明一下，作为支持阿里云的首个版本仍然有很多不成熟之处，还有一些使用限制，如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;需使用阿里云VPC网络&lt;/li&gt;
&lt;li&gt;创建VM时，只支持密钥，不支持设置密码&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;首先下载RancherOS专属阿里云镜像，比如：&lt;a href=&#34;https://releases.rancher.com/os/latest/rancheros-aliyun.vhd&#34;&gt;https://releases.rancher.com/os/latest/rancheros-aliyun.vhd&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;将镜像上传到阿里云对象存储中，因为自定义镜像是需要从对象存储中获取。&lt;/p&gt;

&lt;p&gt;创建自定义镜像，按如下方式填写：&lt;br /&gt;
&lt;img src=&#34;https://ws4.sinaimg.cn/large/006tKfTcly1fmm2l7gfbhj312a0rewfl.jpg&#34; alt=&#34;&#34; /&gt;
注意事项包括：系统盘大小大于10GB，系统架构为x86_64，系统平台为Others Linux，镜像格式为VHD。&lt;/p&gt;

&lt;p&gt;创建VM时，按以下方式选择镜像：&lt;br /&gt;
&lt;img src=&#34;https://ws1.sinaimg.cn/large/006tKfTcly1fmwjd489e6j30wi09odg1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;创建成功后，使用ssh，访问用户是rancher，当然不要忘记指定密钥：&lt;br /&gt;
&lt;img src=&#34;https://ws2.sinaimg.cn/large/006tKfTcly1fm77ckj7ywj31ek0aq3z5.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
&lt;img src=&#34;https://ws4.sinaimg.cn/large/006tKfTcly1fm76t2hvs7j316u12641q.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;由于现在ROS镜像还没有发布到阿里云镜像市场中，当你创建完自定义镜像后，你可以按照如下方式共享给你的伙伴使用：&lt;br /&gt;
&lt;img src=&#34;https://ws1.sinaimg.cn/large/006tKfTcly1fm76xwulxxj30vy0jw74v.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;反馈渠道&#34;&gt;反馈渠道&lt;/h3&gt;

&lt;p&gt;各种问题欢迎随时提交issue:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;这里建议使用英文 &lt;a href=&#34;https://github.com/rancher/os/issues&#34;&gt;https://github.com/rancher/os/issues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;中文issue可以提交到Maintainer的个人Repo上: &lt;a href=&#34;https://github.com/niusmallnan/os/issues&#34;&gt;https://github.com/niusmallnan/os/issues&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;同时也欢迎大家发送邮件(niusmallnan@gmail.com)给开发者。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Rancher下实现Calico/Flat/Macvlan网络</title>
      <link>http://niusmallnan.com/2017/05/16/rancher-extend-network</link>
      <pubDate>Tue, 16 May 2017 16:31:50 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2017/05/16/rancher-extend-network</guid>
      <description>&lt;p&gt;Rancher下的高性能网络方案
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;目前Rancher的开源版本中提供了IPSec和VXLAN两种网络方案：IPSec更多面向的是安全加密的场景，
但是网络性能就会差很多；Vxlan与IPSec同属Overlay模型，但是内核层面的支持和没有加密环节的损耗，
相对来说Vxlan的性能要远高于IPSec，在AWS上测试的Vxlan带宽性能是逼近于两台VM之间带宽，
当然由于AWS本身的网络质量就很好，用户在自己的环境测试时并不会有这么好的性能。
当面向私有数据中心部署时，Overlay的模型并不一定满足用户的需求，而且当前容器网络发展非常迅猛，
出现了很多独树一帜的网络插件，所以这里我选择了三种插件Calico、Flat、Macvlan对接Rancher，
希望可以为更多用户提供帮助。&lt;/p&gt;

&lt;h3 id=&#34;rancher对接第三方网络插件&#34;&gt;Rancher对接第三方网络插件&lt;/h3&gt;

&lt;p&gt;Rancher在v1.2版本重新架构后，实际上已经为第三方网络插件的支持留下了空间。Rancher中的network-manager
组件是管理网络插件的核心，其管理原理本文不赘述，后面会放一个PPT参考链接，有兴趣可以仔细品味。
但是当真正对接第三方插件时，还有一些小问题需要解决，这个问题就是多个ENV时，每个ENV都有独立的网络插件，
那么network-manager如何匹配自己当前ENV的CNI config文件，关于此问题，
我建了一个issue&lt;a href=&#34;https://github.com/rancher/rancher/issues/8535&#34;&gt;#8535&lt;/a&gt;，并提交了相关PR。
在Rancher v1.6.0+版本中，已经包含了这些修复。&lt;/p&gt;

&lt;p&gt;此外，还需要注意的是，在Rancher中集成第三方网络插件需要满足以下三个条件，否则兼容性和体验会大打折扣：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;容器可以分配IP，并能互相联通，IP与MAC设定尽量与Rancher同步&lt;/li&gt;
&lt;li&gt;容器网络可以访问metadata服务，即容器内可以ping rancher-metadata&lt;/li&gt;
&lt;li&gt;LB与metadata等组件兼容性&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;calico的支持&#34;&gt;Calico的支持&lt;/h3&gt;

&lt;p&gt;Calico本身的设计思路很独到，利用BGP技术学习路由，通过路由构建高性能的网络，但是Calico本身在用户端的落地阻力还是相当大的，
很多用户纯粹把Calico的支持当作一个Poc的要点，因为如果用户真的决定使用Calico网络，那么来自用户内部网管的阻力是相当大的。
BGP路由动态学习对于很多行业，这种网络几乎认为是没有管理的状态。&lt;/p&gt;

&lt;p&gt;出于对Poc支持的考虑和一些付费用户的定制需求，大家如有兴趣可以参考一个demo版本的Rancher Calico的实现：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Issue描述&lt;a href=&#34;https://github.com/rancher/rancher/issues/8603&#34;&gt;#rancher-8603&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Demo Catalog &lt;a href=&#34;https://github.com/niusmallnan/test-calico-catalog-item&#34;&gt;#test-calico-catalog-item&lt;/a&gt;，请使用test分支&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;flat的支持&#34;&gt;Flat的支持&lt;/h3&gt;

&lt;p&gt;Flat网络，中国用户喜欢称它为扁平网络，其目的是构建一种不带有任何路由或者overlay技术的网络，基本上容器的网络就是在一张大网中。
通常容器的Flat网络是在交换机中配置好的，主机和容器其实使用的是同一个网段，这时候需要规划好网络的设置，比如主机的ip范围和容器的ip范围不能冲突。&lt;/p&gt;

&lt;p&gt;Flat网络的实现非常简单，原理上用户也很容易了解，在用户自身的网络部门推广起来几乎没有阻力，很多商业用户依赖这个网络。
所以Rancher已经开源了Flat网络的实现，用户可以在v1.6.11版本上使用Flat网络，在Catalog中就可以找到。&lt;/p&gt;

&lt;h3 id=&#34;macvlan的支持&#34;&gt;Macvlan的支持&lt;/h3&gt;

&lt;p&gt;Macvlan网络更多是在Docker的CNM下被提及，其实CNI下也有Macvlan的实现，通常我们利用Macvlan可以隔离不同的容器子网。
但是实际上这个特性Rancher并不是十分需要，Rancher的设计中一个ENV下就是一张网，这样可以很方便用户使用。
如果不需要Macvlan的隔离特性，实际上这里其实更多就是利用Macvlan构建了一个类似Flat的网络，对于大部分Rancher用户来说Macvlan并没有很大的使用价值。&lt;/p&gt;

&lt;p&gt;出于对Poc支持的考虑和一些付费用户的定制需求，大家如有兴趣可以参考一个demo版本的Rancher Macvlan的实现：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Issue描述&lt;a href=&#34;https://github.com/rancher/rancher/issues/8686&#34;&gt;#8686&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Demo Catalog App &lt;a href=&#34;https://github.com/niusmallnan/flatnet-catalog&#34;&gt;macvlan&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;更多内容可以参考我之前对外分享的PPT，请自行搭梯子，&lt;a href=&#34;https://docs.google.com/presentation/d/1U4YZpsXnFg6l7YNIcaq0SJ5DsUlSr14mtWnxQ-rvFwQ/edit?usp=sharing&#34;&gt;Rancher network management&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Rancher体系下容器日志采集</title>
      <link>http://niusmallnan.com/2017/04/11/rancher-logging</link>
      <pubDate>Tue, 11 Apr 2017 11:56:14 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2017/04/11/rancher-logging</guid>
      <description>&lt;p&gt;在Rancher体系内，实现容器日志采集。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;一个完整的容器平台，容器日志都是很重要的一环。尤其在微服务架构大行其道状况下，程序的访问监控健康状态很多都依赖日志信息的收集，
由于Docker的存在，让容器平台中的日志收集和传统方式很多不一样，日志的输出和采集点收集和以前大有不同。
本文就探讨一下，Rancher平台内如何做容器日志收集。&lt;/p&gt;

&lt;h3 id=&#34;当前现状&#34;&gt;当前现状&lt;/h3&gt;

&lt;p&gt;纵览当前容器日志收集的场景，无非就是两种方式：一是直接采集Docker标准输出，容器内的服务将日志信息写到标准输出，
这样通过Docker的log driver可以发送到相应的收集程序中；二是延续传统的日志写入方式，容器内的服务将日志直接写到普通文件中，
通过Docker volume将日志文件映射到Host上，日志采集程序就可以收集它。&lt;/p&gt;

&lt;p&gt;第一种方式足够简单，直接配置相关的Log Driver就可以，但是这种方式也有些劣势：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;当主机的容器密度比较高的时候，对Docker Engine的压力比较大，毕竟容器标准输出都要通过Docker Engine来处理。&lt;/li&gt;
&lt;li&gt;尽管原则上，我们希望遵循一容器部署一个服务的原则，但是有时候特殊情况不可避免容器内有多个业务服务，
这时候很难做到所有服务都向标准输出写日志，这就需要用到前面所说的第二种场景模式。&lt;/li&gt;
&lt;li&gt;虽然我们可以先选择很多种Log Driver，但是有些Log Driver会破坏Docker原生的体验，比如docker logs无法直接看到容器日志。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;基于以上考虑，一个完整的日志方案必须要同时满足标准输出和日志卷两种模式才可以。当然完整的日志体系中，并不仅仅是采集，
还需要有日志存储和UI展现。日志存储有很多种开源的实现，这个一般用户都会有自己钟情的选择，而UI展现更是各家有各家的需求，
很难形成比较好的标准，一般都是通过定制化方式解决。所以此文主要展现的方案是日志采集方案，当然在存储和UI展现上会对接开源实现，
没有特殊需求的情况下，也可以拥有一个完整的体验。&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tNc79ly1feiptl1cx3j31js0v6401.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;rancher下的解决方案&#34;&gt;Rancher下的解决方案&lt;/h3&gt;

&lt;p&gt;如上面图中所示，日志存储和UI展现可以直接使用ElasticSearch &amp;amp; Kibana，日志采集方面如之前所分析，需要对接两种采集模式，
所以这部分采用Fluentd &amp;amp; Logging Helper的组合，Fluentd是很通用的日志采集程序，拥有优异的性能，相对Logstash来说同等压力下，
其内存消耗要少很多。Logging Helper是可以理解微Fluentd的助手，它可以识别容器内的日志卷文件，通知Fluentd进行采集。
此外，由于要保证Dokcer和Rancher体验的完整性，在Docker Log Driver的选型上支持json-file和journald，其原因：
一是json-file和journald相对来说比较常用；二是这两种驱动下，docker logs依然可以有内容输出，保证了体验的完整性。&lt;/p&gt;

&lt;p&gt;下面开始说明，整个方案的部署过程。先用一张图来描述整体的部署结构，如下：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tNc79ly1feirbhlr0ij31700j4aaj.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
总共划分三个ENV，其中一个ENV部署ES &amp;amp; Kibana，另外两个ENV分别添加json-file驱动的主机和journald驱动的主机。
详细部署过程如下：&lt;/p&gt;

&lt;p&gt;创建三个ENV，使用Cattle引擎。设置Logging Catalog方便部署，
在Admin&amp;ndash;Settings页面中添加Catalog，地址为：&lt;a href=&#34;https://github.com/niusmallnan/rancher-logging-catalog.git&#34;&gt;https://github.com/niusmallnan/rancher-logging-catalog.git&lt;/a&gt;&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tNc79ly1feirqbal91j31du0b2q3d.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;进入ES-Kibana ENV中，部署ElasticSearch &amp;amp; Kibana，这两个应用在Community Catalog中均可以找到，部署非常简单，
需要注意的是，建议选择Elasticsearch 2.x，Kibana中的Elasicsearch Source选择elasticseach-clients：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tNc79ly1feirvmhq6qj30sc0mimxt.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;分配两台主机并安装Docker，其中Log Driver分别选择json-file和journald，并将主机添加到各自的ENV中。
在这两个ENV中添加External Link指向之前部署的Elasticsearch地址：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tNc79ly1feis5geevwj319e0gmaas.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
然后在Jsonfile &amp;amp; Journald ENV中添加Rancher Logging应用，打开对应的catalog，ES link指向刚才设定的External link，
其中Volume Pattern是日志卷的匹配模式，File Pattern是日志卷内的日志文件匹配模式：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tNc79ly1feis130yinj31ke0scwfk.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;以上部署完成之后，部署一些应用并产生一些访问日志，就可以在Kibana的界面中看到：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tNc79ly1feistxqn3ej31a60x0jto.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;若要使用日志卷方式，则需要在service启动的时候配置volume，volume name需要匹配之前设定的Volume Pattern：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tNc79ly1feisvr5tlkj317m0ju3zd.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;秉承一切开源的原则，相关实现可以查看一下链接：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/niusmallnan/rancher-fluentd-package&#34;&gt;https://github.com/niusmallnan/rancher-fluentd-package&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/niusmallnan/logging-helper&#34;&gt;https://github.com/niusmallnan/logging-helper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/niusmallnan/rancher-logging-catalog&#34;&gt;https://github.com/niusmallnan/rancher-logging-catalog&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;通过Fluentd我们可以对接很多第三方日志存储体系，但是Fluentd自身并不能完成日志采集的所有场景，所以非常需要Logging Helper的帮助。
通过Logging Helper可以定制出一些额外采集规则，比如可以过滤某些容器日志等等。当然真正大规模生产环境日志平台，其实是对整个运维体系的考验，
单纯靠开源程序的组合并不能真正解决问题。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Kubelet无法访问rancher-metadata问题分析</title>
      <link>http://niusmallnan.com/2017/03/09/analysis-of-kubelet-start-failure</link>
      <pubDate>Thu, 09 Mar 2017 15:45:41 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2017/03/09/analysis-of-kubelet-start-failure</guid>
      <description>&lt;p&gt;拯救一脸懵逼
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;Rancher能够支持Kubernetes，可以快速几乎无障碍的拉起一套K8s环境，这对刚入门K8s的小白来说简直是一大利器。
当然由于系统特性五花八门，系统内置软件也相互影响，所以有时候伙伴们会碰到比较难缠的问题。
本文就分析一下关于kubelet无法访问rancher-metadata问题，当然这个问题并不是必现，
这里要感谢一位Rancher社区网友的帮助，他的环境中发现了这个问题，并慷慨得将访问权限共享给了我。&lt;/p&gt;

&lt;h3 id=&#34;问题现象&#34;&gt;问题现象&lt;/h3&gt;

&lt;p&gt;使用Rancher部署K8s后，发现一切服务状态均正常，这时候打开K8s dashboard却无法访问，
细心得查看会发现，dashboard服务并没有部署起来，这时下意识的行为是查看kubelet的日志，
此时会发现一个异常：&lt;br /&gt;
&lt;img src=&#34;https://ww1.sinaimg.cn/large/006tNbRwly1fdgnj5h1bjj30g503omxw.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
你会发现kubelet容器内部一直无法访问rancher-metadata，查看rancher-k8s-package源码，
kubelet服务启动之前需要通过访问rancher-metadata做一些初始化动作，由于访问不了，
便一直处于sleep状态，也就是出现了上面提到的那些异常日志的现象：&lt;br /&gt;
&lt;img src=&#34;https://ww2.sinaimg.cn/large/006tNbRwly1fdgnn1tbzzj30k2071q41.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;同样，在github上也能看到类似的issue：&lt;a href=&#34;https://github.com/rancher/rancher/issues/7160&#34;&gt;https://github.com/rancher/rancher/issues/7160&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;排查分析&#34;&gt;排查分析&lt;/h3&gt;

&lt;p&gt;进入kubelet容器一探究竟，分别用ping和dig测试对rancher-metadata访问情况如下：&lt;br /&gt;
&lt;img src=&#34;https://ww1.sinaimg.cn/large/006tNbRwly1fdgnq66xs1j30ey0abmyh.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
dig明显可以解析，但是ping无法解析，因此基本排除了容器内dns nameserver或者网络链路情况的问题。&lt;/p&gt;

&lt;p&gt;既然dig没有问题，ping有问题，那么我们就直接采取使用strace（&lt;code&gt;strace ping rancher-metadata -c 1&lt;/code&gt;）来调试，
这样可以打印系统内部调用的情况，可以更深层次找到问题根源：&lt;br /&gt;
&lt;img src=&#34;https://ww1.sinaimg.cn/large/006tNbRwly1fdgnsufow0j30k7090tb2.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;之前提到这个问题并不是必现的，所以我们找一个正常的环境，同样用strace调试，如下：&lt;br /&gt;
&lt;img src=&#34;https://ww2.sinaimg.cn/large/006tNbRwly1fdgnu63kl2j30m0077dhj.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;对这两张图，其实已经能够很明显的看出区别，有问题的kubelet在解析rancher-metadata之前，
向nscd请求的解析结果，nscd返回了unkown host，所以就没有进行dns解析。
而正常的kubelet节点并没有找到nscd.socket，而后直接请求dns进行解析rancher-metadata地址。&lt;/p&gt;

&lt;p&gt;经过以上的分析，基本上断定问题出在nscd上，那么为什么同样版本的rancher-k8s，
一个有nscd socket，而另一个却没有，仔细看一下kubelet的compose定义：&lt;br /&gt;
&lt;img src=&#34;https://ww4.sinaimg.cn/large/006tNbRwly1fdgny3wpihj30c5087wf5.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
kubelet启动时候映射了主机目录/var/run，那么基本可以得知nscd来自于系统。
检查一下有问题的kubelet节点的系统，果然会发现安装了nscd服务（服务名为unscd）。&lt;/p&gt;

&lt;p&gt;用比较暴力的方案证明一下分析过程，直接删除nscd socket文件，这时候你会发现kubelet服务正常启动了，
rancher-metadata也可以访问了。&lt;/p&gt;

&lt;p&gt;回过头来思考一下原理，为什么ping/curl这种会先去nscd中寻找解析结果呢，而dig/nslookup则不受影响。
ping/curl这种在解析地址前都会先读取/etc/nsswitch.conf，这是由于其底层均引用了glibc，
由nsswitch调度，最终指引ping/curl先去找nscd服务。nscd服务是一个name services cache服务，
很多解析结果他会缓存，而我们知道这个nscd是运行在Host上的，Host上是不能直接访问rancher-metadata这个服务名，
所以kubelet容器中就无法访问rancher-metadata。&lt;/p&gt;

&lt;h3 id=&#34;其他解决方案&#34;&gt;其他解决方案&lt;/h3&gt;

&lt;p&gt;其实我们也未必要如此暴力删除nscd，nscd也有一些配置，我们可以修改一下以避免这种情况，
可以disable hosts cache，这样nscd中便不会有相应内容的缓存，所以解析rancher-metadata并不会出现unknown host，
而是继续向dns nameserver申请解析地址，这样也不会有问题。&lt;br /&gt;
&lt;img src=&#34;https://ww4.sinaimg.cn/large/006tNbRwly1fdgo48mxf8j30ej06lab0.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;遇到问题不能慌，关键是要沉得住气，很多看似非常复杂的问题，其实往往都是一个小配置引发的血案。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Rancher如何按计算资源调度</title>
      <link>http://niusmallnan.com/2017/02/23/rancher-scheduler-based-on-resource</link>
      <pubDate>Thu, 23 Feb 2017 18:58:19 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2017/02/23/rancher-scheduler-based-on-resource</guid>
      <description>&lt;p&gt;非常简单的说明一下，Rancher如何做按计算资源调度。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;按计算资源调度基本上是各大编排引擎的标配，Rancher在v1.2版本后也推出了这个功能，
很多朋友并不知道，主要是因为当前的实现还并不是那么智能，故不才欲写下此文以助视听。&lt;/p&gt;

&lt;h3 id=&#34;实现机制&#34;&gt;实现机制&lt;/h3&gt;

&lt;p&gt;Rancher的实现比较简单，其主要是通过Infra services中的scheduler服务来实现，整体的逻辑架构如下：&lt;br /&gt;
&lt;img src=&#34;https://ww3.sinaimg.cn/large/006tNc79ly1fd0lxplni2j30m90a7q49.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
scheduler会订阅Rancher Events，主要是scheduler相关事件，当有调度需求时候，scheduler就会收到消息，
通过计算将合适的调度目标返回给cattle。比如说现在支持memory和cpu为基准，
那么scheduler会不断根据metadata的数据变化来计算资源的使用量，最后可根据资源剩余量为调度目标排序，
这样就可以完成按计算资源调度的目标。&lt;/p&gt;

&lt;p&gt;之前有说，Rancher的实现并不智能，这在于在计算资源使用量的时候，Rancher并不是通过一套复杂数据采集机制来计算，
而是通过用户在创建service的时候标注reservation的方式，这个地方很多朋友并没有注意到：&lt;br /&gt;
&lt;img src=&#34;https://ww4.sinaimg.cn/large/006tNc79ly1fd0m3wzn0dj30lf08f3ze.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;除此之外，在每个节点的资源总量上也是可配置的，我们完全可以进行一个整体预留的设置，比如：&lt;br /&gt;
&lt;img src=&#34;https://ww4.sinaimg.cn/large/006tNc79ly1fd0m5p3spuj30f90amaao.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;这个实现看似简单，其实这是提供了一个很好的扩展能力。如果我们有自己的监控采集体系，
完全可以在scheduler的时候调用我们自身监控接口来计算资源，这样就能达到我们所认可的“智能”了。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>关于Subscribe Rancher Events的思考</title>
      <link>http://niusmallnan.com/2017/02/23/thinking-about-subcribe-rancher-events</link>
      <pubDate>Thu, 23 Feb 2017 17:33:56 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2017/02/23/thinking-about-subcribe-rancher-events</guid>
      <description>&lt;p&gt;路漫漫其修远兮，吾将上下而求索
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;几乎每个大型的分布式的集群软件，都离不开一样东西，就是所谓的message bus（消息总线），
它就如同人体的血管一样，连通着各个组件，相互协调，一起工作。
与很多同类软件不同的是，Rancher使用的基于websocket协议实现的消息总线，
Rancher不会依赖任何MQ，基于websocket的实现十分轻量级，
同时在各种语言库的支持上，也毫无压力，毕竟websocket是HTTP的标准规范之一。&lt;/p&gt;

&lt;h3 id=&#34;初识rancher-events&#34;&gt;初识Rancher Events&lt;/h3&gt;

&lt;p&gt;基于websocket的消息总线可以很好的与前端兼容，让消息的传递不再是后端的专利。
在Rancher UI上，很容易就能捕获到rancher events，比如：&lt;br /&gt;
&lt;img src=&#34;https://ww4.sinaimg.cn/large/006tNc79ly1fd0k5tuin9j30it07at9x.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
这里面监听的事件名称是resource.change，这个resource.change在前端UI上有很大的用处，
其实我们都知道，很多POST形式的create请求并不是同步返回结果的，因为调度引擎需要处理，
这个等待的过程中，当然不能前端一直wait，所以做法都是发起create后直接返回HTTP 202，
转入后台执行后，Rancher的后端会把创建的执行过程中间状态不断发送给消息总线，
那么前端通过监听resource.change就会获得这些中间状态，这样在UI上就可以给用户一个很好的反馈体验。&lt;/p&gt;

&lt;p&gt;当然Rancher Events并不是只有resource.change，比如在Iaas Events集合中就有如下这些：&lt;br /&gt;
&lt;img src=&#34;https://ww1.sinaimg.cn/large/006tNc79ly1fd0kccvpwxj30gk0ac40k.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;除了Events的事件定义，当然还有如何去subscribe 这些events，这部分内容我之前的文章中有所涉猎，
便不赘言。&lt;a href=&#34;http://niusmallnan.com/2016/08/25/rancher-envent/&#34;&gt;http://niusmallnan.com/2016/08/25/rancher-envent/&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;subscribe-rancher-events的架构模式&#34;&gt;Subscribe Rancher Events的架构模式&lt;/h3&gt;

&lt;p&gt;Rancher的体系内，很多微服务的组件都是基于Subscribe Rancher Events这种架构，举个例子来看，
以rancher-metadata组件为例：&lt;br /&gt;
&lt;img src=&#34;https://ww4.sinaimg.cn/large/006tNc79ly1fd0kmwbmu4j30iy07wq3n.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
metadata服务可以提供当前host的元数据查询，我们可以很容器的知道env内的stack/service/container的情况，
这些数据其实由rancher-server也就是cattle引擎生成的，那么生成之后怎么发送给各个agent呢？
其实就是metadata进行了subscribe rancher events，当然它只监听了config.update事件，
只要这个事件有通知，metadata服务便会下载新的元数据，这样就达到了不断更新元数据的目的。&lt;/p&gt;

&lt;p&gt;随着深入的使用Rancher，肯定会有一些伙伴需要对Rancher进行扩展，那就需要自行研发了，
毕竟常见的方式就是监听一些事件做一些内部处理逻辑，并在DB中存入一些数据，
同时暴露API服务，架构如下：&lt;br /&gt;
&lt;img src=&#34;https://ww3.sinaimg.cn/large/006tNc79ly1fd0kt6rttwj309o072jrn.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;如果需要做HA，可能需要scale多个这样的服务，那么架构就变成这样：&lt;br /&gt;
&lt;img src=&#34;https://ww2.sinaimg.cn/large/006tNc79ly1fd0ku7zaafj30g30avdgq.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
这里其实会有一个问题，如果你监听了一些广播事件，那么实际上每个实例都会收到同样的事件，
那么你的处理逻辑就要注意了，尤其是在处理向DB中写数据时，一定要考虑到这样的情况。&lt;/p&gt;

&lt;p&gt;比如，可以只有其中一个实例来监听广播事件，这样不会导致事件重复收取：&lt;br /&gt;
&lt;img src=&#34;https://ww4.sinaimg.cn/large/006tNc79ly1fd0kxdd6vmj30g30b1dgm.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
Event Handler要考虑一定failover机制，这样事件收取不会长时间中断。&lt;/p&gt;

&lt;p&gt;Rancher Events有一些非广播事件，那么就需要在subscribe的时候指定一些特殊参数，
这样事件就会只发送给注册方，不会发送给每个节点，比如：&lt;br /&gt;
&lt;img src=&#34;https://ww3.sinaimg.cn/large/006tNc79ly1fd0kztt3k6j30fx0audgr.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;此文算是这段时间做Rancher服务扩展的心得，深度参与一个开源软件最终肯定会希望去改动它扩展它。
这也是客观需求所致，开源软件可以拿来即用，但是真正可用实用，必须加以改造，适应自身需求。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Rancher K8s Helm使用体验</title>
      <link>http://niusmallnan.com/2017/02/05/rancher-k8s-helm-practice</link>
      <pubDate>Sun, 05 Feb 2017 14:08:38 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2017/02/05/rancher-k8s-helm-practice</guid>
      <description>&lt;p&gt;潇潇洒洒玩一玩
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;之前的文章已经阐述了Rancher K8s中国区的使用优化，
&lt;a href=&#34;http://niusmallnan.com/2017/01/19/optimize-rancher-k8s-in-china/&#34;&gt;http://niusmallnan.com/2017/01/19/optimize-rancher-k8s-in-china/&lt;/a&gt;，
本文则是选取一个特殊的组件Helm，深入一些体验使用过程。Helm是K8s中类似管理catalog的组件，
在较新的Rancher K8s版本中，Helm已经默认集成可直接使用。&lt;/p&gt;

&lt;h3 id=&#34;初始化cli环境&#34;&gt;初始化CLI环境&lt;/h3&gt;

&lt;p&gt;如果已经习惯使用Rancher UI上自带的kubectl tab，那么可以跳过此步。
大部分玩家还是更喜欢在自己的机器上使用kubectl和helm CLI来做管理的。
在自己的机器上部署kubectl和helm命令行工具，有一个比较偷懒的方法，
就是直接到kubectld容器拷贝出来，主要过程如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# kubectrld容器ID为 42291346c064
$ docker cp 42291346c064:/usr/bin/kubectl /usr/local/bin/kubectl
$ docker cp 42291346c064:/usr/bin/helm /usr/local/bin/helm
$ docker cp 42291346c064:/tmp/.helm ~/.helm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然也不要忘记kubectl的配置文件，在Rancher UI上生成，然后拷贝到对应的本地目录上。&lt;br /&gt;
&lt;img src=&#34;https://ww1.sinaimg.cn/large/006tNc79ly1fcfkc0a5yjj30dx07edgb.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;基于helm部署mysql&#34;&gt;基于Helm部署Mysql&lt;/h3&gt;

&lt;p&gt;K8s官方的Charts中已经有了mysql这个应用，
这里&lt;a href=&#34;https://github.com/kubernetes/charts/tree/master/stable&#34;&gt;https://github.com/kubernetes/charts/tree/master/stable&lt;/a&gt;可以找到，
几乎所有的Chart都需要依赖PersistentVolumeClaim提供卷，所以在一切开始之前，
我们需要先创建一个PersistentVolume，来提供一个数据卷池。这里可以选择部署比较方便的NFS。&lt;/p&gt;

&lt;p&gt;选择一台主机，安装nfs-kernel-server，并做相应配置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ apt-get install nfs-kernel-server

# 配置卷
# 修改 /etc/exports，添加如下内容
/nfsexports *(rw,async,no_root_squash,no_subtree_check)

# 重启nfs-server
service nfs-kernel-server restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用kubectl创建PV，文件内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0003
  annotations:
    volume.beta.kubernetes.io/storage-class: &amp;quot;slow&amp;quot;
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  nfs:
    path: /nfsexports #NFS mount path
    server: 172.31.17.169 #NFS Server IP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装mysql之前，需要准备一份mysql chart的配置文件，也就是对应的values.yaml，
其他内容&lt;a href=&#34;https://github.com/kubernetes/charts/blob/master/stable/mysql/values.yaml&#34;&gt;https://github.com/kubernetes/charts/blob/master/stable/mysql/values.yaml&lt;/a&gt;基本不变，
主要修改persistence部分，这样所依赖的PVC才能bound到对应的PV上，如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;persistence:
  enabled: true
  storageClass: slow
  accessMode: ReadWriteMany
  size: 1Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一切准备妥当，就可以进行Mysql Chart的安装，执行过程如下：&lt;code&gt;helm install --name ddb -f values.yaml stable/mysql&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;安装完毕后，在Rancher UI和K8s Dashboard上都可以看到。&lt;br /&gt;
&lt;img src=&#34;https://ww1.sinaimg.cn/large/006tNc79ly1fcfkfju2ayj30k407q750.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
&lt;img src=&#34;https://ww3.sinaimg.cn/large/006tNc79ly1fcfkfsuoayj30ed05zaae.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>优化Rancher k8s中国区的使用体验</title>
      <link>http://niusmallnan.com/2017/01/19/optimize-rancher-k8s-in-china</link>
      <pubDate>Thu, 19 Jan 2017 17:32:56 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2017/01/19/optimize-rancher-k8s-in-china</guid>
      <description>&lt;p&gt;拯救一脸懵逼
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;Kubernetes（以下简称K8s）是Rancher平台重点支持的一个编排引擎，Rancher K8s具有部署灵活使用方便的特点，
而且Rancher基本是同步更新支持K8s的新版本新组件，用户也可以选择部署指定的K8s版本。
但是这一切的便利，身在中国的我们这些贱民无法深刻体验，万恶的GFW把很多部署的依赖挡在之外，
而服务全球开发者的Rancher平台亦不可能为中国用户单独定制，所以我只好自己动手丰衣足食。&lt;/p&gt;

&lt;h3 id=&#34;部署要点&#34;&gt;部署要点&lt;/h3&gt;

&lt;p&gt;部署之前的操作系统选型上，相对来说我比较推荐ubuntu+docker的组合，
毕竟这个组合在国外使用的用户比较多，相对来说bug fix的速度也是比较快的，
如果你是一个docker重度用户，应该深知docker本身的bug并不少。&lt;/p&gt;

&lt;p&gt;如果是部署一个新的Rancher环境，我推荐用下面的脚本来启动，通过设置DEFAULT_CATTLE_CATALOG_URL，
这样可以直接指定我定制过的Rancher K8s：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d --restart=unless-stopped \
     -e DEFAULT_CATTLE_CATALOG_URL=&#39;{&amp;quot;catalogs&amp;quot;:{&amp;quot;community&amp;quot;:{&amp;quot;url&amp;quot;:&amp;quot;https://github.com/rancher/community-catalog.git&amp;quot;,&amp;quot;branch&amp;quot;:&amp;quot;master&amp;quot;},&amp;quot;library&amp;quot;:{&amp;quot;url&amp;quot;:&amp;quot;https://github.com/niusmallnan/rancher-catalog.git&amp;quot;,&amp;quot;branch&amp;quot;:&amp;quot;k8s-cn&amp;quot;}}}&#39; \
     --name rancher-server \
     -p 8082:8080 rancher/server:stable
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然如果是已经部署的Rancher环境，那就需要在Rancher UI上，做一下简单的修改，
Disable已有的library catalog repo，指向我定制过的即可，注意branch的设置，网络状况不好的需要耐心等待重新拉取repo内容：
&lt;img src=&#34;https://ww3.sinaimg.cn/large/006y8lValy1fbw2toyl38j30s30a50u6.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在部署agent节点前，如果是一个干净的环境最好，但是如果是曾经做过agent节点，
尤其是之前部署过rancher k8s的，我强烈建议你执行一次大扫除，否则会出现各种意想不到的状况，
大扫除的脚本可以参考执行我的这个，具体都做了什么事可自行阅读：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash

docker rm -f $(docker ps -qa)

rm -rf /var/etcd/

for m in $(tac /proc/mounts | awk &#39;{print $2}&#39; | grep /var/lib/kubelet); do
    umount $m || true
done
rm -rf /var/lib/kubelet/

for m in $(tac /proc/mounts | awk &#39;{print $2}&#39; | grep /var/lib/rancher); do
    umount $m || true
done
rm -rf /var/lib/rancher/

rm -rf /run/kubernetes/

docker volume rm $(docker volume ls -q)

docker ps -a
docker volume ls
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;一切opensource&#34;&gt;一切OpenSource&lt;/h3&gt;

&lt;p&gt;如果你对我在其中的改动颇有疑虑，亦大可放心。我主要是改动两个地方：
fork了rancher-catalog建立了k8s-cn的分支，只要将Rancher的library catalog repo指向我的工程分支即可；
fork了kubernetes-package，每次Rancher K8s发布新版本，
我都会基于该版本建立一个CN分支（如：v1.5.1-rancher1-7-cn），
一切对于中国区的优化修改都会在这个分支上。最终我会更新出中国区的使用镜像，并push到镜像仓库上，
目前使用的是阿里云的镜像仓库（招牌比较大短时间内不会倒。。。）。&lt;/p&gt;

&lt;p&gt;参考链接：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/niusmallnan/rancher-catalog&#34;&gt;https://github.com/niusmallnan/rancher-catalog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/niusmallnan/kubernetes-package&#34;&gt;https://github.com/niusmallnan/kubernetes-package&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;后续支持计划&#34;&gt;后续支持计划&lt;/h3&gt;

&lt;p&gt;截止到本文写作之时，我刚开始支持rancher-k8s v1.5.1-rancher1-7版本，并在Rancher v1.3.1版本上做了测试。
后续Rancher官方发布新版本，我会进行同步更新，并做一些简单的测试。
后续考虑加入离线安装，可以指定本地镜像仓库，依赖镜像一键导入等方便的功能。
当然如果在使用中发现各种疑难杂症，可以发邮件给我niusmallnan@gmail.com。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Rancher-LB开启访问日志</title>
      <link>http://niusmallnan.com/2017/01/06/rancher-lb-logs</link>
      <pubDate>Fri, 06 Jan 2017 15:12:10 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2017/01/06/rancher-lb-logs</guid>
      <description>&lt;p&gt;拯救一脸懵逼
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;在Rancher v1.2之前的版本，Rancher LB也就是Haproxy都是放在agent/instance内的，由于这个镜像本身要支持很多功能，
所以要开启Haproxy的访问日志比较麻烦。因为Harpoxy的日志和其他Nginx之类略有不同，它是通过syslog协议讲日志发送出去的，
要想展现日志还要开启syslog进行收集。Rancher v1.2版本开始，
Rancher LB的功能已经解耦成独立的镜像rancher/lb-service-haproxy，如此日志收集的工作就可以做了。&lt;/p&gt;

&lt;h3 id=&#34;开启访问日志&#34;&gt;开启访问日志&lt;/h3&gt;

&lt;p&gt;lb-service-haproxy的实现可以仔细阅读源码&lt;a href=&#34;https://github.com/rancher/lb-controller&#34;&gt;https://github.com/rancher/lb-controller&lt;/a&gt;，
简单的说，其内置了rsyslog来收集haproxy的日志，在容器内部查看配置文件&lt;strong&gt;/etc/haproxy/rsyslogd.conf&lt;/strong&gt;，
可以知道其开启了udp 8514端口，不同的级别的日志发送到不同的文件中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ModLoad imudp.so
$UDPServerRun 8514
$template CustomFormat,&amp;quot;%timegenerated% %HOSTNAME% %syslogtag%%msg%\n&amp;quot;
$ActionFileDefaultTemplate CustomFormat

/* info */
if $programname contains &#39;haproxy&#39; and $syslogseverity == 6 then (
    action(type=&amp;quot;omfile&amp;quot; file=&amp;quot;/var/log/haproxy/traffic&amp;quot;)
)
if $programname contains &#39;haproxy&#39; and $syslogseverity-text == &#39;err&#39; then (
    action(type=&amp;quot;omfile&amp;quot; file=&amp;quot;/var/log/haproxy/errors&amp;quot;)
)
/* notice */
if $programname contains &#39;haproxy&#39; and $syslogseverity &amp;lt;= 5 then (
    action(type=&amp;quot;omfile&amp;quot; file=&amp;quot;/var/log/haproxy/events&amp;quot;)
)

*.* /var/log/messages
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关于syslog的Severity level可以参考这里：&lt;a href=&#34;https://en.wikipedia.org/wiki/Syslog&#34;&gt;https://en.wikipedia.org/wiki/Syslog&lt;/a&gt;，
所以我们要查看的访问日志，应该就是/var/log/haproxy/traffic文件。
同时还添加了logrotate的配置进行日志分割和压缩，可查看容器内的&lt;strong&gt;/etc/logrotate.d/haproxy&lt;/strong&gt;文件。&lt;/p&gt;

&lt;p&gt;当前lb-service-haproxy版本默认是不开启日志收集的，需要自己手动开启，
我们都知道Haproxy开启日志需要在其global和defaults配置中添加log的配置。
Rancher LB是支持custom haproxy.cfg的，所以在Rancher中添加这两块配置就可以这样：&lt;br /&gt;
&lt;img src=&#34;https://ww2.sinaimg.cn/large/006tKfTcjw1fbgy5m62l3j30i80ecgn1.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
Rancher会自动合并这几条配置，最终在lb-service-haproxy容器内生成的配置就是这样：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;global
    log 127.0.0.1:8514 local0
    log 127.0.0.1:8514 local1 notice
    chroot /var/lib/haproxy
    daemon
    group haproxy
    maxconn 4096
.....
.....
defaults
    log global
    option tcplog
    errorfile 400 /etc/haproxy/errors/400.http
    errorfile 403 /etc/haproxy/errors/403.http
.....
.....
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;那么查看之前分析的日志目录文件，可以看到响应的访问日志信息：&lt;br /&gt;
&lt;img src=&#34;https://ww1.sinaimg.cn/large/006tKfTcjw1fbgya5nb4vj30l309vn1r.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;注意事项&#34;&gt;注意事项&lt;/h3&gt;

&lt;p&gt;日志虽然可以这样灵活的开启，但是使用时需要注意：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;日志文件放在容器内会不断增大，虽然有logrotate，但是扛不住日积月累。
临时测试用本地可以，但是长远看还是发送到外部syslog服务上比较好。&lt;/li&gt;
&lt;li&gt;lb-service-haproxy的早期版本是默认就开启了访问日志，v0.4.5版本后取消了这个默认配置，
所以你就得使用我上面所说的方式，早期的lb-service-haproxy会积存很多访问日志，
所以尽早升级到v0.4.5以上的版本。&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    
    <item>
      <title>Dead容器导致Rancher Server过载的解决方案</title>
      <link>http://niusmallnan.com/2017/01/01/rancher-server-overwhelmed-by-dead-containers</link>
      <pubDate>Sun, 01 Jan 2017 07:42:28 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2017/01/01/rancher-server-overwhelmed-by-dead-containers</guid>
      <description>&lt;p&gt;接之前的一篇文章&lt;a href=&#34;http://niusmallnan.com/2016/12/27/docker-device-resource-busy/&#34;&gt;device or resource busy&lt;/a&gt;，
其中描述了Docker的一个bug，这个bug其实会导致Rancher Server出现一个比较严重的问题。
&lt;/p&gt;

&lt;h3 id=&#34;现象与原因&#34;&gt;现象与原因&lt;/h3&gt;

&lt;p&gt;Rancher Server运行一段时间后，如若发现各种操作卡死，Host添加不上，Stack&amp;amp;Service也无法active，
且在proceeses列表里发现大量volumestoragepoolmap.remove任务，可以参考本文的解决方案：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tKfTcjw1fbateimvjnj30iv0a40ug.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这些无法完成的volumestoragepoolmap.remove任务，背后其实就是那些dead containers，
分析之后，大致原因如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;docker本身的一个bug，导致一些容器删除失败，进入dead状态。&lt;/li&gt;
&lt;li&gt;Rancher的调度系统在不断重试删除这些dead容器，但是无法删除。&lt;/li&gt;
&lt;li&gt;无法删除导致，这些删除任务不断加入rancher调度系统中，造成调度系统过载。&lt;/li&gt;
&lt;li&gt;主机创建或是Stack/Service active请求都在排队，不能被迅速执行。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;临时解决方案&#34;&gt;临时解决方案&lt;/h3&gt;

&lt;p&gt;依据之前的分析，processes里面出现了很多永远无法完成的任务，这些任务在不断的重试，这会压垮Rancher Server。
所以解决方案的目标就是，介入外部力量删除这些dead容器，升级docker engine是一个选择，
但是直接在生产环境升级是一个需要勇气的选择。比较温和的做法是定制脚本到每台agent节点上
强行删除这些dead容器。&lt;/p&gt;

&lt;p&gt;考虑到这里有多台主机批量执行的场景，所以我们可以选择使用ansible工具，
创建一个工作目录如/opt/agent_upgrade，所有脚本都放到这个目录下。
首先我们要生成一个agent host的列表，这里可以借用Rancher的CLI工具，
具体脚本get_hosts.sh如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
#set -ex
rm -f all_hosts_ip

RANCHER=&amp;quot;/usr/local/bin/rancher&amp;quot;

for env in `$RANCHER env -a --format {{.ID}}`; do
    for host in `$RANCHER --env $env hosts --format json| jq .Host.agentIpAddress| sed &#39;s/\&amp;quot;//g&#39;`; do
        echo &amp;quot;${host}&amp;quot; &amp;gt;&amp;gt; all_hosts_ip
    done
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;定义一个ansible的配置文件，由于所有agent host都是有运维专有访问秘钥的，
所以这里直接定义private_key_file比较方便，如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[defaults]
hostfile=/opt/agent_upgrade/all_hosts_ip
host_key_checking = False
remote_user = ubuntu
private_key_file=/opt/agent_upgrade/aws-prd.pem
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后我们就可以使用ansible来做批量操作了，删除dead容器的脚本非常简单，
clean_dead_containers.sh脚本如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash

#set -ex
ansible all --sudo -m shell -a &amp;quot;docker ps -aq --filter status=dead &amp;gt; ~/dead&amp;quot; -vvvv
ansible all --sudo -m shell -a &amp;quot;cat ~/dead | xargs docker rm -f&amp;quot; -vvvv
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于Rancher集群在运行时仍然会产生这些dead containers，所以我们可以借用crontab来做定时清理，
添加一条crontab如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;*/30 * * * * cd /opt/agent_upgrade &amp;amp;&amp;amp; sh get_hosts.sh &amp;amp;&amp;amp; sh clean_dead_containers.sh &amp;gt; /tmp/dead_containers.log 2&amp;gt;&amp;amp;1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如此，每隔30分钟定期清理这些dead containers，Rancher Server就不会受其影响。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Docker中的device or resource busy问题分析</title>
      <link>http://niusmallnan.com/2016/12/27/docker-device-resource-busy</link>
      <pubDate>Tue, 27 Dec 2016 07:15:44 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/12/27/docker-device-resource-busy</guid>
      <description>&lt;p&gt;拯救一脸懵逼
&lt;/p&gt;

&lt;h3 id=&#34;现象描述&#34;&gt;现象描述&lt;/h3&gt;

&lt;p&gt;使用docker的时候，当我们删除某些容器，有时候会报出device or resource busy错误：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Error response from daemon:
Unable to remove filesystem for
a9a1c11e8210d60ddba09f95ea93ae21f32327c4e5877c218862c752d1088533: 
remove /var/lib/docker/containers/a9a1c11e8210d60ddba09f95ea93ae21f32327c4e5877c218862c752d1088533/shm:
device or resource busy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后可以查看相关容器的状态变成了很少见的Dead状态。&lt;/p&gt;

&lt;h3 id=&#34;原因分析&#34;&gt;原因分析&lt;/h3&gt;

&lt;p&gt;通常我们看到device or resource busy首先想到的是，这个设备被其他程序占用导致。
在容器中，其实理论是一样的，每个容器生成都会有一个containers/&lt;uuid&gt;/shm设备产生，
恰巧这个设备被其他程序mount了，就会被占用无法卸载，也就是device or resource busy。&lt;/p&gt;

&lt;p&gt;那么什么情况下会占用containers/&lt;uuid&gt;/shm设备呢？其最大的可能原因就是容器启动时挂载了/var/lib/docker目录，
此时恰巧docker出现了bug，顺带mount了containers/&lt;uuid&gt;/shm设备。
为证实猜想，我们启动一个挂载/var/lib/docker目录的容器，然后到容器中查看/proc/mounts，如下：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tKfTcjw1fb4znsftd3j30kh08ntfm.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
其实除了/var/lib/docker做了多余挂载之外，映射/var/run和/run/目录都会有同样的问题：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tKfTcjw1fb4zo5bmavj30ex04n0u4.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;修复方式&#34;&gt;修复方式&lt;/h3&gt;

&lt;p&gt;首先这肯定是docker的一个bug，很明显它不应该做多余的mount。这个bug在docker v1.12.4版本做了一些修复，
相关PR&lt;a href=&#34;https://github.com/docker/docker/pull/29083&#34;&gt;https://github.com/docker/docker/pull/29083&lt;/a&gt;，类似的issue&lt;a href=&#34;https://github.com/docker/docker/issues/20560&#34;&gt;https://github.com/docker/docker/issues/20560&lt;/a&gt;，
都可以在github上找到。所以docker v1.12.4之前也都会比较容易碰到这个问题，
虽然有一些FIX已经merge了，但是其实并没有完全解决，起码我在docker v1.12.6上使用AUFS驱动，
还是会发现这个问题。&lt;/p&gt;

&lt;p&gt;那么如果因为各种原因无法变更docker，但是还想避免这个问题怎么办？
比如可以在mount /var/lib/docker（或是你迁移之后的DockerRootDir）的容器中执行以下脚本：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash

for i in $(curl -s --unix /var/run/docker.sock http://localhost/info | jq -r .DockerRootDir) /var/lib/docker /run /var/run; do
    for m in $(tac /proc/mounts | awk &#39;{print $2}&#39; | grep ^${i}/); do
        umount $m || true
    done
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要目的是把多余mount的path卸载掉，这样就不会因为同时挂载导致device or resource busy的问题。
需要注意的是，是否也要umount /run和/var/run要结合自身实际应用去考虑。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Rancher v1.2 Swarmkit的实现</title>
      <link>http://niusmallnan.com/2016/12/14/rancher12-swarmkit-architect</link>
      <pubDate>Wed, 14 Dec 2016 21:53:27 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/12/14/rancher12-swarmkit-architect</guid>
      <description>&lt;p&gt;Rancher v1.2系列文章，已独家授权Rancher Labs，转载请联系Rancher China相关人员。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;Rancher v1.2更新了之前对Swarm的支持，与Docker一样抛弃了就有的Swarm，
选择支持Swarmkit。Swarmkit引擎非常轻量级，由于其内置早Docker Engine中，
所以部署起来会非常方便。虽然目前Swarmkit引擎还在不断发展，而且bug也很多，
但是它也有其擅长的使用场景，比如简单的CI/CD场景，它会非常灵活简洁。
本文将带大家体验一下，Rancher v1.2对Swarmkit的支持。&lt;/p&gt;

&lt;h3 id=&#34;部署与使用&#34;&gt;部署与使用&lt;/h3&gt;

&lt;p&gt;部署方面秉承Rancher一贯的原则，非常简单，只需要在创建Env时选择Sawrm即可。&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tKfTcjw1favae6x30jj30nn09odgr.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
Env创建完毕后，会看到多个Infra Service需要创建，这时候和其他引擎一样，
我们需要向Env中添加Host。我们知道Swarm的node有两种：Manager和Worker。
Rancher创建的Swarm集群默认是3个Manager，多个Manager内有一个是Leader，
另外两个备用。这样单个Host出问题，新的Leader会很快选举出来，保证集群的稳定性。
比如我添加了两个Host，默认是先添加Manager角色，
所以2个Host都会以Manager方式添加，如下图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tKfTcjw1favaeq4hrhj30hc074aav.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
进入其中一台Host内，查看swarm集群状态，可以看到一个是Leader，另外一个Reachable做备用。&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tKfTcjw1favaey8g8qj30fc01wq3g.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
尝试创建一个简单的程序，查看与UI上的联动效果，如图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tKfTcjw1favaf8yytqj30eb05vdgv.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
如果使用Swarmkit的自定义网络方式，情况如何？虽然在UI上显示无IP，
但是进入容器内部可以看到overlay对应的网卡，如图所示：
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tKfTcjw1favafscwv8j30ii07habk.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;实现原理&#34;&gt;实现原理&lt;/h3&gt;

&lt;p&gt;那么Rancher是如何来完成Swarmkit的部署和联动呢？Rancher中Swarmkit也是基于Cattle来部署的，
根据之前的文章分析，我们可以知道Rancher的基础设施编排的定义都是通过catalog中的infra-templates实现的，
Swarmkit比较特殊它是在community-catalog中定义的，如果一直在rancher-catalog中寻找肯定找不到。
compose文件中定义了一个service swarmkit-mon，如图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tKfTcjw1favaggy2snj30iz0at40g.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
如果探究原理，我们就需要知道swarmkit-mon对应的镜像是如何定义的。
rancher/swarmkit这个dockerfile并没有在&lt;a href=&#34;https://github.com/rancher&#34;&gt;https://github.com/rancher&lt;/a&gt;下面的项目中，
这个需要顺藤摸瓜，找到该Dockerfile的维护者（其实也是Rancher的一名员工），
最终地址是&lt;a href=&#34;https://github.com/LLParse/swarmkit-catalog&#34;&gt;https://github.com/LLParse/swarmkit-catalog&lt;/a&gt;。如图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tKfTcjw1favahpg2pzj30kc0aiq57.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
swarmkit-mon中内置了docker，并映射了Host上的docker.sock，
这样可以在swarmkit-mon容器中控制docker创建swarmkit集群。swarmkit-mon的实现比较简单，
主要包括两个shell脚本：run.sh负责swarmkit集群的管理和Rancher的联动，
agent节点信息需要通过rancher-metadata读取，设置Host Label则直接调用Rancher API；
health.sh负责监控swarmkit节点的状态（通过与docker.sock通信读取Swarm.LocalNodeState的状态），
并与giddyup协作暴露健康检查端口，这样可以利用Rancher Cattle的healthcheck来保证swarmkit-mon服务的高可用性，
每个Host的swarmkit-mon出问题时可以进行自动重建恢复。原理如图：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tKfTcjw1favaidbyfgj30bh08qmyh.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;目前来看，由于Kubernetes的发展的确迅猛，所以Rancher的更多精力都放在K8s上。
针对Swarmkit的支持显得略显单薄，但是Swarmkit本身的问题也很多，目前也难以应对复杂场景，
所以目前的支持力度应该是足够了。后续对docker1.13版本的Swarmkit支持也在持续迭代中。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>为什么Rancher v1.2中netstat看不到开放端口</title>
      <link>http://niusmallnan.com/2016/12/09/rancher-netstat-miss-port</link>
      <pubDate>Fri, 09 Dec 2016 15:07:14 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/12/09/rancher-netstat-miss-port</guid>
      <description>&lt;p&gt;拯救一脸懵逼。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;随着Rancher v1.2的发布，越来越多的伙伴参与到新特性的尝鲜中。有时不免会碰到一些问题，
比如网络不通之类的，这时通常我们都会下意识的使用netstat命令查看端口是否正确开启，
可是出现的结果却让伙伴们一脸疑问。比如我们使用了ipsec网络，
netstat命令却看不到UDP 500和4500端口的开放，这是为什么呢？本文将释疑这个问题。&lt;/p&gt;

&lt;h3 id=&#34;问题现象&#34;&gt;问题现象&lt;/h3&gt;

&lt;p&gt;启动之前的Rancher v1.1版本并对比v1.2版本，可以看到如下现象，
ipsec网络正常，但在新版中netstat却看不到端口开放：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tKfTcjw1fakk2e84hcj30d509z76g.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
用LB暴露一个服务访问，UI上明明显示了端口，但是在netstat中却看不到：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tKfTcjw1fakk3aasx6j30j7075my6.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
这些现象是为什么？&lt;/p&gt;

&lt;h3 id=&#34;原生docker和k8s可以看到端口&#34;&gt;原生docker和k8s可以看到端口&lt;/h3&gt;

&lt;p&gt;原先的docker，一般run一个容器加入-p参数，就可以看到端口。
这其实是由当前tcp/ip栈内的docker-proxy开放的，如图：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tKfTcjw1fakk45w280j30ju06t0v2.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
k8s是可以通过netstat看到端口，举个例子，创建一个service暴露NodePort，如下图：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tKfTcjw1fakk4l1qplj308z069wer.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
而通过netstat命令却能看到对应的端口，这是因为这个端口是由kube-proxy开放的，
就是说在这个tcp/ip栈内有应用程序bind了这个端口：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tKfTcjw1fakk4z18gzj30j605jtah.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;本质原因&#34;&gt;本质原因&lt;/h3&gt;

&lt;p&gt;端口的本质其实是应用程序，就是说需要有process bind port，
而netstat要能显示出开放端口，则必须在当前的namespace里有应用程序监听了端口才行。
Rancher中因为是使用CNI模型，虽然也是使用docker0，但是这个docker0是CNI Bridge，
并不和docker-proxy有协作，所以端口并没有通过docker-proxy暴露，
只是通过iptables dnat转发到其他的namespace上。在当前的namespace里使用netstat看不到开放的端口，
iptables dnat规则如下：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tKfTcjw1fakk5qzfr6j30jg03idh7.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;有时候惯性思维让我们忽略了事物的本质，让人匪夷所思的问题往往是很简单的道理。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>