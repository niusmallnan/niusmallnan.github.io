<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Niusmallnan</title>
    <link>http://niusmallnan.com/index.xml</link>
    <description>Recent content on Niusmallnan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <copyright>Niusmallnan</copyright>
    <lastBuildDate>Wed, 14 Dec 2016 21:53:27 +0800</lastBuildDate>
    <atom:link href="http://niusmallnan.com/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Rancher v1.2 Swarmkit的实现</title>
      <link>http://niusmallnan.com/2016/12/14/rancher12-swarmkit-architect</link>
      <pubDate>Wed, 14 Dec 2016 21:53:27 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/12/14/rancher12-swarmkit-architect</guid>
      <description>&lt;p&gt;Rancher v1.2系列文章，已独家授权Rancher Labs，转载请联系Rancher China相关人员。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;Rancher v1.2更新了之前对Swarm的支持，与Docker一样抛弃了就有的Swarm，
选择支持Swarmkit。Swarmkit引擎非常轻量级，由于其内置早Docker Engine中，
所以部署起来会非常方便。虽然目前Swarmkit引擎还在不断发展，而且bug也很多，
但是它也有其擅长的使用场景，比如简单的CI/CD场景，它会非常灵活简洁。
本文将带大家体验一下，Rancher v1.2对Swarmkit的支持。&lt;/p&gt;

&lt;h3 id=&#34;部署与使用&#34;&gt;部署与使用&lt;/h3&gt;

&lt;p&gt;部署方面秉承Rancher一贯的原则，非常简单，只需要在创建Env时选择Sawrm即可。&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tKfTcjw1favae6x30jj30nn09odgr.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
Env创建完毕后，会看到多个Infra Service需要创建，这时候和其他引擎一样，
我们需要向Env中添加Host。我们知道Swarm的node有两种：Manager和Worker。
Rancher创建的Swarm集群默认是3个Manager，多个Manager内有一个是Leader，
另外两个备用。这样单个Host出问题，新的Leader会很快选举出来，保证集群的稳定性。
比如我添加了两个Host，默认是先添加Manager角色，
所以2个Host都会以Manager方式添加，如下图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tKfTcjw1favaeq4hrhj30hc074aav.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
进入其中一台Host内，查看swarm集群状态，可以看到一个是Leader，另外一个Reachable做备用。&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tKfTcjw1favaey8g8qj30fc01wq3g.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
尝试创建一个简单的程序，查看与UI上的联动效果，如图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tKfTcjw1favaf8yytqj30eb05vdgv.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
如果使用Swarmkit的自定义网络方式，情况如何？虽然在UI上显示无IP，
但是进入容器内部可以看到overlay对应的网卡，如图所示：
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tKfTcjw1favafscwv8j30ii07habk.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;实现原理&#34;&gt;实现原理&lt;/h3&gt;

&lt;p&gt;那么Rancher是如何来完成Swarmkit的部署和联动呢？Rancher中Swarmkit也是基于Cattle来部署的，
根据之前的文章分析，我们可以知道Rancher的基础设施编排的定义都是通过catalog中的infra-templates实现的，
Swarmkit比较特殊它是在community-catalog中定义的，如果一直在rancher-catalog中寻找肯定找不到。
compose文件中定义了一个service swarmkit-mon，如图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tKfTcjw1favaggy2snj30iz0at40g.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
如果探究原理，我们就需要知道swarmkit-mon对应的镜像是如何定义的。
rancher/swarmkit这个dockerfile并没有在&lt;a href=&#34;https://github.com/rancher&#34;&gt;https://github.com/rancher&lt;/a&gt;下面的项目中，
这个需要顺藤摸瓜，找到该Dockerfile的维护者（其实也是Rancher的一名员工），
最终地址是&lt;a href=&#34;https://github.com/LLParse/swarmkit-catalog&#34;&gt;https://github.com/LLParse/swarmkit-catalog&lt;/a&gt;。如图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tKfTcjw1favahpg2pzj30kc0aiq57.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
swarmkit-mon中内置了docker，并映射了Host上的docker.sock，
这样可以在swarmkit-mon容器中控制docker创建swarmkit集群。swarmkit-mon的实现比较简单，
主要包括两个shell脚本：run.sh负责swarmkit集群的管理和Rancher的联动，
agent节点信息需要通过rancher-metadata读取，设置Host Label则直接调用Rancher API；
health.sh负责监控swarmkit节点的状态（通过与docker.sock通信读取Swarm.LocalNodeState的状态），
并与giddyup协作暴露健康检查端口，这样可以利用Rancher Cattle的healthcheck来保证swarmkit-mon服务的高可用性，
每个Host的swarmkit-mon出问题时可以进行自动重建恢复。原理如图：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tKfTcjw1favaidbyfgj30bh08qmyh.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;目前来看，由于Kubernetes的发展的确迅猛，所以Rancher的更多精力都放在K8s上。
针对Swarmkit的支持显得略显单薄，但是Swarmkit本身的问题也很多，目前也难以应对复杂场景，
所以目前的支持力度应该是足够了。后续对docker1.13版本的Swarmkit支持也在持续迭代中。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>为什么Rancher v1.2中netstat看不到开放端口</title>
      <link>http://niusmallnan.com/2016/12/09/rancher-netstat-miss-port</link>
      <pubDate>Fri, 09 Dec 2016 15:07:14 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/12/09/rancher-netstat-miss-port</guid>
      <description>&lt;p&gt;拯救一脸懵逼。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;随着Rancher v1.2的发布，越来越多的伙伴参与到新特性的尝鲜中。有时不免会碰到一些问题，
比如网络不通之类的，这时通常我们都会下意识的使用netstat命令查看端口是否正确开启，
可是出现的结果却让伙伴们一脸疑问。比如我们使用了ipsec网络，
netstat命令却看不到UDP 500和4500端口的开放，这是为什么呢？本文将释疑这个问题。&lt;/p&gt;

&lt;h3 id=&#34;问题现象&#34;&gt;问题现象&lt;/h3&gt;

&lt;p&gt;启动之前的Rancher v1.1版本并对比v1.2版本，可以看到如下现象，
ipsec网络正常，但在新版中netstat却看不到端口开放：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tKfTcjw1fakk2e84hcj30d509z76g.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
用LB暴露一个服务访问，UI上明明显示了端口，但是在netstat中却看不到：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tKfTcjw1fakk3aasx6j30j7075my6.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
这些现象是为什么？&lt;/p&gt;

&lt;h3 id=&#34;原生docker和k8s可以看到端口&#34;&gt;原生docker和k8s可以看到端口&lt;/h3&gt;

&lt;p&gt;原先的docker，一般run一个容器加入-p参数，就可以看到端口。
这其实是由当前tcp/ip栈内的docker-proxy开放的，如图：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tKfTcjw1fakk45w280j30ju06t0v2.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
k8s是可以通过netstat看到端口，举个例子，创建一个service暴露NodePort，如下图：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tKfTcjw1fakk4l1qplj308z069wer.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
而通过netstat命令却能看到对应的端口，这是因为这个端口是由kube-proxy开放的，
就是说在这个tcp/ip栈内有应用程序bind了这个端口：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tKfTcjw1fakk4z18gzj30j605jtah.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;本质原因&#34;&gt;本质原因&lt;/h3&gt;

&lt;p&gt;端口的本质其实是应用程序，就是说需要有process bind port，
而netstat要能显示出开放端口，则必须在当前的namespace里有应用程序监听了端口才行。
Rancher中因为是使用CNI模型，虽然也是使用docker0，但是这个docker0是CNI Bridge，
并不和docker-proxy有协作，所以端口并没有通过docker-proxy暴露，
只是通过iptables dnat转发到其他的namespace上。在当前的namespace里使用netstat看不到开放的端口，
iptables dnat规则如下：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tKfTcjw1fakk5qzfr6j30jg03idh7.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;有时候惯性思维让我们忽略了事物的本质，让人匪夷所思的问题往往是很简单的道理。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>go-machine-service访问Rancher API的授权机制分析</title>
      <link>http://niusmallnan.com/2016/12/05/go-machine-service-access-api-key</link>
      <pubDate>Mon, 05 Dec 2016 11:14:19 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/12/05/go-machine-service-access-api-key</guid>
      <description>&lt;p&gt;一段分析调用机制的旅程。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;深更半夜一位道上的朋友突然微信问我，Rancher Server内Cattle之外的服务如何调用Rancher API。
调用API本身没问题，但是Rancher API是需要授权访问的，
也就是说Rancher Server内其他服务如何授权访问API，这个机制是什么样的？
Rancher Server内有很多服务，如catalog、go-machine-service、websocket-proxy等等，
本文就以go-machine-service为例说明其授权机制。&lt;/p&gt;

&lt;h3 id=&#34;以go-machine-service为例&#34;&gt;以go-machine-service为例&lt;/h3&gt;

&lt;p&gt;首先我们要了解go-machine-service在做什么事，官方文档里很清楚的描述到，
它实现了physicalhost相关事件的handler，主要包括：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;physicalhost.create - Calls machine create &amp;hellip;.&lt;/li&gt;
&lt;li&gt;physicalhost.activate - Runs docker run rancher/agent &amp;hellip;&lt;/li&gt;
&lt;li&gt;physicalhost.delete|purge - Calls machine delete &amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;而若实现这些handler，需要和rancher events打交道，
我们认为rancher events也是rancher api的一部分，
所以访问rancher events也需要和API服务一样的鉴权信息。
go-machine-service在启动之时，很明确的读取了CATTLE相关变量：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006y8lVajw1fafqwxo6pyj30ir0ccdhx.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;而且rancher还给go-machine-service专门创建了相应的API Key，查看DB可以得知：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006y8lVajw1fafqxbv1wbj30f90iwgnb.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;进入rancher-server容器内部，查看go-machine-service进程的环境变量，
可以看到API Key的授权信息就在其中：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006y8lVajw1fafqxofpl1j30m805dmz0.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;那么go-machine-service的环境变量是如何设置的呢？查看Cattle源码，找到MachineLancher，
它是启动go-machine-service服务的具体实现，我们可以看到设置环境变量这一步：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006y8lVajw1fafqy3ywfbj30gg03tt9w.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;Cattle本身就是一个框架，Rancher Server内的其他服务都是通过Cattle的Proxy机制来转发请求的，
授权机制则是由Cattle统一提供出来，其他服务自身无需单独实现授权机制。
如果我们想对Rancher Server扩展，再增加一些其他服务，利用本文所描述的机制，就会非常方便。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>实践指南-快速解锁Rancher v1.2</title>
      <link>http://niusmallnan.com/2016/12/02/rancher12-run-on-laptop</link>
      <pubDate>Fri, 02 Dec 2016 21:31:53 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/12/02/rancher12-run-on-laptop</guid>
      <description>&lt;p&gt;Rancher v1.2系列文章，已独家授权Rancher Labs，转载请联系Rancher China相关人员。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;Rancher v1.2已经发布，相信众多容器江湖的伙伴们正魔拳擦准备好好体验一番。
由于Docker能够落地的操作系统众多，各种Docker版本不同的Graph driver，
所以通常大版本的第一个release都会在兼容性上有一些小问题。
为了更好的体验Rancher v1.2的完整特性，我们选取了Rancher测试比较严格的运行环境。
手握众多服务器资源的devops们可以飘过此文，身背MBP或Windows笔记本的Sales/Pre-Sales们可以品读一番。&lt;/p&gt;

&lt;h3 id=&#34;基础软件安装&#34;&gt;基础软件安装&lt;/h3&gt;

&lt;p&gt;首先需要安装基础软件，由于Rancher v1.2已经支持Docker v1.2，
所以可以直接使用Docker的Mac或Windows版（以下以Mac为例），
下载地址：&lt;a href=&#34;https://www.docker.com/&#34;&gt;https://www.docker.com/&lt;/a&gt;。在Mac上，
Docker会使用xhyve轻量级虚拟化来保证一个Linux环境，所以可以把Rancher Server直接运行起来。&lt;/p&gt;

&lt;p&gt;因为要在MBP上添加多个Host组成小集群，所以需要用虚拟化扩展多个节点添加到Rancher集群中。
这里可以使用docker-machine控制VirtualBox来添加节点，
VirtualBox下载地址：&lt;a href=&#34;https://www.virtualbox.org/wiki/Downloads&#34;&gt;https://www.virtualbox.org/wiki/Downloads&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;在Host节点的操作系统上，可以选取RancherOS，我们的目标是快速体验新特性，
而Rancher Labs在Rancher和RancherOS的相互兼容性上是做了大量测试的，
这样可以避免我们少进坑，直接体验新特性。
RancherOS下载地址：&lt;a href=&#34;https://github.com/rancher/os&#34;&gt;https://github.com/rancher/os&lt;/a&gt;，推荐使用最新release版本。&lt;/p&gt;

&lt;p&gt;在用docker-machine驱动VirtualBox来创建Host时，可以指定操作系统ISO的URL路径，
由于我们使用RancherOS，所以最好把RancherOS放到本机HTTP服务器内。
MBP内自带Apache HTTPD，将Apache的vhosts模块开启，并添加配置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 开启vhost /etc/apache2/httpd.conf
# 以下两行的默认注释去掉
LoadModule vhost_alias_module libexec/apache2/mod_vhost_alias.so
Include /private/etc/apache2/extra/httpd-vhosts.conf

# vhost的配置 /etc/apache2/extra/httpd-vhosts.conf
# DocumentRoot目录就是在用户根目录下创建Sites
# 如用户名niusmallnan，则DocumentRoot就是/Users/niusmallnan/Sites
&amp;lt;VirtualHost *:80&amp;gt;
    DocumentRoot &amp;quot;/Users/niusmallnan/Sites&amp;quot;
    ServerName localhost
    ErrorLog &amp;quot;/private/var/log/apache2/sites-error_log&amp;quot;
    CustomLog &amp;quot;/private/var/log/apache2/sites-access_log&amp;quot;
    common
    &amp;lt;Directory /&amp;gt;
        Options Indexes FollowSymLinks MultiViews
        AllowOverride None
        Order allow,deny
        Allow from all
        Require all granted
    &amp;lt;/Directory&amp;gt;
&amp;lt;/VirtualHost&amp;gt;

# 重启 Apache
$ sudo apachectl restart

# 拷贝 RancherOS的ISO 到 DocumentRoot
$ cp rancheros.iso /Users/niusmallnan/Sites/
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;rancher安装&#34;&gt;Rancher安装&lt;/h3&gt;

&lt;p&gt;首先打开Docker，并配置registry mirror，配置完成后重启Docker。
mirror的服务可以去各个公用云厂商申请一个，比如我这里使用的是阿里云的registry mirror，
如图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tKfTcjw1fav9vw30d1j30jh082403.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;打开terminal，安装Rancher Server：&lt;br /&gt;
&lt;code&gt;$ docker run -d --restart=unless-stopped -p 8080:8080 rancher/server:stable&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;若要添加Host节点，则需要通过docker-machine创建Host，
这里使用的规格是2核2G（具体可根据自身MBP的性能调整），脚本（add_ros_host.sh）参考如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/usr/bin/env bash
ROS_ISO_URL=&#39;http://127.0.0.1/rancheros.iso&#39;
ROS_CPU_COUNT=2
ROS_MEMORY=2048
docker-machine create -d virtualbox \
        --virtualbox-boot2docker-url $ROS_ISO_URL \
        --virtualbox-cpu-count $ROS_CPU_COUNT \
        --virtualbox-memory $ROS_MEMORY \
        $1
docker-machine ls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;添加节点则需执行：&lt;br /&gt;
&lt;code&gt;$ ./add_ros_host.sh ros-1&lt;/code&gt;&lt;br /&gt;
添加完成后，可以进入虚机内进行设置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker-machine ls
NAME  ACTIVE DRIVER     STATE     URL                       SWARM DOCKER ERRORS 
ros-1 -      virtualbox Running   tcp://192.168.99.100:2376       v1.12.3

# 进入VM中
$ docker-machine ssh ros-1
# RancherOS内设置registry mirror
$ sudo ros config set rancher.docker.extra_args \
        &amp;quot;[&#39;--registry-mirror&#39;,&#39;https://s06nkgus.mirror.aliyuncs.com&#39;]&amp;quot;
$ sudo system-docker restart docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于我们要使用VirtualBox的虚机组成一个小集群，所以建议把Rancher的Host Registration URL
设置为&lt;a href=&#34;http://192.168.99.1:8080&#34;&gt;http://192.168.99.1:8080&lt;/a&gt;，如下图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tKfTcjw1fav9z6dih7j30eo072gmf.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
添加Rancher agent的时候也要注意，CATTLE_AGENT_IP参数要设置成虚机内192.168.99.0/24网段的IP，
如下图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tKfTcjw1fav9zo3g5zj30hp06tmyk.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
如此就可以基本完全解锁Rancher v1.2的各种功能了，完整演示各种特性。&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;Docker目前版本分支众多，虽然最新的v1.13即将发布，但是各个公司的使用版本应该说涵盖了v1.9到v1.12，
而且Docker graph driver也有很多，再加上很多的LinuxOS，可以说使用Docker而产生组合有很多种，
这就会带来各种各样的兼容性问题，因此导致的生产环境故障会让人头疼不已。
当然如果纯粹基于演示和调研新功能，我们可以优先兼容性较好的选择。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Rancher v1.2网络架构解读</title>
      <link>http://niusmallnan.com/2016/12/01/rancher12-networking</link>
      <pubDate>Thu, 01 Dec 2016 21:18:36 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/12/01/rancher12-networking</guid>
      <description>&lt;p&gt;Rancher v1.2系列文章，已独家授权Rancher Labs，转载请联系Rancher China相关人员。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;在之前的Rancher版本上，用户经常诟病Rancher的网络只有IPsec，没有其他选择。
而容器社区的发展是十分迅猛的，各种容器网络插件风起云涌，欲在江湖中一争高下。
Rancher v1.2版本中与时俱进，对之前的网络实现进行了改造，支持了CNI标准，
除IPsec之外又实现了呼声比较高的VXLAN网络，同时增加了CNI插件管理机制，
让我们可以hacking接入其他第三方CNI插件。本文将和大家一起解读一下Rancher v1.2中网络的实现。&lt;/p&gt;

&lt;h3 id=&#34;rancher-net-cni化&#34;&gt;Rancher-net CNI化&lt;/h3&gt;

&lt;p&gt;以最简单最快速方式部署Rancher并添加Host，以默认的IPsec网络部署一个简单的应用后，
进入应用容器内部看一看网络情况，对比一下之前的Rancher版本：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tKfTcjw1fav9di74b7j30iv06tgoo.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
我们最直观的感受便是，网卡名从eth0到eth0@if8有了变化，原先网卡多IP的实现也去掉了，
变成了单纯的IPsec网络IP。这其实就引来了我们要探讨的内容，虽然网络实现还是IPsec，
但是rancher-net组件实际上是已经基于CNI标准了。最直接的证明就是看一下，
rancher-net镜像的Dockerfile：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tKfTcjw1fav9e5komyj30ju06o40i.jpg&#34; alt=&#34;&#34; /&gt;
熟悉CNI规范的伙伴都知道/opt/cni/bin目录是CNI的插件目录，bridge和loopback也是CNI的默认插件，
这里的rancher-bridge实际上和CNI原生的bridge没有太大差别，只是在幂等性健壮性上做了增强。
而在IPAM也就是IP地址管理上，Rancher实现了一个自己的rancher-cni-ipam，它的实现非常简单，
就是通过访问rancher-metadata来获取系统给容器分配的IP。
Rancher实际上Fork了CNI的代码并做了这些修改，&lt;a href=&#34;https://github.com/rancher/cni&#34;&gt;https://github.com/rancher/cni&lt;/a&gt;。
这样看来实际上，rancher-net的IPsec和Vxlan网络其实就是基于CNI的bridge基础上实现的。&lt;/p&gt;

&lt;p&gt;在解释rancher-net怎么和CNI融合之前，我们需要了解一下CNI bridge模式是怎么工作的。
举个例子，假设有两个容器nginx和mysql，每个容器都有自己的eth0，
由于每个容器都是在各自的namespace里面，所以互相之间是无法通信的，
这就需要在外部构建一个bridge来做二层转发，
容器内的eth0和外部连接在容器上的虚拟网卡构建成对的veth设备，这样容器之间就可以通信了。
其实无论是docker的bridge还是cni的bridge，这部分工作原理是差不多的，如图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tKfTcjw1fav9ffzi1sj30ch08q750.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;那么我们都知道CNI网络在创建时需要有一个配置，这个配置用来定义CNI网络模式，
读取哪个CNI插件。在这个场景下也就是cni bridge的信息，
这个信息rancher是通过rancher-compose传入metadata来控制的。
查看ipsec服务的rancher-compose.yml可以看到，type使用rancher-bridge，
ipam使用rancher-cni-ipam，bridge网桥则复用了docker0，
有了这个配置我们甚至可以随意定义ipsec网络的CIDR，如下图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tKfTcjw1fav9g15ofmj30c80anab3.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;ipsec服务实际上有两个容器：一个是ipsec主容器，内部包含rancher-net服务和ipsec需要的charon服务；
另一个sidekick容器是cni-driver，它来控制cni bridge的构建。两端主机通过IPsec隧道网络通信时，
数据包到达物理网卡时，需要通过Host内的Iptables规则转发到ipsec容器内，
这个Iptables规则管理则是由network-manager组件来完成的，
&lt;a href=&#34;https://github.com/rancher/plugin-manager&#34;&gt;https://github.com/rancher/plugin-manager&lt;/a&gt;。其原理如下图所示（以IPsec为例）：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tKfTcjw1fav9gp2vkwj30hj06pacf.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
整体上看cni ipsec的实现比之前的ipsec精进了不少，而且也做了大量的解耦工作，
不单纯是走向社区的标准，之前大量的Iptables规则也有了很大的减少，性能上其实也有了很大提升。&lt;/p&gt;

&lt;h3 id=&#34;rancher-net-vxlan的实现&#34;&gt;Rancher-net vxlan的实现&lt;/h3&gt;

&lt;p&gt;那么rancher-net的另外一个backend vxlan又是如何实现的呢？
我们需要创建一套VXLAN网络环境来一探究竟，默认的Cattle引擎网络是IPsec，
如果修改成VXLAN有很多种方式，可以参考我下面使用的方式。&lt;/p&gt;

&lt;p&gt;首先，创建一个新的Environment Template，把Rancher IPsec禁用，同时开启Rancher VXLAN，如下图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tKfTcjw1fav9hpxm04j30ft09wmya.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
然后，我们创建一个新的ENV，并使用刚才创建的模版Cattle-VXLAN，创建完成后，
添加Host即可使用。如下图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tKfTcjw1fav9i7um3fj30d7083js5.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
以分析IPsec网络实现方式来分析VXLAN，基本上会发现其原理大致相同。
同样是基于CNI bridge，使用rancher提供的rancher-cni-bridge、rancher-cni-ipam，
网络配置信息以metadata方式注入。区别就在于rancher-net容器内部，
rancher-net激活的是vxlan driver，它会生成一个vtep1042设备，并开启udp 4789端口，
这个设备基于udp 4789构建vxlan overlay的两端通信，对于本机的容器通过eth0走bridge通信，对于
其他Host的容器，则是通过路由规则转发到vtep1042设备上，再通过overlay到对端主机，
由对端主机的bridge转发到相应的容器上。整个过程如图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tKfTcjw1fav9j4nhblj30e607z0um.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
vxlan实现的原理可更多参考：&lt;a href=&#34;https://www.kernel.org/doc/Documentation/networking/vxlan.txt&#34;&gt;https://www.kernel.org/doc/Documentation/networking/vxlan.txt&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;容器网络是容器云平台中很重要的一环，对于不通规模不通的安全要求会有不同的选型。
Rancher的默认网络改造成了CNI标准，同时也会支持其他第三方CNI插件，
结合Rancher独有的Environment Template功能，用户可以在一个大集群中的每个隔离环境内，
创建不同的网络模式，以满足各种业务场景需求，这种管理的灵活性是其他平台没有的。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Rancher v1.2基础设施引擎整体架构</title>
      <link>http://niusmallnan.com/2016/11/28/rancher12-infra-architect</link>
      <pubDate>Mon, 28 Nov 2016 20:53:44 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/11/28/rancher12-infra-architect</guid>
      <description>&lt;p&gt;Rancher v1.2系列文章，已独家授权Rancher Labs，转载请联系Rancher China相关人员。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;Rancher v1.2可以说是一个里程碑版本，发布时间虽屡次跳票让大家心有不爽，
但是只要体会其新版功能，会发现这个等待绝对是值得的。从架构角度看，
用两个字来概括就是“解耦”，基础设施引擎的分离，agent节点的服务粒度更细；
从产品角度看，给了用户更多定制的空间，Rancher依然秉持着全部OpenSource的理念；
在开发语言上，之前遗留的通过shell脚本方式的粗糙实现也都基于Golang重写，
解耦的新服务也几乎使用Golang开发，agent节点全线基于Golang这也为后续便利地支持ARM埋下伏笔。
在市场选择上，Rancher依然在kubernetes下面投入了大量精力，引入了万众期待的CNI plugin管理机制，
坚持要做最好用的Kubernetes发行版。本文就带着大家从架构角度总览Rancher v1.2版本的特性。&lt;/p&gt;

&lt;h3 id=&#34;总览&#34;&gt;总览&lt;/h3&gt;

&lt;p&gt;在v1.2版本的整体架构图（如下图所示）上，Cattle引擎向下深入演化成了基础设施引擎，
这一点上在v1.1时代也早有体现。Cattle更多得作为基础设施的管理工具，
可以用它来部署其他服务和编排引擎，当然它本身编排能力还是可以使用的，
习惯了stack-service的朋友仍然可以继续使用它，同时rancher scheduler的引入也大大增强了其调度能力。
Rancher仍然支持Kubernetes、Mesos、Swarm三大编排引擎，Kubernetes可以支持到较新的v1.4.6版本，
由于所有的部署过程的代码都是开放的，用户依然可以自己定制部署版本。值得一提的是，
Rancher支持了新版的Swarm Mode也就是Swarmkit引擎，这也意味着Rancher可以在Docker1.12上部署，
不小心装错Docker版本的朋友这回可以放心了。&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tKfTcjw1fav8rinilzj30hj09p0ur.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
在存储方面，Rancher引入了Kubernetes社区的flexvol来做存储插件的管理，
同时也支持Docker原生的volume plugin机制，并实现了对AWS的EFS&amp;amp;EBS以及标准NFS的支持，
先前的Convoy应该会被抛弃，Rancher最终还是选择参与社区标准。在网络方面，
除了CNI插件机制的引入，用户还可以使用rancher-net组件提供的vxlan网络替代先前的ipsec网络。
在可定制性方面，还体现在Rancher提供了用户可以自定义rancher-lb的机制，
如果特殊场景下默认的Haproxy不是很给力时，用户可以自定义使用nginx、openresty或者traefik等等。
下面便做一下详细分解。&lt;/p&gt;

&lt;h3 id=&#34;基础设施引擎&#34;&gt;基础设施引擎&lt;/h3&gt;

&lt;p&gt;初次安装v1.2版本，会发现多了Infrastructure（如下图所示）的明显标识，
默认的Cattle引擎需要安装healthcheck、ipsec、network-services、scheduler等服务。
这个是有rancher-catalog来定义的，&lt;a href=&#34;https://github.com/rancher/rancher-catalog&#34;&gt;https://github.com/rancher/rancher-catalog&lt;/a&gt;，
新分离出来了infra-templates和project-templates：infra-templates就是Rancher定义的各种基础设施服务，
包括基础服务和编排引擎；project-templates对应的是Env初始化时默认安装的服务，
它可以针对不同的编排引擎进行配置。&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tKfTcjw1fav8t4d321j30dl0b275b.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
以Cattle引擎为例，可以在project-templates的Cattle目录中找到相应的配置文件，
当ENV创建初始化时会创建这里面定义的服务，这样一个机制就可以让我们可以做更深入的定制，
让ENV初始化时创建我们需要的服务：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;name: Cattle
description: Default Cattle template
stacks:
- name: network-services
templateId: &#39;library:infra*network-services&#39;
- name: ipsec
templateId: &#39;library:infra*ipsec&#39;
- name: scheduler
templateId: &#39;library:infra*scheduler&#39;
- name: healthcheck
templateId: &#39;library:infra*healthcheck&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在Cattle引擎调度方面，Rancher实现了rancher-scheduler，&lt;a href=&#34;https://github.com/rancher/scheduler&#34;&gt;https://github.com/rancher/scheduler&lt;/a&gt;。
它实现了允许用户按计算资源调度，目前支持memory、cpu的Reservation。其实现原理是，
内部有一个resource watcher，通过监听rancher metadata的获取Host的使用资源数据变化，
进而得到ENV内所有Host资源汇总信息。与此同时，
监听rancher events的scheduler.prioritize、scheduler.reserve、scheduler.release等各种事件，
通过排序过滤可用主机后发送回执信息，Rancher Server就有了可以选择的Host列表。如下图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tKfTcjw1fav8vafyqaj30h005xwfk.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
需要注意的是，rancher-scheduler并没有和rancher-server部署在一起，
而是在你添加Host时候部署在agent节点上，当然rancher-scheduler在一个ENV内只会部署一个。&lt;/p&gt;

&lt;h3 id=&#34;agent节点服务解耦&#34;&gt;Agent节点服务解耦&lt;/h3&gt;

&lt;p&gt;agent节点一个最大的变化就是，agent-instance容器没有了，它被拆分成多个容器服务，
包括rancher-metadata、network-manager、rancher-net、rancher-dns、healthcheck等。&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tKfTcjw1fav8w3brm9j30ea06labt.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;metadata服务是老朋友了，它在每个agent节点上保存了host、stack、service、container等的信息，
可以非常方便的在本地调用取得，&lt;a href=&#34;https://github.com/rancher/rancher-metadata&#34;&gt;https://github.com/rancher/rancher-metadata&lt;/a&gt;，
在新的体系中它扮演了重要角色，几乎所有agent节点上的服务均依赖它。
在先前的体系中，metadata的answer file的更新是通过事件驱动shell脚本来执行下载，
比较简单粗暴。v1.2开始使用监听rancher events方式来reload metadata的answer file，
但是answer file还需要到rancher server端下载，总的来说效率还是有一定提升的。&lt;/p&gt;

&lt;p&gt;dns在新的体系中仍然承担着服务发现的功能，&lt;a href=&#34;https://github.com/rancher/rancher-dns&#34;&gt;https://github.com/rancher/rancher-dns&lt;/a&gt;。
除了拆分成单独容器之外，它也在效率上做了改进，它与rancher-metadata容器共享网络，
以metadata的结果生成dns的answer file。与之前的架构相比，
省去了dns answer file下载的过程。需要注意的是，rancher-dns的TTL默认是600秒，
如果出于各种原因觉得dns作为服务发现不是很可靠，那么可以使用etc-host-updater和rancher-metadata的组合，
&lt;a href=&#34;https://github.com/rancher/etc-host-updater&#34;&gt;etc-host-updater&lt;/a&gt;
会根据metadata数据动态生成hosts文件并写入容器内，这样通过服务名访问时，
其实已经在本地转换成了IP，无需经过dns，如下图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tKfTcjw1fav8z58gm6j30bc04mmxg.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;rancher-net作出了比较重大的革新，&lt;a href=&#34;https://github.com/rancher/rancher-net&#34;&gt;https://github.com/rancher/rancher-net&lt;/a&gt;，
除了继续支持原有的ipsec外，还支持了vxlan。这个支持是原生支持，
只要内核有vxlan的支持模块就可以。vxlan并不是Cattle的默认网络，
使用时可以在infra-catalog中重新选择它来部署，其实现以及部署方式后续会在专门的文章中进行探讨：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tKfTcjw1fav8zxs951j30ds0al753.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;network-manager的引入为Rancher v1.2带来了一个重要特性就是CNI插件管理，
在之前的版本中很多用户都提到rancher-net本身的网络无法满足业务需求。
容器网络之争，无非就是CNM与CNI，Rancher选择站队CNI，这也是为了更好与Kubernetes融合。
而CNI的插件很多种，Calico、Weave等之流，每个插件的部署方式都不一样。
Rancher为了简化管理提出了network-manager，&lt;a href=&#34;https://github.com/rancher/plugin-manager&#34;&gt;https://github.com/rancher/plugin-manager&lt;/a&gt;，
它可以做到兼容主流的CNI插件，它实际上定义了一个部署框架，让CNI插件在框架内部署。
network-manager是以容器方式部署，由于每种插件在初始化时可能需要暴露端口或加入一些NAT规则，
所以network-manager能够动态设置不同插件的初始化规则，
它的做法是以metadata作为 host port和host nat规则的数据源，
然后获取数据后生成相应的Iptables规则加入的Host中。而对于真正的CNI插件，
需要在network-manager容器内/opt/cni目录下部署对应cni插件的执行程序（calico/weave），
/etc/cni目录下部署cni插件的配置，这两个目录映射了docker卷rancher-cni-driver，
也就是Host上的/var/lib/docker/volumes/rancher-cni-driver目录下。&lt;/p&gt;

&lt;p&gt;关于healthcheck，先前是通过agent-instance镜像实现，里面内置了Haproxy，
事件驱动shell脚本来下载healchcheck配置并reload。新的架构中，
Rancher实现了单独的healthcheck，&lt;a href=&#34;https://github.com/rancher/healthcheck&#34;&gt;https://github.com/rancher/healthcheck&lt;/a&gt;，
采用Golang微服务的方式，数据源是metadata。
当然healthcheck的最终检查仍然是通过与Haproxy sock通信来查看相应member的健康状态（原理如下图），
healthcheck的实现主要是为了将其从agent-instance中解耦出来。&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tKfTcjw1fav928q6bwj30hu06mad4.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;Rancher v1.2的新特性还是非常多的，基础设施引擎的变化时一切特性的基础，
这篇文章算是开篇之作。后续会持续为大家带来，Kubernetes、Swarmkit的支持，自定义rancher-lb，
vxlan的支持，各种CNI插件的集成，以及各种存储接入的实践操作指南等等。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>基于容器实现树莓派的动态域名绑定</title>
      <link>http://niusmallnan.com/2016/11/22/rpi-noip-with-docker</link>
      <pubDate>Tue, 22 Nov 2016 13:59:30 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/11/22/rpi-noip-with-docker</guid>
      <description>&lt;p&gt;家庭使用树莓派场景中，如何给树莓派绑定一个域名，让我们一扫运营商动态IP的困扰，
可以轻轻松松在外网使用域名访问。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;我们在家里使用树莓派时，如果在上面搭建了一些服务，有时会期待能在外网可以访问。
家庭宽带会给我们分配一个外网IP，可以通过这个IP在公网上访问树莓派上的服务。
但是运营商提供的这个IP是非静态的，很可能在凌晨的时候会被重新分配，
而且IP也不是很好记，所以通常我们都希望能有一个域名可以直接访问。
动态域名解析是个老话题，国内最早花生壳就做过，具体原理不必多说。&lt;/p&gt;

&lt;h3 id=&#34;我们的需求清单如下&#34;&gt;我们的需求清单如下：&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;一个动态域名解析的软件，最好是一定程度Free。&lt;/li&gt;
&lt;li&gt;域名解析的client端一定要是开源的（谁也不想被当肉鸡&amp;hellip;）。&lt;/li&gt;
&lt;li&gt;部署控制要非常简单。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;最终选型如下&#34;&gt;最终选型如下：&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;使用国外&lt;a href=&#34;https://www.noip.com/&#34;&gt;noip&lt;/a&gt;的服务，免费账户可以绑定三个免费域名，
每个域名30天有效，失效后需要手动添加回来。这种程度对于我这种玩家已经完全足够了。&lt;/li&gt;
&lt;li&gt;noip的client端支持各种平台，支持&lt;a href=&#34;https://www.noip.com/download?page=linux&#34;&gt;Linux&lt;/a&gt;，同时是开源。&lt;/li&gt;
&lt;li&gt;为了让部署变得更加简洁，决定使用Docker容器来部署。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;执行过程&#34;&gt;执行过程&lt;/h3&gt;

&lt;p&gt;首先，需要到&lt;a href=&#34;https://www.noip.com/&#34;&gt;noip&lt;/a&gt;上注册账户，并填写自己的域名，注册过程请自行体验，比如：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tNc79jw1fa0utbt10oj30ja04maap.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后，我们就需要在下载客户端，并在树莓派上进行编译，生成适合ARM运行的版本，编译只要执行make即可：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tNc79jw1fa0uxw14ulj30fk0a4wgz.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;初次执行需要先在client端注册账号信息，其实就是生成一下相关配置文件，再次执行就可以运行起来了。&lt;/p&gt;

&lt;p&gt;这么看来，对于很多用户来说还是太复杂，所以我们决定使用容器来简化这个过程。
可以把上面繁琐操作，全部放在容器中，为了精简程序的编译环境，
我们使用alpine-linux来进行编译，容器的Dockerfile如下：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tNc79jw1fa0uz7rv7oj30i304pmyk.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;接着，我们需要对树莓派的系统进行容器化，以便我们可以运行noip容器程序，此时可以有三种选择：
1. RancherOS
2. HypriotOS
3. Rasbian 基础上安装Docker&lt;/p&gt;

&lt;p&gt;前两个都是内置Docker Engine的，用起来比较方便，Rasbian需要自行安装Docker。我选择的OS是RancherOS。&lt;/p&gt;

&lt;h3 id=&#34;最简方式部署noip-只需三步&#34;&gt;最简方式部署noip，只需三步&lt;/h3&gt;

&lt;p&gt;在RancherOS上修改registry mirror后，可以加速镜像下载，然后拉取前面Dockerfile编译的镜像：&lt;br /&gt;
&lt;code&gt;$ docker pull hypriot/rpi-noip&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;注册noip client，按照提示添加用户名密码等信息：&lt;br /&gt;
&lt;code&gt;$ docker run -ti -v noip:/usr/local/etc/ hypriot/rpi-noip noip2 -C&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;运行noip clent：&lt;br /&gt;
&lt;code&gt;$ docker run -v noip:/usr/local/etc/ --restart=always hypriot/rpi-noip&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;登录No-IP的&lt;a href=&#34;https://my.noip.com/#!/dynamic-dns&#34;&gt;控制台&lt;/a&gt;可以看到 dns绑定情况：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tNc79jw1fa0v2h5umhj30km05rjs0.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在本地使用dig确认一下解析情况：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tNc79jw1fa0v2po6wfj30gy08yjt4.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;以上所有源码可以参考：&lt;a href=&#34;https://github.com/hypriot/rpi-noip&#34;&gt;https://github.com/hypriot/rpi-noip&lt;/a&gt;。
这样，我们就在树莓派上非常简单的实现了动态域名绑定。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>关于我</title>
      <link>http://niusmallnan.com/aboutme/</link>
      <pubDate>Tue, 22 Nov 2016 12:07:35 +0800</pubDate>
      
      <guid>http://niusmallnan.com/aboutme/</guid>
      <description>&lt;p&gt;
张智博（niusmallnan），笔名牛小腩，初出茅庐在阿里巴巴口碑网，参与本地搜索业务研发工作，
后与朋友联合创办美食点评社区“美食行”，之后在各种公司从事云计算研发工作。&lt;/p&gt;

&lt;p&gt;喜欢参与技术社区活动，曾担任：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;OpenStack中国社区长期作者&lt;/li&gt;
&lt;li&gt;InfoQ兼职编辑&lt;/li&gt;
&lt;li&gt;Rancher中国社区布道师&lt;/li&gt;
&lt;li&gt;MBH树莓派社区成员&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;热爱coding，热爱技术分享，技术宅&amp;amp;科幻粉。&lt;/p&gt;

&lt;p&gt;简历更多信息，请参考&lt;a href=&#34;https://cn.linkedin.com/in/niusmallnan&#34;&gt;LinkedIn&lt;/a&gt;。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>树莓派上的Docker集群管理</title>
      <link>http://niusmallnan.com/2016/11/07/docker-on-rpi</link>
      <pubDate>Mon, 07 Nov 2016 17:36:37 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/11/07/docker-on-rpi</guid>
      <description>&lt;p&gt;随着IOT市场的火热发展，Docker天然的轻量级以及帮助业务快速重构的特性，
将会在IOT领域迎来巨大发展潜力，甚至有可能会比它在云端的潜力更大。
本文将致力于构建一个利用RancherOS来管理运行在树莓派上的容器集群。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;目前业界主流基本都是在x86架构上使用Docker，除了因为Intel在服务器领域的绝对领导地位之外，
x86 CPU的确在性能上有着卓越的表现。但是近些年来，随着云计算的迅猛发展，
引来了数据中心的大规模建设，慢慢地大家对数据中心PUE尤其是CPU功耗有了更高的要求。
ARM CPU虽然性能不如x86，但是在功耗上绝对有着无法比拟的优势，
同时我们知道并不是所有的服务都有高性能的CPU需要。很多厂商在都对ARM服务器投入了研发资源，
但是效果上目前来看并不是太好，ARM处理器在服务器领域并没有如在移动端那样被快速接受，
主要是因为市场接受度及服务器市场的性能要求所致。&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/7853084cjw1fa26h172hoj20ib09dgmz.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;但是在物联网（IOT）领域，ARM处理器却是霸主级的地位，毕竟在这个领域功耗胜过一切。
那么我们可以想象，未来会占大市场的IOT设备中，会出现各种尺寸各种架构，
内置操作系统也不统一，没有通用程序打包标准，几乎每种设备程序的开发框架均不同，
IOT设备中程序部署升级回滚等操作不够灵活，等等这样那样的问题。&lt;/p&gt;

&lt;h3 id=&#34;分析与实践&#34;&gt;分析与实践&lt;/h3&gt;

&lt;p&gt;这些问题，我们可以借鉴X86时代的经验，用Docker容器来解决它们。
Docker能降低IOT应用管理的负载度，但是在物理设备和Docker之间，
我们还需要一个轻量级的操作系统。这个OS需要是完全可以定制的，
可以针对不同设备需求，裁剪或增加对应的程序模块，更小体积更少进程意味着更低的功耗。&lt;/p&gt;

&lt;p&gt;根据以上判断和需求，我经过了一番探索，最终选择了RancherOS。它本身的特性是：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;真正容器化的Linux操作系统极致精简，所有服务（包括系统服务）均运行在容器中，
可以以容器方式对其进行任意定制&lt;/li&gt;
&lt;li&gt;内置了Docker Engine，无需在安装系统后再进行Docker安装&lt;/li&gt;
&lt;li&gt;完全开源&lt;a href=&#34;https://github.com/rancher/os&#34;&gt;https://github.com/rancher/os&lt;/a&gt;，我们可以进行各种深度定制&lt;/li&gt;
&lt;li&gt;最最重要的，支持ARM&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在RancherOS的整体架构中，最底层毋庸置疑是Linux kernel，系统启动后的PID 1用system-docker代替，
由它来把udev、dhcp、console等系统服务启动，同时会启动user-docker，
用户运行的应用程序均跑在user-docker下。&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/7853084cjw1fa26kta7rkj20l109s3zt.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我们以树莓派为例，将RancherOS部署在其之上。这里需要提示的是，RancherOS每个版本release之时，
都会放出树莓派的支持版本，
比如本次分享使用的&lt;a href=&#34;https://github.com/rancher/os/releases/
download/v0.7.0/rancheros-raspberry-pi.zip&#34;&gt;v0.7.0版本&lt;/a&gt;。通过dd命令将RancherOS写到树莓派的SD卡上，
通电点亮树莓派。&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/7853084cjw1fa26m5gaz0j20aq0aawg4.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;查看PID 1是system-docker：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/7853084cjw1fa26mgfy51j20fn03ldh0.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;通过system-docker ps 查看启动的系统服务：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/7853084cjw1fa26mp3zphj20s603u0uw.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;正常来说，我们都得设置一下docker registry mirror，这样方便pull镜像。
RancherOS的配置，都是通过ros config命令来配置，比如设置user-docker的mirror：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo ros config set rancher.docker.extra_args [--registry-mirror,https://xxxxxxx]
$ sudo system-docker restart docker # 重启user-docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最终，可以看到：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/7853084cjw1fa26nqlvixj20wf02ydi4.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;RancherOS有一个我认为比较好的特性，就是支持很方便的对Docker Engine版本进行切换。
目前Docker迭代的速度并不慢，实际上很多程序不一定会兼容比较新的Engine，
Docker Engine版本的管理变得越来越重要。尤其是在测试环境中，
我们有时确实需要变换Docker Engine版本，来构建测试场景：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo ros engine list  #查看当前版本支持的engine有哪些
disabled docker-1.10.3
disabled docker-1.11.2
current  docker-1.12.1
$ sudo ros engine switch docker-1.11.2 #切换docker-1.11版本
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此外，如果对docker engine有更特殊的需求，还可以定制自己的版本，然后让system-docker来加载它。
只需将编译好的docker engine放到scrach镜像中即可：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/7853084cjw1fa26p7gcphj20fx06fmy1.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
这部分可以参考：&lt;a href=&#34;https://github.com/rancher/os-engines&#34;&gt;https://github.com/rancher/os-engines&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;另外，如果习惯了使用相应Linux发行版的命令行，
那么也可以加载对应的console镜像（当然如果考虑精简系统也可不必加载）：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/7853084cjw1fa26pvzryej20by03pdg5.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
此部分需要进行深度定制，可以参考：&lt;a href=&#34;https://github.com/rancher/os-images&#34;&gt;https://github.com/rancher/os-images&lt;/a&gt;&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/7853084cjw1fa26qd06b7j20ej0bm410.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;RancherOS更多酷炫的功能，可以访问官方的文档：&lt;a href=&#34;http://docs.rancher.com/os&#34;&gt;http://docs.rancher.com/os&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;RancherOS介绍完毕后，我们可以在单机树莓派上做容器管理了，喜欢命令行的当然最好，
喜欢UI管理的，推荐两款可以在树莓派上运行的管理程序。
&lt;strong&gt;portainer&lt;/strong&gt;&lt;a href=&#34;https://github.com/portainer/portainer&#34;&gt;https://github.com/portainer/portainer&lt;/a&gt;，
其有专门的arm镜像portainer/portainer:arm ，运行后：
&lt;code&gt;$ docker run --restart=always -d -p 9000:9000 --privileged 
    -v /var/run/docker.sock:/var/run/docker.sock 
    portainer/portainer:arm&lt;/code&gt;&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/7853084cjw1fa26s6xjj2j20oz0e2jtp.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;更加简约的 ui-for-docker &lt;a href=&#34;https://github.com/kevana/ui-for-docker&#34;&gt;https://github.com/kevana/ui-for-docker&lt;/a&gt; ，运行如下：
&lt;code&gt;$ docker run --restart=always -d -p 9000:9000 
    -v /var/run/docker.sock:/var/run/docker.sock 
    hypriot/rpi-dockerui&lt;/code&gt;&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/7853084cjw1fa26sxv8ofj20ot0g0abm.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;单机树莓派之后，我们就要考虑如何将多个树莓派组成Docker集群。
一提到Docker集群，我们就会考虑需要编排引擎的支持，
无非就是主流的Mesos、Kubernetes、Swarm，还有非主流的Cattle、Nomad之流。
那么在IOT场景下，我们最需要考虑的就是精简，所以我选择了新版的Swarm。
将RancherOS的Engine切换到1.12.3，然后构建Swarm集群。&lt;/p&gt;

&lt;p&gt;简单得执行swarm init和join后，我们就得到了一个树莓派Docker集群：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/7853084cjw1fa26tjxxlyj20fi01twey.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后我们可以快速执行一个小demo：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#swarmkit demo
$ docker service create --replicas 1 -p 80 --name app armhf/httpd
$ docker service scale app=2
$ docker service ps app
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://ww3.sinaimg.cn/large/7853084cjw1fa26u6875lj20jk01y751.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;更多ARM相关的Docker镜像，可以到这两个地方查找：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://hub.docker.com/r/armhf&#34;&gt;https://hub.docker.com/r/armhf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hub.docker.com/u/aarch64&#34;&gt;https://hub.docker.com/u/aarch64&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;RancherOS设计之初是为了构建一个运行Rancher的轻量级操作系统，
那么Rancher本身在ARM的支持上也在不断推进中，
相应的&lt;a href=&#34;https://github.com/rancher/rancher/pull/4704&#34;&gt;PR&lt;/a&gt;也有提交。
不过目前来看，对rancher-server的ARM化还是比较麻烦，对agent的节点支持ARM相对简单一些，
也就是说rancher-server仍然运行在x86架构上，而agent节点可以支持ARM和x86。&lt;/p&gt;

&lt;p&gt;Kubernetes的ARM支持在社区中也有很多人在做，比如：
&lt;a href=&#34;https://github.com/luxas/kubernetes-on-arm&#34;&gt;https://github.com/luxas/kubernetes-on-arm&lt;/a&gt;，来自社区的分享：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/7853084cjw1fa26w8ifrrj20i00h577r.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;秀一下，我的“家庭树莓派数据中心”：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/7853084cjw1fa26wjj274j20c50chmzj.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;最后，我要特别感谢RancherOS的开发者们，他们帮助我解决了很多问题；
另外还要特别感谢MBH树莓派社区的伙伴，提供了硬件设备，支持我的技术探索，
并提供了很多帮助。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>如何hack一下rancher k8s</title>
      <link>http://niusmallnan.com/2016/10/08/rancher-k8s-hacking</link>
      <pubDate>Sat, 08 Oct 2016 16:09:24 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/10/08/rancher-k8s-hacking</guid>
      <description>&lt;p&gt;Rancher可以轻松实现Kubernetes的部署，尽管默认的部署已经完全可用，
但是如果我们想修改部署的K8s版本，这时应当如何应对？
&lt;/p&gt;

&lt;h3 id=&#34;原理分析与执行&#34;&gt;原理分析与执行&lt;/h3&gt;

&lt;p&gt;在Rancher中，由于K8s是基于Cattle引擎来部署，所以在K8s在部署完成之后，
我们可以通过Link Graph来很清晰的看到整体的部署情况。&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tNc79jw1fa0ybyprq4j30mi07tdh0.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;既然基于Cattle引擎部署，也就是说需要两个compose文件，
k8s引擎的compose文件放在&lt;a href=&#34;https://github.com/rancher/rancher-catalog/tree/master/templates&#34;&gt;https://github.com/rancher/rancher-catalog/tree/master/templates&lt;/a&gt;下面，
这里面有两个相关目录kubernetes与k8s，k8s是Rancher1.2开始使用的，
而kubernetes则是Rancher1.2之后开始使用的。&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tNc79jw1fa0ydzd1dgj30gt07qmyg.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;为了我们可以自己hack一下rancher k8s的部署，我们可以在github上fork一下rancher-catalog，
同时还需要修改一下Rancher中默认的catalog的repo地址，
这个可以在&lt;a href=&#34;http://rancher-server/v1/settings&#34;&gt;http://rancher-server/v1/settings&lt;/a&gt;页面下，
寻找名为 catalog.url 的配置项，然后进入编辑修改。比如我这里将library库的地址换成了自己的：
&lt;a href=&#34;https://github.com/niusmallnan/rancher-catalog.git&#34;&gt;https://github.com/niusmallnan/rancher-catalog.git&lt;/a&gt;&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tNc79jw1fa0yff7sbzj30h206imyy.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;此时，我们就可以修改了，找一个比较实用的场景。我们都知道k8s的pod都会依赖一个基础镜像，
这个镜像默认的地址是被GFW挡在墙外了，一般我们会把kubelet的启动参数调整一下，
以便重新指定这个镜像地址，比如指定到国内的镜像源上。&lt;br /&gt;
&lt;strong&gt;&amp;ndash;pod-infra-container-image=index.tenxcloud.com/google_containers/pause:2.0&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;如果我们要让rancher k8s部署时自动加上该参数，
可以直接修改私有rancher-catalog中的k8s compose文件。&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tNc79jw1fa0ygscal6j30gv0asjtp.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;修改之后稍等片刻（主要是为了让rancher-server更新到新的catalog compose文件），
添加一个k8s env并在其中添加host，k8s引擎就开始自动部署，
部署完毕后，我们可以看到Kubernetes Stack的compose文件，
已经有了&amp;ndash;pod-infra-container-image这个启动参数。&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tNc79jw1fa0yhg3alsj30gn09rtav.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
如此我们在添加pod时再也不用手动导入pod基础镜像了。&lt;/p&gt;

&lt;p&gt;在compose file中，部署k8s的基础镜像是rancher/k8s，这个镜像的Dockerfile在rancher维护的k8s分支中，
如在rancher-k8s 1.2.4分支中可以看到：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tNc79jw1fa0yi5rmplj30e007ogmr.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这样如果想对rancher-k8s发行版进行深度定制，就可以重新build相关镜像，通过rancher-compose来部署自己的发行版。&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;本文写于Rancher1.2行将发布之际，1.2版本是非常重大的更新，Rancher会支持部署原生的K8s版本，
同时CNI网络和Cloud Provider等都会以插件方式，用户可以自己定义，并且在UI上都会有很好的体现。
只要了解Rancher部署K8s的原理和过程，我们就可以定制非常适合自身使用的k8s，
通过Rancher来部署自定义的k8s，我们就可以很容易的扩展了k8s不擅长的UI、Catalog、
用户管理、审计日志维护等功能，这也是本文的目的。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Rancher HA部署实践</title>
      <link>http://niusmallnan.com/2016/09/23/rancher-ha-practice</link>
      <pubDate>Fri, 23 Sep 2016 17:12:27 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/09/23/rancher-ha-practice</guid>
      <description>&lt;p&gt;Rancher1.1版本HA结构部署实践记录
&lt;/p&gt;

&lt;h3 id=&#34;过程记录&#34;&gt;过程记录&lt;/h3&gt;

&lt;p&gt;以三节点为例，节点信息：
ubuntu(aufs)+docker1.11.2+rancher1.1.x
Haproxy+Mysql 放在同一节点 使用VM
Rancher HA Node 三个节点 使用baremetal（也可以使用VM，建议使用4核8G以上flavor）&lt;/p&gt;

&lt;p&gt;先部署Haproxy：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d --name rancher-haproxy \
           -v /opt/conf/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro \
           -p 80:80 \
           haproxy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;文件 /opt/conf/haproxy.cfg参考：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;global
        log 127.0.0.1 local0
        log 127.0.0.1 local1 notice
        maxconn 4096
        maxpipes 1024
        daemon

defaults
        log     global
        mode    tcp
        option  tcplog
        option  dontlognull
        option  redispatch
        option http-server-close
        option forwardfor
        retries 3
        timeout connect 5000
        timeout client 50000
        timeout server 50000

frontend default_frontend
        bind *:80
        mode http

        default_backend rancher-ha-node

backend rancher-ha-node
        mode http
        server r-ha-1 xx.xx.xx.xx
        server r-ha-2 xx.xx.xx.xx
        server r-ha-3 xx.xx.xx.xx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建容器化的mysql：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run --name rancher-mysql \
    -v /opt/data/mysql:/var/lib/mysql \
    -e MYSQL_ROOT_PASSWORD=root \
    -p 3306:3306 \
    -d mysql:5.5

mysql -uroot -proot
CREATE DATABASE IF NOT EXISTS cattle COLLATE = &#39;utf8_general_ci&#39; CHARACTER SET = &#39;utf8&#39;;
GRANT ALL ON cattle.* TO &#39;cattle&#39;@&#39;%&#39; IDENTIFIED BY &#39;cattle&#39;;
GRANT ALL ON cattle.* TO &#39;cattle&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;cattle&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;正常运行rancher，配置并下载HA脚本，如果不想用HTTPS，
记得把&lt;strong&gt;CATTLE_HA_HOST_REGISTRATION_URL&lt;/strong&gt;的值换成HTTP的，再执行脚本。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>CNM macvlan实践</title>
      <link>http://niusmallnan.com/2016/09/19/docker-cnm-practice</link>
      <pubDate>Mon, 19 Sep 2016 17:22:00 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/09/19/docker-cnm-practice</guid>
      <description>&lt;p&gt;针对Dcoker原生网络模型CNM的一次实践记录，网络模式macvlan。&lt;/p&gt;

&lt;p&gt;
首先测试一下系统对bridge和namespace的支持情况：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# add the namespaces
ip netns add ns1
ip netns add ns2

# create the macvlan link attaching it to the parent host eno1
ip link add mv1 link eno1 type macvlan mode bridge
ip link add mv2 link eno1 type macvlan mode bridge

# move the new interface mv1/mv2 to the new namespace
ip link set mv1 netns ns1
ip link set mv2 netns ns2

# bring the two interfaces up
ip netns exec ns1 ip link set dev mv1 up
ip netns exec ns2 ip link set dev mv2 up

# set ip addresses
ip netns exec ns1 ifconfig mv1 192.168.1.50/24 up
ip netns exec ns2 ifconfig mv2 192.168.1.60/24 up

# show interface detail
ip netns exec ns1 ip a
ip netns exec ns2 ip a

# ping from one ns to another
ip netns exec ns1 ping -c 4 192.168.1.60

# cleanup
ip netns del ns1
ip netns del ns2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;给网卡创建一个vlan设备：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# create a new subinterface tied to dot1q vlan 1045
ip link add link eno1 name eno1.1045 type vlan id 1045

# assign an IP addr
ip a add 192.168.120.24/24 dev eno1.1045

# enable the new sub-interface
ip link set eno1.1045 up

# remove sub-interface
#ip link del eno1.1045
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建docker network：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker network create -d macvlan \
    --subnet=192.168.225.0/24 \
    --gateway=192.168.225.1 \
    -o parent=eno1.1045 macvlan1045
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建容器，测试网络连通性：
&lt;code&gt;docker run --net=macvlan1045 --rm --ip=192.168.225.24 -it alpine /bin/sh&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Rancher event机制及其实践指南</title>
      <link>http://niusmallnan.com/2016/08/25/rancher-envent</link>
      <pubDate>Thu, 25 Aug 2016 13:44:04 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/08/25/rancher-envent</guid>
      <description>&lt;p&gt;Rancher在v1.0版本为了Kubernetes的集成，开始扩展并逐步完善了Rancher event机制，
event机制可以完成技术架构上的解耦。那么Rancher都提供了哪些event？
Rancher内部是如何使用的？以及我们如何用event来增强一些定制服务？
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;我们的Rancher官方技术社区已经创立些许时日了，相信通过我们的线下meetup和线上布道工作，
很多朋友对Rancher的使用已经掌握得很纯熟了。一些高级用户开始真正把自己的业务进行微服务化并向Rancher迁移，
在迁移的过程中，由于业务本身的复杂性特殊性，
可能需要利用Rancher的一些高级特性甚至要对Rancher进行一定的扩展，
这就需要对Rancher的一些组件的实现机制有些许了解。&lt;/p&gt;

&lt;p&gt;本次分享就介绍一下Rancher的event机制，由于相关内容文档极其欠缺，本人也只是通过实践和代码阅读分析其原理，
如有谬误欢迎交流指正，小腩双手红包奉上！同时为保证收视率，本次分享会以原理+实践的方式进行。&lt;/p&gt;

&lt;h3 id=&#34;原理分析&#34;&gt;原理分析&lt;/h3&gt;

&lt;p&gt;在大规模系统架构中，event机制通常采用消息驱动 ，它对提升分布式架构的容错性灵活性有很大帮助，
同时也是各个组件之间解耦的利器。Rancher能够管理N多的agent同时又拆分出各种服务组件，
event机制是必不可少的。为实现event机制，通常我们会采用RabbitMQ、ActiveMQ、ZeroMQ等中间件来实现。
而Rancher则采用了基于websocket协议的一种非常轻量级的实现方式，
它的好处就是极大程度的精简了Rancher的部署，Rancher无需额外维护一个MQ集群，
毕竟websocket的消息收发实现是非常简单的，各种语言库均可以支持。&lt;/p&gt;

&lt;p&gt;这里我们会考虑一个问题，websocket毕竟不是真正工业级MQ的实现，消息不能持久化，
一旦某个event的处理出现问题，或者发生消息丢失，Rancher如何保证各个资源的原子性一致性？
Rancher中有一个processpool的概念，它可以看做一个所有event的执行池，
当API/UI/CLI有操作时，Rancher会把操作分解成多个event并放入processpool中。
比如删除一个容器时会把 compute.instance.remove 放入processpool中，
这个event会发送到对应的host agent上，agent处理完成后会发送reply给rancher-server。
如果在这个过程中，由于网络问题消息丢失，或者agent上执行出现问题，
rancher-server没有收到reply信息，cattle会把这个event重新放到processpool中再次重复上面的过程，
直到 compute.instance.remove 完成操作，这个容器的状态才会在DB中更新，
否则该容器状态会一直处于lock不能被其他服务更新。当然cattle不会把这些event不停的重复执行下去，
通常会设置一下TIMEOUT超出后便不再执行（有些资源没有TIMEOUT机制）。&lt;/p&gt;

&lt;p&gt;上面的表述，我们其实可以在UI上看到这个过程，RancherUI上的Processes的Running Tab页上就能实时得看到这些信息，
Processes 在排查一些Rancher的相关问题是非常有用的，大家可以养成 ”查问题先查Processes“的好习惯：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/7853084cjw1fa1zux9y7bj20ly091dh6.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
那么监听event的URL怎么设定呢？非常简单：&lt;br /&gt;
&lt;code&gt;ws://&amp;lt;rancher-server-ip&amp;gt;:8080/v1/projects/&amp;lt;projectId&amp;gt;/subscribe?eventNames=xxxx&lt;/code&gt;&lt;br /&gt;
除此之外还需要加上basic-auth的header信息：&lt;br /&gt;
&lt;code&gt;Authorization: Basic +base64encode(&amp;lt;cattle-access-key&amp;gt;:&amp;lt;cattle-secret-key&amp;gt;)&lt;/code&gt;&lt;br /&gt;
如果是Host上的agent组件，除此之外还需要添加agentId参数：&lt;br /&gt;
&lt;code&gt;ws://&amp;lt;rancher-server-ip&amp;gt;:8080/v1/projects/&amp;lt;projectId&amp;gt;/subscribe?eventNames=xxxx&amp;amp;agentId=xxxx&lt;/code&gt;&lt;br /&gt;
agentId 是注册Host时生成的，如果没有agentId参数，
任何有关无关的event都会发送到所有的Host agent上，这样就会发生类似“广播风暴”的效果。&lt;/p&gt;

&lt;p&gt;Host agent上运行很多组件，其中python-agent是负责接收和回执event信息的，
其运行日志可以在Host上的/var/log/rancher/agent.log文件中查看。
细心的朋友可能会有疑问，我们在添加Host时执行agent容器时并没有指定cattle-access-key和cattle-secret-key，
也就是说python-agent运行时如何获取这两个秘钥信息呢？&lt;/p&gt;

&lt;p&gt;其实Rancher有两种apikey：一种是我们熟知的在UI上手动创建的apikey；
另外一种就是agentApikey，它是系统级的，专门为agent设定，
添加Host时会先把agentApikey发送到Host上。在cattle的credential表中可以查询到相关信息：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/7853084cjw1fa1zxokn7dj20ea05774u.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;eventNames都定义了哪些呢？下面两个文件可以参考：&lt;br /&gt;
* &lt;a href=&#34;https://github.com/rancher/cattle/blob/master/code/iaas/events/src/main/java/io/cattle/platform/iaas/event/IaasEvents.java&#34;&gt;IaasEvents.java&lt;/a&gt;
* &lt;a href=&#34;https://github.com/rancher/cattle/blob/master/code/packaging/app-config/src/main/resources/META-INF/cattle/process/spring-process-context.xml&#34;&gt;系统级的event定义&lt;/a&gt;，详细到每种资源(host、volume、instance、stack、service等)的event定义。&lt;/p&gt;

&lt;p&gt;此外，我们在UI/CLI/API上的某个操作都会被分解成多个event来执行，
每个event信息都会被保存在mysql中，每个event执行成功后会设置成purged状态，
所以记录并不会真正删除，这就会导致相应的表（container_event表、service_event表、process_instance表）会无限膨胀下去。
Rancher为解决此问题提供了周期性清理机制：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;events.purge.after.seconds 可以清理container_event和service_event，每两周清理一次&lt;/li&gt;
&lt;li&gt;process_instance.purge.after.seconds可以清理process_instance，每天清理一次。&lt;br /&gt;
这两个配置我们都可以在&lt;a href=&#34;http://rancher-server-ip:8080/v1/settings&#34;&gt;http://rancher-server-ip:8080/v1/settings&lt;/a&gt;动态修改。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;实践操作&#34;&gt;实践操作&lt;/h3&gt;

&lt;p&gt;下面我们来实践一下，看看如何在程序中实现对Rancher event的监听。
Rancher提供了resource.change事件，这个事件是不用reply的，就是说不会影响Rancher系统的运行，
它是专门开放给开发者用来实现一些自己的定制功能，所以我们就以resource.change作为例子实践一下。&lt;/p&gt;

&lt;p&gt;Rancher的组件大部分都是基于Golang编写，所以我们也采用相同的语言。
为了能够快速实现这个程序，我们需要了解一些辅助快速开发的小工具。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Trash，Golang package管理的小工具，可以帮助我们定义依赖包的路径和版本，非常轻量且方便；&lt;/li&gt;
&lt;li&gt;Dapper，这个是基于容器实现Golang编译的工具，主要是可以帮助我们统一编译环境；&lt;/li&gt;
&lt;li&gt;Go-skel，它可以帮我们快速创建一个Rancher式的微服务程序，可以为我们省去很多的基本代码，
同时还集成了Trash和Dapper这两个实用的小工具。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;首先我们基于go-skel创建一个工程名为 scale-subscriber （名字很随意），执行过程需要耐心的等待：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/7853084cjw1fa2044jpu9j20im0ce78n.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;执行完毕后，我们把工程放到GOPATH中，开始添加相关的逻辑代码。
在这之前我们可以考虑添加一个healthcheck的服务端口，纵观Rancher所有的微服务组件，
基本上除了主程序之外都会添加 healthcheck port，这个主要是为了与Rancher中的healthcheck功能配合，
通过设定对这个端口的检查机制来保证微服务的可靠性。我们利用Golang的goroutine机制，
分别添加主服务和healthcheck服务：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/7853084cjw1fa2053h2k7j20i908u0uu.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;主服务的核心就是监听resource.change的信息，同时注册handler来获取event的payload信息，
从而可以定制扩展自己的逻辑，这里需要利用Rancher提供的
&lt;a href=&#34;https://github.com/rancher/event-subscriber&#34;&gt;event-subsriber&lt;/a&gt;库来快速实现。
如下图，在&lt;strong&gt;reventhandlers.NewResourceChangeHandler().Handler&lt;/strong&gt;中就可以实现自己的逻辑：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/7853084cjw1fa206sahqtj20ka07habz.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这里我们只是演示监听event的机制，所以我们就不做过多的业务逻辑处理，打印输出event信息即可。
然后在项目根目录下执行make，make会自动调用dapper，在bin目录下生成scale-subscriber，
我们执行scale-subscriber就可以监听resource.change的信息：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/7853084cjw1fa207fymwlj20u904gdhf.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这里我们可以看到分别启动了healthcheck功能和event listener。然后可以在UI上随意删除一个stack，
scale-subsciber就可以获取event信息：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/7853084cjw1fa207xudcej20ni07mgpy.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;附加小技巧&#34;&gt;附加小技巧&lt;/h3&gt;

&lt;p&gt;如要将其部署在Rancher中，那就可以将scale-subsciber执行程序打包放到镜像中，通过compose来启动。
但是我们知道scale-subsciber启动需要指定CATTLE_URL、CATTLE_ACCESS_KEY、CATTLE_SECRET_KEY，
正常的做法就是我们需要先生成好apikey，然后启动service的时候设置对应的环境变量。
这样做就会有弊端，就是apikey这种私密的信息不得不对外暴露，而且还要手动维护这些apikey，非常不方便。&lt;/p&gt;

&lt;p&gt;Rancher提供了一个非常方便的做法，就是在service中添加两个label，
&lt;strong&gt;io.rancher.container.create_agent: true&lt;/strong&gt;和&lt;strong&gt;io.rancher.container.agent.role: environment&lt;/strong&gt;，
设定这两个label后，Rancher引擎会自动创建apikey，并把相应的值设置到容器的ENV中，
只要你的程序通过系统环境变量来读取这些值，就会非常顺利的运行起来。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>扒一扒Rancher社区中的小工具</title>
      <link>http://niusmallnan.com/2016/08/15/rancher-smart-tools</link>
      <pubDate>Mon, 15 Aug 2016 14:16:42 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/08/15/rancher-smart-tools</guid>
      <description>&lt;p&gt;除了我们所熟知的Rancher &amp;amp; RancherOS，Rancher Labs的开发团队在实践中提炼了很多实用的小工具，
这些小工具虽然并不会左右Rancher发展的大局，但是在项目标准化和开发效率上给团队带来巨大的便捷。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;与Linux、OpenStack等成熟的技术社区相比，Rancher社区还是处于初级发展阶段，
一个技术社区的成败并不是单纯的代码贡献，而学习文档的数量和代码管理作业流程也是非常重要的。
如何让怀揣不同需求的工程师都能在社区中快速找到相应的解决方案，这就需要大家协同合作共同促进社区发展与完善。
除了我们所熟知的Rancher &amp;amp; RancherOS，Rancher Labs的开发团队在实践中提炼了很多实用的小工具，
这些小工具虽然并不会左右Rancher发展的大局，但是在项目标准化和开发效率上给团队带来巨大的便捷。
这次主要是带着大家一起来认识一下这些小工具。&lt;/p&gt;

&lt;h5 id=&#34;golang包管理工具-trash&#34;&gt;Golang包管理工具-Trash&lt;/h5&gt;

&lt;p&gt;项目地址：&lt;a href=&#34;https://github.com/rancher/trash&#34;&gt;https://github.com/rancher/trash&lt;/a&gt;&lt;br /&gt;
目前主流的编程语言 Python、Ruby、Java、Php 等已经把包管理的流程设计的犹如行云流水般流畅，
一般情况下开发者是不需要操心类库包依赖管理以及升级、备份、团队协作的。Golang在1.5版本开始，
官方开始引入包管理的设计，加了 vendor目录来支持本地包管理依赖，
但是需要特殊设置 GO15VENDOREXPERIMENT=1，在1.6时代这个特性已经是默认的了。
可是vendor并没有统一的版本号管理功能，只是额外提供了project内包的依赖路径。
于是Trash这个工具就应运而生了，Trash的使用非常简单，只需要有一份依赖包的描述文件即可。&lt;/p&gt;

&lt;p&gt;描述文件 trash.conf 支持两种格式，普通方式和YAML方式，
可以直接在其中描述依赖库的远程地址、版本号等，一个简单的例子（我这里使用普通格式）：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/7853084cjw1fa20qdpd40j20c302t0t0.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后在根目录执行trash，即可获得相关版本的依赖包，非常轻量级，非常简洁：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/7853084cjw1fa20qtj6kcj20av06cjrr.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;golang编译工具-dapper&#34;&gt;Golang编译工具-Dapper&lt;/h5&gt;

&lt;p&gt;项目地址：&lt;a href=&#34;https://github.com/rancher/dapper&#34;&gt;https://github.com/rancher/dapper&lt;/a&gt;&lt;br /&gt;
我们在编译golang执行程序的时候，因为涉及到协作开发，这就会碰到一个问题，
就是如何保证编译环境的一致性。环境一致性最好的办法就是使用容器技术，
Dapper就是利用Docker build镜像的过程中可以执行各种命令生成容器的原理。
只需在项目的根目录下创建 Dockerfile.dapper 文件，这是一个参考Dockerfile标准的文件，
执行dapper命令即可按照约定规则生成最终的执行程序，通过这种方式统一的编译环境。&lt;/p&gt;

&lt;p&gt;几乎所有的Rancher项目都是基于Dapper来编译的，随意打开一个项目比如
&lt;a href=&#34;https://github.com/rancher/rancher-dns&#34;&gt;rancher-dns&lt;/a&gt;就可以看到Dockerfile.dapper文件：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/7853084cjw1fa20thy5rhj20j809ojtr.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;DAPPER_SOURCE 指定了容器内的源码路径&lt;br /&gt;
DAPPER_OUTPUT 指定了编译后的输出路径，bin dist 目录会自动在项目根目录下创建&lt;br /&gt;
DAPPER_DOCKER_SOCKET 设置为True相当于&lt;code&gt;docker run -v /var/run/docker.sock:/var/run/docker.sock ...&lt;/code&gt;&lt;br /&gt;
DAPPER_ENV 相当于&lt;code&gt;docker run -e TAG -e REPO ...&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;有一点需要注意的是，目前Dapper装载源码有两种方式bind和cp，bind就是直接mount本地的源码路径，
但是如果使用remote docker daemon方式那就得使用cp模式了。&lt;/p&gt;

&lt;h5 id=&#34;golang项目标准化工具-go-skel&#34;&gt;Golang项目标准化工具 Go-skel&lt;/h5&gt;

&lt;p&gt;项目地址：&lt;a href=&#34;https://github.com/rancher/go-skel&#34;&gt;https://github.com/rancher/go-skel&lt;/a&gt;&lt;br /&gt;
介绍了包管理工具和打包编译工具，如果我们在创建一个golang项目时能把这两个工具整合起来一起使用，
那就太赞了。go-skel就是提供了这样一个便利，我们直接来demo一下。&lt;/p&gt;

&lt;p&gt;clone一份go-skel的源码，创建一个rancher-tour（./skel.sh rancher-tour）的项目：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/7853084cjw1fa20wzh1pxj20jt09v0vf.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;执行完毕后，会创建一个标准的项目，包含了dapper和trash这两个工具，
同时定义了一份Makefile，我们可以通过make命令来简化操作：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/7853084cjw1fa20xj5jd3j20cw0art9r.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;比如我们要执行一个ci操作，那就可以直接运行 make ci，自动运行单元测试，
并在bin目录下生成最终的可执行程序：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/7853084cjw1fa20xw28tzj20gz05kmyl.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;标准项目已经创建了一些初始化代码，集成了 github.com/urfave/cli ，所以我们可以执行执行rancher-tour：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/7853084cjw1fa20y6wzdjj20kq08xgmw.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;微服务辅助小工具-giddyup&#34;&gt;微服务辅助小工具 Giddyup&lt;/h5&gt;

&lt;p&gt;项目地址：&lt;a href=&#34;https://github.com/cloudnautique/giddyup&#34;&gt;https://github.com/cloudnautique/giddyup&lt;/a&gt;&lt;br /&gt;
一个传统的服务在容器化的过程中，通常我们会将其拆分成多个微服务，
充分展现每个容器只做好一件事情这样的哲学。那么就会出现我们在Rancher中看到的sidekick容器，
数据卷容器，专门负责更新配置信息的容器等等。实际应用中我们会遇到一些问题，
比如这些微服务容器的启动是无序的，无序启动会导致微服务之间可能出现连接失败，
进而导致整个服务不可用；再比如我们如何来判定依赖的微服务已经正常启动，
这可能需要一个health check的服务端口。giddyup就是简化这种微服务检查工作的利器，它都能做些什么呢：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Get connection strings from DNS or Rancher Metadata.&lt;/li&gt;
&lt;li&gt;Determine if your container is the leader in the service.&lt;/li&gt;
&lt;li&gt;Proxy traffic to the leader.&lt;/li&gt;
&lt;li&gt;Wait for service to have the desired scale.&lt;/li&gt;
&lt;li&gt;Get the scale of the service.&lt;/li&gt;
&lt;li&gt;Get Managed-IP of the container (/self/container/primary_ip).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;我们还是创建stack并在其中创建service来体验一下giddyup的部分功能，
service中包含两个容器分别为主容器main和sidekick容器conf，他们共享网络栈：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/7853084cjw1fa210w5l9pj20c807xt9a.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我们可以在conf内启动giddyup health，会启动一个监听1620端口的http服务，服务路径是/ping：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/7853084cjw1fa2115foy9j20a703h3yl.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;此时我们可以在其他服务的容器内查看这个服务 health port的健康状态，
通过giddyup ip stringify获取服务地址，使用giddyup probe可以查看相关状态：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/7853084cjw1fa211iwkz2j20ql03jgmj.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到probe返回了OK的信息，说明giddy/main的health port是正常的，
如果我们把giddyup health的端口停掉，giddyup probe会如何？&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/7853084cjw1fa211ulol7j20mw06kn1u.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;除了开启health port功能外，还可以sleep等待service内所有容器都启动，
比如刚才的service，我在main上做wait操作，同时在UI上对service扩容，
可以看到为了等待完成扩容差不多sleep了3s：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/7853084cjw1fa212n0tykj20ep031mxg.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;更多功能可以去看一看giddyup项目的README文档，另外也可以看看catalog中的一些项目是怎么使用giddyup的，
参考：&lt;a href=&#34;https://github.com/rancher/catalog-dockerfiles&#34;&gt;https://github.com/rancher/catalog-dockerfiles&lt;/a&gt;&lt;/p&gt;

&lt;h5 id=&#34;rancher-cli-工具&#34;&gt;Rancher CLI 工具&lt;/h5&gt;

&lt;p&gt;项目地址：&lt;a href=&#34;https://github.com/rancher/cli&#34;&gt;https://github.com/rancher/cli&lt;/a&gt;&lt;br /&gt;
现在我们管理Rancher的方式包括UI、API方式，为了能够和其他工具更好的融合Rancher开发了CLI工具，
可以非常轻量级的嵌入到其他工具中。CLI将会在Rancher 1.2-pre2版本中正式放出，
兼容性上目前来看没有支持老版本的计划，所以在老版本上出现各种问题也是正常的。&lt;/p&gt;

&lt;p&gt;直接在&lt;a href=&#34;https://github.com/rancher/cli/releases&#34;&gt;https://github.com/rancher/cli/releases&lt;/a&gt;下载最新版本即可试用，
将rancher cli放到PATH中，正式使用前需要初始化：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/7853084cjw1fa214oqzn5j20hy07a405.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后便可以执行各种犀利的操作，比如针对某个service进行scale操作，
这比之前需要用rancher-compose cli要简洁的多：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/7853084cjw1fa2155xxeaj20ps05ggng.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;再比如可以实时监听rancher events，可以即时查看Rancher中正在发生的各种任务，
对协助排查问题，分析系统运行情况都非常有用：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/7853084cjw1fa215gzjcrj20dr077myp.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;后语&#34;&gt;后语&lt;/h4&gt;

&lt;p&gt;小工具中内含着大智慧，工具文化是工程师Team高效协作的重要标志，
高质量的工具能让项目开发有事半功倍之效，其背后也蕴藏着深厚的团队文化理念，
就是不计项目KPI利用个人业余时间为团队做贡献的和谐氛围。
其实国内很多互联网公司都是有专门设立工具开发工程师的岗位，
对工具带来的生产效率提升，其重视程度不言而喻！&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Kubernetes in Rancher1.0 架构分析</title>
      <link>http://niusmallnan.com/2016/07/28/k8s-in-rancher-1-0</link>
      <pubDate>Thu, 28 Jul 2016 14:49:47 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/07/28/k8s-in-rancher-1-0</guid>
      <description>&lt;p&gt;该主题是本人于Rancher k8s 北京meetup进行的一次线下分享，应Rancher China官方之邀，
重新梳理成文字版本，便于大家阅读传播，如有问题纰漏或任何不妥之处随时联系牛小腩（niusmallnan），
我会以最快速度更正。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;在Rancher 1.0版本开始，Rancher逐步增加了Kubernetes、Swarm、Mesos等多编排引擎的支持，
很多朋友就此产生了疑惑，诸如Cattle引擎和这几个之间到底什么关系？
每种引擎是如何支持的？自家的业务环境如何选型？我们将逐步揭开这些神秘面纱，
了解基础架构才能在遇到问题时进行有效的分析，进而准确的定位问题并解决问题，
因为没有一种生产环境是完全可靠的。基于这个背景下，这次我们首先向大家介绍kubernetes in Rancher的架构。&lt;/p&gt;

&lt;h3 id=&#34;分析&#34;&gt;分析&lt;/h3&gt;

&lt;p&gt;从现在Rancher的发展节奏来看，Cattle引擎已经被定义成Rancher的基础设施引擎，
而Rancher的基础设施服务都包括哪些呢？如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Networking，rancher的统一网络服务，由rancher-net组件提供&lt;/li&gt;
&lt;li&gt;Load Balancer，rancher的负载均衡服务，目前来看套路基本上是基于Haproxy来构建&lt;/li&gt;
&lt;li&gt;DNS Service，rancher的dns服务，主要是为了提供服务发现能力，由rancher-dns组件来提供&lt;/li&gt;
&lt;li&gt;Metadata Service，rancher的元数据服务，metadata是我们通过compose编排应用时的利器，
可以很灵活的像service中注入特定信息&lt;/li&gt;
&lt;li&gt;Persistent Storage Service，持久化存储服务目前是由convoy来提供，
而对于真正的后端存储的实现rancher还有longhorn没有完全放出&lt;/li&gt;
&lt;li&gt;Audit Logging，审计日志服务是企业场景中比较重要的一个属性，
目前是集成在cattle内部没有被完全分离出来&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所以Rancher在接入任何一种编排引擎，最终都会把基础设施服务整合到该引擎中，
Kubernetes in Rancher的做法正是如此。&lt;/p&gt;

&lt;p&gt;Kubernetes各个组件的角色可以归为三类即Master、Minion、Etcd，
Master主要是kube-apiserver、kube-scheduler、kube-controller-manager，
Minion主要是kubelet和kube-proxy。Rancher为了融合k8s的管控功能，
又在Master中添加了kuberctrld、ingress-controller、kubernetes-agent三个服务来打通Rancher和K8s，
同时每个node上都会依赖Rancher提供的rancher-dns、rancher-metadata、rancher-net这些基础设施服务。&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/7853084cjw1fa21xnsfbrj20p40c0wgm.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;由于K8s是基于Cattle引擎来部署，所以在K8s在部署完成之后，
我们可以通过Link Graph来很清晰的看到整体的部署情况。&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/7853084cjw1fa21y87258j20mi07tdh0.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
整个服务基于Cattle引擎的rancher-compose构建，新增节点后自动添加kubelet和kube-proxy服务
（此处利用了Global Label的特性），各个组件都添加了health-check机制，
保证一定程度的高可用。考虑到Etcd最低1个最多3个节点，
单台agent host就可以部署K8s，三节点agent host则更合理些。&lt;/p&gt;

&lt;p&gt;K8s集群完成部署后，我们就可以在其中添加各种应用服务，
目前Rancher支持管理K8s的service、pod、replication-controller等，
我们可以用一张图来形象得描述一下应用视图结构。&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/7853084cjw1fa21z5ellcj20qj0cuq5u.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
rancher-net组件会给每个pod分配一个ip，rancher-dns则替代了K8s的Skydns来实现服务发现，
在pod的容器内部依然可以访问rancher-metadata服务来获取元数据信息。除了这三个基础服务外，
我们之前提到的kuberctrld、ingress-controller、kubernetes-agent也在其中扮演者重要角色。&lt;/p&gt;

&lt;p&gt;无论是K8s还是Rancher，其中一些抽象对象（如rancher的stack/service，或者K8s的serivice/pod）
在属性更新时都会有events产生，在任何服务入口来更改这些抽象对象都会有events产生，
所以要保证Rancher和k8s能够互相感知各自对象的更新，那么kubernetes-agent就应运而生了。&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/7853084cjw1fa22166xnyj20nt0d60ut.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
诸如K8s的namespaces、services、replicationcontrollers、pods等对象的信息变更会及时通知给Rancher，
而Cattle管理的Host资源出现信息变更（诸如host label的变动）也会通知给K8s。&lt;/p&gt;

&lt;p&gt;简单得说kubernetes-agent是为了维护Rancher和K8s之间的对象一致性，
而真正要通过Rancher来创建K8s中的service或者pod之类的对象，
还需要另外一个服务来实现，它就是kubectrld，直观的讲它就是包装了kubectrl，
实现了其中kubectl create/apply/get 等功能。&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/7853084cjw1fa221z612jj20o80bhq4q.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
所有的K8s对象创建请求都会走cattle引擎，cattle会把请求代理到kubectrld启动的一个api服务。
除此之外，还会监听rancher events来辅助实现相关对象的CRUD。&lt;/p&gt;

&lt;p&gt;K8s上创建的service如需对外暴露访问，那么必然会用到LoadBalancer Type和Ingress kind，
注意K8s概念下的LoadBalancer和Ingress略有不同，LoadBalancer的功能主要关注在L4支持http/tcp，
而Ingrees则是要实现L7的负载均衡且只能支持http。K8s 的LoadBalancer需要在K8s中实现一个Cloud Provider，
目前只有GCE，而Rancher则维护了自己的K8s版本在其中提供了Rancher Cloud Provider。
对于Ingress则是提供了Ingress-controller组件，它实现了K8s的ingress框架，
可以获取ingress的创建信息并执行相应的接口。当然最终这两者都会调用cattle api来创建Rancher的负载均衡，
且都是通过Haproxy完成负责均衡功能。&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/7853084cjw1fa22370rzfj20hv0b6gn5.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;后语&#34;&gt;后语&lt;/h3&gt;

&lt;p&gt;以目前K8s社区的火热势头，Rancher应该会持续跟进并不断更新功能优化架构，
待到Rancher1.2发布之后，CNI的支持会是一个里程碑，
到那时Kubernetes in Rancher也会更加成熟，一起向着最好用Kubernetes发行版大踏步的前进。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>