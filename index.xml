<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Niusmallnan</title>
    <link>http://niusmallnan.com/index.xml</link>
    <description>Recent content on Niusmallnan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <copyright>Niusmallnan</copyright>
    <lastBuildDate>Tue, 22 Nov 2016 13:59:30 +0800</lastBuildDate>
    <atom:link href="http://niusmallnan.com/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>基于容器实现树莓派的动态域名绑定</title>
      <link>http://niusmallnan.com/2016/11/22/rpi-noip-with-docker</link>
      <pubDate>Tue, 22 Nov 2016 13:59:30 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/11/22/rpi-noip-with-docker</guid>
      <description>&lt;p&gt;家庭使用树莓派场景中，如何给树莓派绑定一个域名，让我们一扫运营商动态IP的困扰，
可以轻轻松松在外网使用域名访问。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;我们在家里使用树莓派时，如果在上面搭建了一些服务，有时会期待能在外网可以访问。
家庭宽带会给我们分配一个外网IP，可以通过这个IP在公网上访问树莓派上的服务。
但是运营商提供的这个IP是非静态的，很可能在凌晨的时候会被重新分配，
而且IP也不是很好记，所以通常我们都希望能有一个域名可以直接访问。
动态域名解析是个老话题，国内最早花生壳就做过，具体原理不必多说。&lt;/p&gt;

&lt;h3 id=&#34;我们的需求清单如下&#34;&gt;我们的需求清单如下：&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;一个动态域名解析的软件，最好是一定程度Free。&lt;/li&gt;
&lt;li&gt;域名解析的client端一定要是开源的（谁也不想被当肉鸡&amp;hellip;）。&lt;/li&gt;
&lt;li&gt;部署控制要非常简单。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;最终选型如下&#34;&gt;最终选型如下：&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;使用国外&lt;a href=&#34;https://www.noip.com/&#34;&gt;noip&lt;/a&gt;的服务，免费账户可以绑定三个免费域名，
每个域名30天有效，失效后需要手动添加回来。这种程度对于我这种玩家已经完全足够了。&lt;/li&gt;
&lt;li&gt;noip的client端支持各种平台，支持&lt;a href=&#34;https://www.noip.com/download?page=linux&#34;&gt;Linux&lt;/a&gt;，同时是开源。&lt;/li&gt;
&lt;li&gt;为了让部署变得更加简洁，决定使用Docker容器来部署。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;执行过程&#34;&gt;执行过程&lt;/h3&gt;

&lt;p&gt;首先，需要到&lt;a href=&#34;https://www.noip.com/&#34;&gt;noip&lt;/a&gt;上注册账户，并填写自己的域名，注册过程请自行体验，比如：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tNc79jw1fa0utbt10oj30ja04maap.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后，我们就需要在下载客户端，并在树莓派上进行编译，生成适合ARM运行的版本，编译只要执行make即可：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tNc79jw1fa0uxw14ulj30fk0a4wgz.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;初次执行需要先在client端注册账号信息，其实就是生成一下相关配置文件，再次执行就可以运行起来了。&lt;/p&gt;

&lt;p&gt;这么看来，对于很多用户来说还是太复杂，所以我们决定使用容器来简化这个过程。
可以把上面繁琐操作，全部放在容器中，为了精简程序的编译环境，
我们使用alpine-linux来进行编译，容器的Dockerfile如下：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tNc79jw1fa0uz7rv7oj30i304pmyk.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;接着，我们需要对树莓派的系统进行容器化，以便我们可以运行noip容器程序，此时可以有三种选择：
1. RancherOS
2. HypriotOS
3. Rasbian 基础上安装Docker&lt;/p&gt;

&lt;p&gt;前两个都是内置Docker Engine的，用起来比较方便，Rasbian需要自行安装Docker。我选择的OS是RancherOS。&lt;/p&gt;

&lt;h3 id=&#34;最简方式部署noip-只需三步&#34;&gt;最简方式部署noip，只需三步&lt;/h3&gt;

&lt;p&gt;在RancherOS上修改registry mirror后，可以加速镜像下载，然后拉取前面Dockerfile编译的镜像：&lt;br /&gt;
&lt;code&gt;$ docker pull hypriot/rpi-noip&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;注册noip client，按照提示添加用户名密码等信息：&lt;br /&gt;
&lt;code&gt;$ docker run -ti -v noip:/usr/local/etc/ hypriot/rpi-noip noip2 -C&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;运行noip clent：&lt;br /&gt;
&lt;code&gt;$ docker run -v noip:/usr/local/etc/ --restart=always hypriot/rpi-noip&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;登录No-IP的&lt;a href=&#34;https://my.noip.com/#!/dynamic-dns&#34;&gt;控制台&lt;/a&gt;可以看到 dns绑定情况：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tNc79jw1fa0v2h5umhj30km05rjs0.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在本地使用dig确认一下解析情况：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tNc79jw1fa0v2po6wfj30gy08yjt4.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;以上所有源码可以参考：&lt;a href=&#34;https://github.com/hypriot/rpi-noip&#34;&gt;https://github.com/hypriot/rpi-noip&lt;/a&gt;。
这样，我们就在树莓派上非常简单的实现了动态域名绑定。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>关于我</title>
      <link>http://niusmallnan.com/aboutme/</link>
      <pubDate>Tue, 22 Nov 2016 12:07:35 +0800</pubDate>
      
      <guid>http://niusmallnan.com/aboutme/</guid>
      <description>&lt;p&gt;
张智博（niusmallnan），笔名牛小腩，初出茅庐在阿里巴巴口碑网，参与本地搜索业务研发工作，
后与朋友联合创办美食点评社区“美食行”，之后在各种公司从事云计算研发工作。&lt;/p&gt;

&lt;p&gt;喜欢参与技术社区活动，曾担任：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;OpenStack中国社区长期作者&lt;/li&gt;
&lt;li&gt;InfoQ兼职编辑&lt;/li&gt;
&lt;li&gt;Rancher中国社区布道师&lt;/li&gt;
&lt;li&gt;MBH树莓派社区成员&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;热爱coding，热爱技术分享，技术宅&amp;amp;科幻粉。&lt;/p&gt;

&lt;p&gt;简历更多信息，请参考&lt;a href=&#34;https://cn.linkedin.com/in/niusmallnan&#34;&gt;LinkedIn&lt;/a&gt;。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>如何hack一下rancher k8s</title>
      <link>http://niusmallnan.com/2016/10/08/rancher-k8s-hacking</link>
      <pubDate>Sat, 08 Oct 2016 16:09:24 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/10/08/rancher-k8s-hacking</guid>
      <description>&lt;p&gt;Rancher可以轻松实现Kubernetes的部署，尽管默认的部署已经完全可用，
但是如果我们想修改部署的K8s版本，这时应当如何应对？
&lt;/p&gt;

&lt;h3 id=&#34;原理分析与执行&#34;&gt;原理分析与执行&lt;/h3&gt;

&lt;p&gt;在Rancher中，由于K8s是基于Cattle引擎来部署，所以在K8s在部署完成之后，
我们可以通过Link Graph来很清晰的看到整体的部署情况。&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tNc79jw1fa0ybyprq4j30mi07tdh0.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;既然基于Cattle引擎部署，也就是说需要两个compose文件，
k8s引擎的compose文件放在&lt;a href=&#34;https://github.com/rancher/rancher-catalog/tree/master/templates&#34;&gt;https://github.com/rancher/rancher-catalog/tree/master/templates&lt;/a&gt;下面，
这里面有两个相关目录kubernetes与k8s，k8s是Rancher1.2开始使用的，
而kubernetes则是Rancher1.2之后开始使用的。&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tNc79jw1fa0ydzd1dgj30gt07qmyg.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;为了我们可以自己hack一下rancher k8s的部署，我们可以在github上fork一下rancher-catalog，
同时还需要修改一下Rancher中默认的catalog的repo地址，
这个可以在&lt;a href=&#34;http://rancher-server/v1/settings&#34;&gt;http://rancher-server/v1/settings&lt;/a&gt;页面下，
寻找名为 catalog.url 的配置项，然后进入编辑修改。比如我这里将library库的地址换成了自己的：
&lt;a href=&#34;https://github.com/niusmallnan/rancher-catalog.git&#34;&gt;https://github.com/niusmallnan/rancher-catalog.git&lt;/a&gt;&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tNc79jw1fa0yff7sbzj30h206imyy.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;此时，我们就可以修改了，找一个比较实用的场景。我们都知道k8s的pod都会依赖一个基础镜像，
这个镜像默认的地址是被GFW挡在墙外了，一般我们会把kubelet的启动参数调整一下，
以便重新指定这个镜像地址，比如指定到国内的镜像源上。&lt;br /&gt;
&lt;strong&gt;&amp;ndash;pod-infra-container-image=index.tenxcloud.com/google_containers/pause:2.0&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;如果我们要让rancher k8s部署时自动加上该参数，
可以直接修改私有rancher-catalog中的k8s compose文件。&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tNc79jw1fa0ygscal6j30gv0asjtp.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;修改之后稍等片刻（主要是为了让rancher-server更新到新的catalog compose文件），
添加一个k8s env并在其中添加host，k8s引擎就开始自动部署，
部署完毕后，我们可以看到Kubernetes Stack的compose文件，
已经有了&amp;ndash;pod-infra-container-image这个启动参数。&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tNc79jw1fa0yhg3alsj30gn09rtav.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
如此我们在添加pod时再也不用手动导入pod基础镜像了。&lt;/p&gt;

&lt;p&gt;在compose file中，部署k8s的基础镜像是rancher/k8s，这个镜像的Dockerfile在rancher维护的k8s分支中，
如在rancher-k8s 1.2.4分支中可以看到：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tNc79jw1fa0yi5rmplj30e007ogmr.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这样如果想对rancher-k8s发行版进行深度定制，就可以重新build相关镜像，通过rancher-compose来部署自己的发行版。&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;本文写于Rancher1.2行将发布之际，1.2版本是非常重大的更新，Rancher会支持部署原生的K8s版本，
同时CNI网络和Cloud Provider等都会以插件方式，用户可以自己定义，并且在UI上都会有很好的体现。
只要了解Rancher部署K8s的原理和过程，我们就可以定制非常适合自身使用的k8s，
通过Rancher来部署自定义的k8s，我们就可以很容易的扩展了k8s不擅长的UI、Catalog、
用户管理、审计日志维护等功能，这也是本文的目的。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Rancher event机制及其实践指南</title>
      <link>http://niusmallnan.com/2016/08/25/rancher-envent</link>
      <pubDate>Thu, 25 Aug 2016 13:44:04 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/08/25/rancher-envent</guid>
      <description>&lt;p&gt;Rancher在v1.0版本为了Kubernetes的集成，开始扩展并逐步完善了Rancher event机制，
event机制可以完成技术架构上的解耦。那么Rancher都提供了哪些event？
Rancher内部是如何使用的？以及我们如何用event来增强一些定制服务？
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;我们的Rancher官方技术社区已经创立些许时日了，相信通过我们的线下meetup和线上布道工作，
很多朋友对Rancher的使用已经掌握得很纯熟了。一些高级用户开始真正把自己的业务进行微服务化并向Rancher迁移，
在迁移的过程中，由于业务本身的复杂性特殊性，
可能需要利用Rancher的一些高级特性甚至要对Rancher进行一定的扩展，
这就需要对Rancher的一些组件的实现机制有些许了解。&lt;/p&gt;

&lt;p&gt;本次分享就介绍一下Rancher的event机制，由于相关内容文档极其欠缺，本人也只是通过实践和代码阅读分析其原理，
如有谬误欢迎交流指正，小腩双手红包奉上！同时为保证收视率，本次分享会以原理+实践的方式进行。&lt;/p&gt;

&lt;h3 id=&#34;原理分析&#34;&gt;原理分析&lt;/h3&gt;

&lt;p&gt;在大规模系统架构中，event机制通常采用消息驱动 ，它对提升分布式架构的容错性灵活性有很大帮助，
同时也是各个组件之间解耦的利器。Rancher能够管理N多的agent同时又拆分出各种服务组件，
event机制是必不可少的。为实现event机制，通常我们会采用RabbitMQ、ActiveMQ、ZeroMQ等中间件来实现。
而Rancher则采用了基于websocket协议的一种非常轻量级的实现方式，
它的好处就是极大程度的精简了Rancher的部署，Rancher无需额外维护一个MQ集群，
毕竟websocket的消息收发实现是非常简单的，各种语言库均可以支持。&lt;/p&gt;

&lt;p&gt;这里我们会考虑一个问题，websocket毕竟不是真正工业级MQ的实现，消息不能持久化，
一旦某个event的处理出现问题，或者发生消息丢失，Rancher如何保证各个资源的原子性一致性？
Rancher中有一个processpool的概念，它可以看做一个所有event的执行池，
当API/UI/CLI有操作时，Rancher会把操作分解成多个event并放入processpool中。
比如删除一个容器时会把 compute.instance.remove 放入processpool中，
这个event会发送到对应的host agent上，agent处理完成后会发送reply给rancher-server。
如果在这个过程中，由于网络问题消息丢失，或者agent上执行出现问题，
rancher-server没有收到reply信息，cattle会把这个event重新放到processpool中再次重复上面的过程，
直到 compute.instance.remove 完成操作，这个容器的状态才会在DB中更新，
否则该容器状态会一直处于lock不能被其他服务更新。当然cattle不会把这些event不停的重复执行下去，
通常会设置一下TIMEOUT超出后便不再执行（有些资源没有TIMEOUT机制）。&lt;/p&gt;

&lt;p&gt;上面的表述，我们其实可以在UI上看到这个过程，RancherUI上的Processes的Running Tab页上就能实时得看到这些信息，
Processes 在排查一些Rancher的相关问题是非常有用的，大家可以养成 ”查问题先查Processes“的好习惯：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/7853084cjw1fa1zux9y7bj20ly091dh6.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
那么监听event的URL怎么设定呢？非常简单：&lt;br /&gt;
&lt;code&gt;ws://&amp;lt;rancher-server-ip&amp;gt;:8080/v1/projects/&amp;lt;projectId&amp;gt;/subscribe?eventNames=xxxx&lt;/code&gt;&lt;br /&gt;
除此之外还需要加上basic-auth的header信息：&lt;br /&gt;
&lt;code&gt;Authorization: Basic +base64encode(&amp;lt;cattle-access-key&amp;gt;:&amp;lt;cattle-secret-key&amp;gt;)&lt;/code&gt;&lt;br /&gt;
如果是Host上的agent组件，除此之外还需要添加agentId参数：&lt;br /&gt;
&lt;code&gt;ws://&amp;lt;rancher-server-ip&amp;gt;:8080/v1/projects/&amp;lt;projectId&amp;gt;/subscribe?eventNames=xxxx&amp;amp;agentId=xxxx&lt;/code&gt;&lt;br /&gt;
agentId 是注册Host时生成的，如果没有agentId参数，
任何有关无关的event都会发送到所有的Host agent上，这样就会发生类似“广播风暴”的效果。&lt;/p&gt;

&lt;p&gt;Host agent上运行很多组件，其中python-agent是负责接收和回执event信息的，
其运行日志可以在Host上的/var/log/rancher/agent.log文件中查看。
细心的朋友可能会有疑问，我们在添加Host时执行agent容器时并没有指定cattle-access-key和cattle-secret-key，
也就是说python-agent运行时如何获取这两个秘钥信息呢？&lt;/p&gt;

&lt;p&gt;其实Rancher有两种apikey：一种是我们熟知的在UI上手动创建的apikey；
另外一种就是agentApikey，它是系统级的，专门为agent设定，
添加Host时会先把agentApikey发送到Host上。在cattle的credential表中可以查询到相关信息：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/7853084cjw1fa1zxokn7dj20ea05774u.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;eventNames都定义了哪些呢？下面两个文件可以参考：&lt;br /&gt;
* &lt;a href=&#34;https://github.com/rancher/cattle/blob/master/code/iaas/events/src/main/java/io/cattle/platform/iaas/event/IaasEvents.java&#34;&gt;IaasEvents.java&lt;/a&gt;
* &lt;a href=&#34;https://github.com/rancher/cattle/blob/master/code/packaging/app-config/src/main/resources/META-INF/cattle/process/spring-process-context.xml&#34;&gt;系统级的event定义&lt;/a&gt;，详细到每种资源(host、volume、instance、stack、service等)的event定义。&lt;/p&gt;

&lt;p&gt;此外，我们在UI/CLI/API上的某个操作都会被分解成多个event来执行，
每个event信息都会被保存在mysql中，每个event执行成功后会设置成purged状态，
所以记录并不会真正删除，这就会导致相应的表（container_event表、service_event表、process_instance表）会无限膨胀下去。
Rancher为解决此问题提供了周期性清理机制：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;events.purge.after.seconds 可以清理container_event和service_event，每两周清理一次&lt;/li&gt;
&lt;li&gt;process_instance.purge.after.seconds可以清理process_instance，每天清理一次。&lt;br /&gt;
这两个配置我们都可以在&lt;a href=&#34;http://rancher-server-ip:8080/v1/settings&#34;&gt;http://rancher-server-ip:8080/v1/settings&lt;/a&gt;动态修改。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;实践操作&#34;&gt;实践操作&lt;/h3&gt;

&lt;p&gt;下面我们来实践一下，看看如何在程序中实现对Rancher event的监听。
Rancher提供了resource.change事件，这个事件是不用reply的，就是说不会影响Rancher系统的运行，
它是专门开放给开发者用来实现一些自己的定制功能，所以我们就以resource.change作为例子实践一下。&lt;/p&gt;

&lt;p&gt;Rancher的组件大部分都是基于Golang编写，所以我们也采用相同的语言。
为了能够快速实现这个程序，我们需要了解一些辅助快速开发的小工具。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Trash，Golang package管理的小工具，可以帮助我们定义依赖包的路径和版本，非常轻量且方便；&lt;/li&gt;
&lt;li&gt;Dapper，这个是基于容器实现Golang编译的工具，主要是可以帮助我们统一编译环境；&lt;/li&gt;
&lt;li&gt;Go-skel，它可以帮我们快速创建一个Rancher式的微服务程序，可以为我们省去很多的基本代码，
同时还集成了Trash和Dapper这两个实用的小工具。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;首先我们基于go-skel创建一个工程名为 scale-subscriber （名字很随意），执行过程需要耐心的等待：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/7853084cjw1fa2044jpu9j20im0ce78n.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;执行完毕后，我们把工程放到GOPATH中，开始添加相关的逻辑代码。
在这之前我们可以考虑添加一个healthcheck的服务端口，纵观Rancher所有的微服务组件，
基本上除了主程序之外都会添加 healthcheck port，这个主要是为了与Rancher中的healthcheck功能配合，
通过设定对这个端口的检查机制来保证微服务的可靠性。我们利用Golang的goroutine机制，
分别添加主服务和healthcheck服务：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/7853084cjw1fa2053h2k7j20i908u0uu.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;主服务的核心就是监听resource.change的信息，同时注册handler来获取event的payload信息，
从而可以定制扩展自己的逻辑，这里需要利用Rancher提供的
&lt;a href=&#34;https://github.com/rancher/event-subscriber&#34;&gt;event-subsriber&lt;/a&gt;库来快速实现。
如下图，在&lt;strong&gt;reventhandlers.NewResourceChangeHandler().Handler&lt;/strong&gt;中就可以实现自己的逻辑：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/7853084cjw1fa206sahqtj20ka07habz.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这里我们只是演示监听event的机制，所以我们就不做过多的业务逻辑处理，打印输出event信息即可。
然后在项目根目录下执行make，make会自动调用dapper，在bin目录下生成scale-subscriber，
我们执行scale-subscriber就可以监听resource.change的信息：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/7853084cjw1fa207fymwlj20u904gdhf.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这里我们可以看到分别启动了healthcheck功能和event listener。然后可以在UI上随意删除一个stack，
scale-subsciber就可以获取event信息：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/7853084cjw1fa207xudcej20ni07mgpy.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;附加小技巧&#34;&gt;附加小技巧&lt;/h3&gt;

&lt;p&gt;如要将其部署在Rancher中，那就可以将scale-subsciber执行程序打包放到镜像中，通过compose来启动。
但是我们知道scale-subsciber启动需要指定CATTLE_URL、CATTLE_ACCESS_KEY、CATTLE_SECRET_KEY，
正常的做法就是我们需要先生成好apikey，然后启动service的时候设置对应的环境变量。
这样做就会有弊端，就是apikey这种私密的信息不得不对外暴露，而且还要手动维护这些apikey，非常不方便。&lt;/p&gt;

&lt;p&gt;Rancher提供了一个非常方便的做法，就是在service中添加两个label，
&lt;strong&gt;io.rancher.container.create_agent: true&lt;/strong&gt;和&lt;strong&gt;io.rancher.container.agent.role: environment&lt;/strong&gt;，
设定这两个label后，Rancher引擎会自动创建apikey，并把相应的值设置到容器的ENV中，
只要你的程序通过系统环境变量来读取这些值，就会非常顺利的运行起来。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>