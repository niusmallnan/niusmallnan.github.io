<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Niusmallnan</title>
    <link>http://niusmallnan.com/tags/rancher/index.xml</link>
    <description>Recent content on Niusmallnan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <copyright>Niusmallnan</copyright>
    <atom:link href="http://niusmallnan.com/tags/rancher/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Kubelet无法访问rancher-metadata问题分析</title>
      <link>http://niusmallnan.com/2017/03/09/analysis-of-kubelet-start-failure</link>
      <pubDate>Thu, 09 Mar 2017 15:45:41 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2017/03/09/analysis-of-kubelet-start-failure</guid>
      <description>&lt;p&gt;拯救一脸懵逼
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;Rancher能够支持Kubernetes，可以快速几乎无障碍的拉起一套K8s环境，这对刚入门K8s的小白来说简直是一大利器。
当然由于系统特性五花八门，系统内置软件也相互影响，所以有时候伙伴们会碰到比较难缠的问题。
本文就分析一下关于kubelet无法访问rancher-metadata问题，当然这个问题并不是必现，
这里要感谢一位Rancher社区网友的帮助，他的环境中发现了这个问题，并慷慨得将访问权限共享给了我。&lt;/p&gt;

&lt;h3 id=&#34;问题现象&#34;&gt;问题现象&lt;/h3&gt;

&lt;p&gt;使用Rancher部署K8s后，发现一切服务状态均正常，这时候打开K8s dashboard却无法访问，
细心得查看会发现，dashboard服务并没有部署起来，这时下意识的行为是查看kubelet的日志，
此时会发现一个异常：&lt;br /&gt;
&lt;img src=&#34;https://ww1.sinaimg.cn/large/006tNbRwly1fdgnj5h1bjj30g503omxw.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
你会发现kubelet容器内部一直无法访问rancher-metadata，查看rancher-k8s-package源码，
kubelet服务启动之前需要通过访问rancher-metadata做一些初始化动作，由于访问不了，
便一直处于sleep状态，也就是出现了上面提到的那些异常日志的现象：&lt;br /&gt;
&lt;img src=&#34;https://ww2.sinaimg.cn/large/006tNbRwly1fdgnn1tbzzj30k2071q41.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;同样，在github上也能看到类似的issue：&lt;a href=&#34;https://github.com/rancher/rancher/issues/7160&#34;&gt;https://github.com/rancher/rancher/issues/7160&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;排查分析&#34;&gt;排查分析&lt;/h3&gt;

&lt;p&gt;进入kubelet容器一探究竟，分别用ping和dig测试对rancher-metadata访问情况如下：&lt;br /&gt;
&lt;img src=&#34;https://ww1.sinaimg.cn/large/006tNbRwly1fdgnq66xs1j30ey0abmyh.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
dig明显可以解析，但是ping无法解析，因此基本排除了容器内dns nameserver或者网络链路情况的问题。&lt;/p&gt;

&lt;p&gt;既然dig没有问题，ping有问题，那么我们就直接采取使用strace（&lt;code&gt;strace ping rancher-metadata -c 1&lt;/code&gt;）来调试，
这样可以打印系统内部调用的情况，可以更深层次找到问题根源：&lt;br /&gt;
&lt;img src=&#34;https://ww1.sinaimg.cn/large/006tNbRwly1fdgnsufow0j30k7090tb2.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;之前提到这个问题并不是必现的，所以我们找一个正常的环境，同样用strace调试，如下：&lt;br /&gt;
&lt;img src=&#34;https://ww2.sinaimg.cn/large/006tNbRwly1fdgnu63kl2j30m0077dhj.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;对这两张图，其实已经能够很明显的看出区别，有问题的kubelet在解析rancher-metadata之前，
向nscd请求的解析结果，nscd返回了unkown host，所以就没有进行dns解析。
而正常的kubelet节点并没有找到nscd.socket，而后直接请求dns进行解析rancher-metadata地址。&lt;/p&gt;

&lt;p&gt;经过以上的分析，基本上断定问题出在nscd上，那么为什么同样版本的rancher-k8s，
一个有nscd socket，而另一个却没有，仔细看一下kubelet的compose定义：&lt;br /&gt;
&lt;img src=&#34;https://ww4.sinaimg.cn/large/006tNbRwly1fdgny3wpihj30c5087wf5.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
kubelet启动时候映射了主机目录/var/run，那么基本可以得知nscd来自于系统。
检查一下有问题的kubelet节点的系统，果然会发现安装了nscd服务（服务名为unscd）。&lt;/p&gt;

&lt;p&gt;用比较暴力的方案证明一下分析过程，直接删除nscd socket文件，这时候你会发现kubelet服务正常启动了，
rancher-metadata也可以访问了。&lt;/p&gt;

&lt;p&gt;回过头来思考一下原理，为什么ping/curl这种会先去nscd中寻找解析结果呢，而dig/nslookup则不受影响。
ping/curl这种在解析地址前都会先读取/etc/nsswitch.conf，这是由于其底层均引用了glibc，
由nsswitch调度，最终指引ping/curl先去找nscd服务。nscd服务是一个name services cache服务，
很多解析结果他会缓存，而我们知道这个nscd是运行在Host上的，Host上是不能直接访问rancher-metadata这个服务名，
所以kubelet容器中就无法访问rancher-metadata。&lt;/p&gt;

&lt;h3 id=&#34;其他解决方案&#34;&gt;其他解决方案&lt;/h3&gt;

&lt;p&gt;其实我们也未必要如此暴力删除nscd，nscd也有一些配置，我们可以修改一下以避免这种情况，
可以disable hosts cache，这样nscd中便不会有相应内容的缓存，所以解析rancher-metadata并不会出现unknown host，
而是继续向dns nameserver申请解析地址，这样也不会有问题。&lt;br /&gt;
&lt;img src=&#34;https://ww4.sinaimg.cn/large/006tNbRwly1fdgo48mxf8j30ej06lab0.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;遇到问题不能慌，关键是要沉得住气，很多看似非常复杂的问题，其实往往都是一个小配置引发的血案。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Rancher如何按计算资源调度</title>
      <link>http://niusmallnan.com/2017/02/23/rancher-scheduler-based-on-resource</link>
      <pubDate>Thu, 23 Feb 2017 18:58:19 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2017/02/23/rancher-scheduler-based-on-resource</guid>
      <description>&lt;p&gt;非常简单的说明一下，Rancher如何做按计算资源调度。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;按计算资源调度基本上是各大编排引擎的标配，Rancher在v1.2版本后也推出了这个功能，
很多朋友并不知道，主要是因为当前的实现还并不是那么智能，故不才欲写下此文以助视听。&lt;/p&gt;

&lt;h3 id=&#34;实现机制&#34;&gt;实现机制&lt;/h3&gt;

&lt;p&gt;Rancher的实现比较简单，其主要是通过Infra services中的scheduler服务来实现，整体的逻辑架构如下：&lt;br /&gt;
&lt;img src=&#34;https://ww3.sinaimg.cn/large/006tNc79ly1fd0lxplni2j30m90a7q49.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
scheduler会订阅Rancher Events，主要是scheduler相关事件，当有调度需求时候，scheduler就会收到消息，
通过计算将合适的调度目标返回给cattle。比如说现在支持memory和cpu为基准，
那么scheduler会不断根据metadata的数据变化来计算资源的使用量，最后可根据资源剩余量为调度目标排序，
这样就可以完成按计算资源调度的目标。&lt;/p&gt;

&lt;p&gt;之前有说，Rancher的实现并不智能，这在于在计算资源使用量的时候，Rancher并不是通过一套复杂数据采集机制来计算，
而是通过用户在创建service的时候标注reservation的方式，这个地方很多朋友并没有注意到：&lt;br /&gt;
&lt;img src=&#34;https://ww4.sinaimg.cn/large/006tNc79ly1fd0m3wzn0dj30lf08f3ze.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;除此之外，在每个节点的资源总量上也是可配置的，我们完全可以进行一个整体预留的设置，比如：&lt;br /&gt;
&lt;img src=&#34;https://ww4.sinaimg.cn/large/006tNc79ly1fd0m5p3spuj30f90amaao.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;这个实现看似简单，其实这是提供了一个很好的扩展能力。如果我们有自己的监控采集体系，
完全可以在scheduler的时候调用我们自身监控接口来计算资源，这样就能达到我们所认可的“智能”了。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>关于Subscribe Rancher Events的思考</title>
      <link>http://niusmallnan.com/2017/02/23/thinking-about-subcribe-rancher-events</link>
      <pubDate>Thu, 23 Feb 2017 17:33:56 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2017/02/23/thinking-about-subcribe-rancher-events</guid>
      <description>&lt;p&gt;路漫漫其修远兮，吾将上下而求索
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;几乎每个大型的分布式的集群软件，都离不开一样东西，就是所谓的message bus（消息总线），
它就如同人体的血管一样，连通着各个组件，相互协调，一起工作。
与很多同类软件不同的是，Rancher使用的基于websocket协议实现的消息总线，
Rancher不会依赖任何MQ，基于websocket的实现十分轻量级，
同时在各种语言库的支持上，也毫无压力，毕竟websocket是HTTP的标准规范之一。&lt;/p&gt;

&lt;h3 id=&#34;初识rancher-events&#34;&gt;初识Rancher Events&lt;/h3&gt;

&lt;p&gt;基于websocket的消息总线可以很好的与前端兼容，让消息的传递不再是后端的专利。
在Rancher UI上，很容易就能捕获到rancher events，比如：&lt;br /&gt;
&lt;img src=&#34;https://ww4.sinaimg.cn/large/006tNc79ly1fd0k5tuin9j30it07at9x.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
这里面监听的事件名称是resource.change，这个resource.change在前端UI上有很大的用处，
其实我们都知道，很多POST形式的create请求并不是同步返回结果的，因为调度引擎需要处理，
这个等待的过程中，当然不能前端一直wait，所以做法都是发起create后直接返回HTTP 202，
转入后台执行后，Rancher的后端会把创建的执行过程中间状态不断发送给消息总线，
那么前端通过监听resource.change就会获得这些中间状态，这样在UI上就可以给用户一个很好的反馈体验。&lt;/p&gt;

&lt;p&gt;当然Rancher Events并不是只有resource.change，比如在Iaas Events集合中就有如下这些：&lt;br /&gt;
&lt;img src=&#34;https://ww1.sinaimg.cn/large/006tNc79ly1fd0kccvpwxj30gk0ac40k.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;除了Events的事件定义，当然还有如何去subscribe 这些events，这部分内容我之前的文章中有所涉猎，
便不赘言。&lt;a href=&#34;http://niusmallnan.com/2016/08/25/rancher-envent/&#34;&gt;http://niusmallnan.com/2016/08/25/rancher-envent/&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;subscribe-rancher-events的架构模式&#34;&gt;Subscribe Rancher Events的架构模式&lt;/h3&gt;

&lt;p&gt;Rancher的体系内，很多微服务的组件都是基于Subscribe Rancher Events这种架构，举个例子来看，
以rancher-metadata组件为例：&lt;br /&gt;
&lt;img src=&#34;https://ww4.sinaimg.cn/large/006tNc79ly1fd0kmwbmu4j30iy07wq3n.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
metadata服务可以提供当前host的元数据查询，我们可以很容器的知道env内的stack/service/container的情况，
这些数据其实由rancher-server也就是cattle引擎生成的，那么生成之后怎么发送给各个agent呢？
其实就是metadata进行了subscribe rancher events，当然它只监听了config.update事件，
只要这个事件有通知，metadata服务便会下载新的元数据，这样就达到了不断更新元数据的目的。&lt;/p&gt;

&lt;p&gt;随着深入的使用Rancher，肯定会有一些伙伴需要对Rancher进行扩展，那就需要自行研发了，
毕竟常见的方式就是监听一些事件做一些内部处理逻辑，并在DB中存入一些数据，
同时暴露API服务，架构如下：&lt;br /&gt;
&lt;img src=&#34;https://ww3.sinaimg.cn/large/006tNc79ly1fd0kt6rttwj309o072jrn.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;如果需要做HA，可能需要scale多个这样的服务，那么架构就变成这样：&lt;br /&gt;
&lt;img src=&#34;https://ww2.sinaimg.cn/large/006tNc79ly1fd0ku7zaafj30g30avdgq.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
这里其实会有一个问题，如果你监听了一些广播事件，那么实际上每个实例都会收到同样的事件，
那么你的处理逻辑就要注意了，尤其是在处理向DB中写数据时，一定要考虑到这样的情况。&lt;/p&gt;

&lt;p&gt;比如，可以只有其中一个实例来监听广播事件，这样不会导致事件重复收取：&lt;br /&gt;
&lt;img src=&#34;https://ww4.sinaimg.cn/large/006tNc79ly1fd0kxdd6vmj30g30b1dgm.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
Event Handler要考虑一定failover机制，这样事件收取不会长时间中断。&lt;/p&gt;

&lt;p&gt;Rancher Events有一些非广播事件，那么就需要在subscribe的时候指定一些特殊参数，
这样事件就会只发送给注册方，不会发送给每个节点，比如：&lt;br /&gt;
&lt;img src=&#34;https://ww3.sinaimg.cn/large/006tNc79ly1fd0kztt3k6j30fx0audgr.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;此文算是这段时间做Rancher服务扩展的心得，深度参与一个开源软件最终肯定会希望去改动它扩展它。
这也是客观需求所致，开源软件可以拿来即用，但是真正可用实用，必须加以改造，适应自身需求。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Rancher K8s Helm使用体验</title>
      <link>http://niusmallnan.com/2017/02/05/rancher-k8s-helm-practice</link>
      <pubDate>Sun, 05 Feb 2017 14:08:38 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2017/02/05/rancher-k8s-helm-practice</guid>
      <description>&lt;p&gt;潇潇洒洒玩一玩
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;之前的文章已经阐述了Rancher K8s中国区的使用优化，
&lt;a href=&#34;http://niusmallnan.com/2017/01/19/optimize-rancher-k8s-in-china/&#34;&gt;http://niusmallnan.com/2017/01/19/optimize-rancher-k8s-in-china/&lt;/a&gt;，
本文则是选取一个特殊的组件Helm，深入一些体验使用过程。Helm是K8s中类似管理catalog的组件，
在较新的Rancher K8s版本中，Helm已经默认集成可直接使用。&lt;/p&gt;

&lt;h3 id=&#34;初始化cli环境&#34;&gt;初始化CLI环境&lt;/h3&gt;

&lt;p&gt;如果已经习惯使用Rancher UI上自带的kubectl tab，那么可以跳过此步。
大部分玩家还是更喜欢在自己的机器上使用kubectl和helm CLI来做管理的。
在自己的机器上部署kubectl和helm命令行工具，有一个比较偷懒的方法，
就是直接到kubectld容器拷贝出来，主要过程如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# kubectrld容器ID为 42291346c064
$ docker cp 42291346c064:/usr/bin/kubectl /usr/local/bin/kubectl
$ docker cp 42291346c064:/usr/bin/helm /usr/local/bin/helm
$ docker cp 42291346c064:/tmp/.helm ~/.helm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然也不要忘记kubectl的配置文件，在Rancher UI上生成，然后拷贝到对应的本地目录上。&lt;br /&gt;
&lt;img src=&#34;https://ww1.sinaimg.cn/large/006tNc79ly1fcfkc0a5yjj30dx07edgb.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;基于helm部署mysql&#34;&gt;基于Helm部署Mysql&lt;/h3&gt;

&lt;p&gt;K8s官方的Charts中已经有了mysql这个应用，
这里&lt;a href=&#34;https://github.com/kubernetes/charts/tree/master/stable&#34;&gt;https://github.com/kubernetes/charts/tree/master/stable&lt;/a&gt;可以找到，
几乎所有的Chart都需要依赖PersistentVolumeClaim提供卷，所以在一切开始之前，
我们需要先创建一个PersistentVolume，来提供一个数据卷池。这里可以选择部署比较方便的NFS。&lt;/p&gt;

&lt;p&gt;选择一台主机，安装nfs-kernel-server，并做相应配置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ apt-get install nfs-kernel-server

# 配置卷
# 修改 /etc/exports，添加如下内容
/nfsexports *(rw,async,no_root_squash,no_subtree_check)

# 重启nfs-server
service nfs-kernel-server restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用kubectl创建PV，文件内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0003
  annotations:
    volume.beta.kubernetes.io/storage-class: &amp;quot;slow&amp;quot;
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  nfs:
    path: /nfsexports #NFS mount path
    server: 172.31.17.169 #NFS Server IP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装mysql之前，需要准备一份mysql chart的配置文件，也就是对应的values.yaml，
其他内容&lt;a href=&#34;https://github.com/kubernetes/charts/blob/master/stable/mysql/values.yaml&#34;&gt;https://github.com/kubernetes/charts/blob/master/stable/mysql/values.yaml&lt;/a&gt;基本不变，
主要修改persistence部分，这样所依赖的PVC才能bound到对应的PV上，如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;persistence:
  enabled: true
  storageClass: slow
  accessMode: ReadWriteMany
  size: 1Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一切准备妥当，就可以进行Mysql Chart的安装，执行过程如下：&lt;code&gt;helm install --name ddb -f values.yaml stable/mysql&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;安装完毕后，在Rancher UI和K8s Dashboard上都可以看到。&lt;br /&gt;
&lt;img src=&#34;https://ww1.sinaimg.cn/large/006tNc79ly1fcfkfju2ayj30k407q750.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
&lt;img src=&#34;https://ww3.sinaimg.cn/large/006tNc79ly1fcfkfsuoayj30ed05zaae.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>优化Rancher k8s中国区的使用体验</title>
      <link>http://niusmallnan.com/2017/01/19/optimize-rancher-k8s-in-china</link>
      <pubDate>Thu, 19 Jan 2017 17:32:56 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2017/01/19/optimize-rancher-k8s-in-china</guid>
      <description>&lt;p&gt;拯救一脸懵逼
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;Kubernetes（以下简称K8s）是Rancher平台重点支持的一个编排引擎，Rancher K8s具有部署灵活使用方便的特点，
而且Rancher基本是同步更新支持K8s的新版本新组件，用户也可以选择部署指定的K8s版本。
但是这一切的便利，身在中国的我们这些贱民无法深刻体验，万恶的GFW把很多部署的依赖挡在之外，
而服务全球开发者的Rancher平台亦不可能为中国用户单独定制，所以我只好自己动手丰衣足食。&lt;/p&gt;

&lt;h3 id=&#34;部署要点&#34;&gt;部署要点&lt;/h3&gt;

&lt;p&gt;部署之前的操作系统选型上，相对来说我比较推荐ubuntu+docker的组合，
毕竟这个组合在国外使用的用户比较多，相对来说bug fix的速度也是比较快的，
如果你是一个docker重度用户，应该深知docker本身的bug并不少。&lt;/p&gt;

&lt;p&gt;如果是部署一个新的Rancher环境，我推荐用下面的脚本来启动，通过设置DEFAULT_CATTLE_CATALOG_URL，
这样可以直接指定我定制过的Rancher K8s：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d --restart=unless-stopped \
     -e DEFAULT_CATTLE_CATALOG_URL=&#39;{&amp;quot;catalogs&amp;quot;:{&amp;quot;community&amp;quot;:{&amp;quot;url&amp;quot;:&amp;quot;https://github.com/rancher/community-catalog.git&amp;quot;,&amp;quot;branch&amp;quot;:&amp;quot;master&amp;quot;},&amp;quot;library&amp;quot;:{&amp;quot;url&amp;quot;:&amp;quot;https://github.com/niusmallnan/rancher-catalog.git&amp;quot;,&amp;quot;branch&amp;quot;:&amp;quot;k8s-cn&amp;quot;}}}&#39; \
     --name rancher-server \
     -p 8082:8080 rancher/server:stable
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然如果是已经部署的Rancher环境，那就需要在Rancher UI上，做一下简单的修改，
Disable已有的library catalog repo，指向我定制过的即可，注意branch的设置，网络状况不好的需要耐心等待重新拉取repo内容：
&lt;img src=&#34;https://ww3.sinaimg.cn/large/006y8lValy1fbw2toyl38j30s30a50u6.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在部署agent节点前，如果是一个干净的环境最好，但是如果是曾经做过agent节点，
尤其是之前部署过rancher k8s的，我强烈建议你执行一次大扫除，否则会出现各种意想不到的状况，
大扫除的脚本可以参考执行我的这个，具体都做了什么事可自行阅读：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash

docker rm -f $(docker ps -qa)

rm -rf /var/etcd/

for m in $(tac /proc/mounts | awk &#39;{print $2}&#39; | grep /var/lib/kubelet); do
    umount $m || true
done
rm -rf /var/lib/kubelet/

for m in $(tac /proc/mounts | awk &#39;{print $2}&#39; | grep /var/lib/rancher); do
    umount $m || true
done
rm -rf /var/lib/rancher/

rm -rf /run/kubernetes/

docker volume rm $(docker volume ls -q)

docker ps -a
docker volume ls
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;一切opensource&#34;&gt;一切OpenSource&lt;/h3&gt;

&lt;p&gt;如果你对我在其中的改动颇有疑虑，亦大可放心。我主要是改动两个地方：
fork了rancher-catalog建立了k8s-cn的分支，只要将Rancher的library catalog repo指向我的工程分支即可；
fork了kubernetes-package，每次Rancher K8s发布新版本，
我都会基于该版本建立一个CN分支（如：v1.5.1-rancher1-7-cn），
一切对于中国区的优化修改都会在这个分支上。最终我会更新出中国区的使用镜像，并push到镜像仓库上，
目前使用的是阿里云的镜像仓库（招牌比较大短时间内不会倒。。。）。&lt;/p&gt;

&lt;p&gt;参考链接：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/niusmallnan/rancher-catalog&#34;&gt;https://github.com/niusmallnan/rancher-catalog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/niusmallnan/kubernetes-package&#34;&gt;https://github.com/niusmallnan/kubernetes-package&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;后续支持计划&#34;&gt;后续支持计划&lt;/h3&gt;

&lt;p&gt;截止到本文写作之时，我刚开始支持rancher-k8s v1.5.1-rancher1-7版本，并在Rancher v1.3.1版本上做了测试。
后续Rancher官方发布新版本，我会进行同步更新，并做一些简单的测试。
后续考虑加入离线安装，可以指定本地镜像仓库，依赖镜像一键导入等方便的功能。
当然如果在使用中发现各种疑难杂症，可以发邮件给我niusmallnan@gmail.com。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Rancher-LB开启访问日志</title>
      <link>http://niusmallnan.com/2017/01/06/rancher-lb-logs</link>
      <pubDate>Fri, 06 Jan 2017 15:12:10 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2017/01/06/rancher-lb-logs</guid>
      <description>&lt;p&gt;拯救一脸懵逼
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;在Rancher v1.2之前的版本，Rancher LB也就是Haproxy都是放在agent/instance内的，由于这个镜像本身要支持很多功能，
所以要开启Haproxy的访问日志比较麻烦。因为Harpoxy的日志和其他Nginx之类略有不同，它是通过syslog协议讲日志发送出去的，
要想展现日志还要开启syslog进行收集。Rancher v1.2版本开始，
Rancher LB的功能已经解耦成独立的镜像rancher/lb-service-haproxy，如此日志收集的工作就可以做了。&lt;/p&gt;

&lt;h3 id=&#34;开启访问日志&#34;&gt;开启访问日志&lt;/h3&gt;

&lt;p&gt;lb-service-haproxy的实现可以仔细阅读源码&lt;a href=&#34;https://github.com/rancher/lb-controller&#34;&gt;https://github.com/rancher/lb-controller&lt;/a&gt;，
简单的说，其内置了rsyslog来收集haproxy的日志，在容器内部查看配置文件&lt;strong&gt;/etc/haproxy/rsyslogd.conf&lt;/strong&gt;，
可以知道其开启了udp 8514端口，不同的级别的日志发送到不同的文件中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ModLoad imudp.so
$UDPServerRun 8514
$template CustomFormat,&amp;quot;%timegenerated% %HOSTNAME% %syslogtag%%msg%\n&amp;quot;
$ActionFileDefaultTemplate CustomFormat

/* info */
if $programname contains &#39;haproxy&#39; and $syslogseverity == 6 then (
    action(type=&amp;quot;omfile&amp;quot; file=&amp;quot;/var/log/haproxy/traffic&amp;quot;)
)
if $programname contains &#39;haproxy&#39; and $syslogseverity-text == &#39;err&#39; then (
    action(type=&amp;quot;omfile&amp;quot; file=&amp;quot;/var/log/haproxy/errors&amp;quot;)
)
/* notice */
if $programname contains &#39;haproxy&#39; and $syslogseverity &amp;lt;= 5 then (
    action(type=&amp;quot;omfile&amp;quot; file=&amp;quot;/var/log/haproxy/events&amp;quot;)
)

*.* /var/log/messages
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关于syslog的Severity level可以参考这里：&lt;a href=&#34;https://en.wikipedia.org/wiki/Syslog&#34;&gt;https://en.wikipedia.org/wiki/Syslog&lt;/a&gt;，
所以我们要查看的访问日志，应该就是/var/log/haproxy/traffic文件。
同时还添加了logrotate的配置进行日志分割和压缩，可查看容器内的&lt;strong&gt;/etc/logrotate.d/haproxy&lt;/strong&gt;文件。&lt;/p&gt;

&lt;p&gt;当前lb-service-haproxy版本默认是不开启日志收集的，需要自己手动开启，
我们都知道Haproxy开启日志需要在其global和defaults配置中添加log的配置。
Rancher LB是支持custom haproxy.cfg的，所以在Rancher中添加这两块配置就可以这样：&lt;br /&gt;
&lt;img src=&#34;https://ww2.sinaimg.cn/large/006tKfTcjw1fbgy5m62l3j30i80ecgn1.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
Rancher会自动合并这几条配置，最终在lb-service-haproxy容器内生成的配置就是这样：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;global
    log 127.0.0.1:8514 local0
    log 127.0.0.1:8514 local1 notice
    chroot /var/lib/haproxy
    daemon
    group haproxy
    maxconn 4096
.....
.....
defaults
    log global
    option tcplog
    errorfile 400 /etc/haproxy/errors/400.http
    errorfile 403 /etc/haproxy/errors/403.http
.....
.....
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;那么查看之前分析的日志目录文件，可以看到响应的访问日志信息：&lt;br /&gt;
&lt;img src=&#34;https://ww1.sinaimg.cn/large/006tKfTcjw1fbgya5nb4vj30l309vn1r.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;注意事项&#34;&gt;注意事项&lt;/h3&gt;

&lt;p&gt;日志虽然可以这样灵活的开启，但是使用时需要注意：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;日志文件放在容器内会不断增大，虽然有logrotate，但是扛不住日积月累。
临时测试用本地可以，但是长远看还是发送到外部syslog服务上比较好。&lt;/li&gt;
&lt;li&gt;lb-service-haproxy的早期版本是默认就开启了访问日志，v0.4.5版本后取消了这个默认配置，
所以你就得使用我上面所说的方式，早期的lb-service-haproxy会积存很多访问日志，
所以尽早升级到v0.4.5以上的版本。&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    
    <item>
      <title>Dead容器导致Rancher Server过载的解决方案</title>
      <link>http://niusmallnan.com/2017/01/01/rancher-server-overwhelmed-by-dead-containers</link>
      <pubDate>Sun, 01 Jan 2017 07:42:28 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2017/01/01/rancher-server-overwhelmed-by-dead-containers</guid>
      <description>&lt;p&gt;接之前的一篇文章&lt;a href=&#34;http://niusmallnan.com/2016/12/27/docker-device-resource-busy/&#34;&gt;device or resource busy&lt;/a&gt;，
其中描述了Docker的一个bug，这个bug其实会导致Rancher Server出现一个比较严重的问题。
&lt;/p&gt;

&lt;h3 id=&#34;现象与原因&#34;&gt;现象与原因&lt;/h3&gt;

&lt;p&gt;Rancher Server运行一段时间后，如若发现各种操作卡死，Host添加不上，Stack&amp;amp;Service也无法active，
且在proceeses列表里发现大量volumestoragepoolmap.remove任务，可以参考本文的解决方案：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tKfTcjw1fbateimvjnj30iv0a40ug.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这些无法完成的volumestoragepoolmap.remove任务，背后其实就是那些dead containers，
分析之后，大致原因如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;docker本身的一个bug，导致一些容器删除失败，进入dead状态。&lt;/li&gt;
&lt;li&gt;Rancher的调度系统在不断重试删除这些dead容器，但是无法删除。&lt;/li&gt;
&lt;li&gt;无法删除导致，这些删除任务不断加入rancher调度系统中，造成调度系统过载。&lt;/li&gt;
&lt;li&gt;主机创建或是Stack/Service active请求都在排队，不能被迅速执行。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;临时解决方案&#34;&gt;临时解决方案&lt;/h3&gt;

&lt;p&gt;依据之前的分析，processes里面出现了很多永远无法完成的任务，这些任务在不断的重试，这会压垮Rancher Server。
所以解决方案的目标就是，介入外部力量删除这些dead容器，升级docker engine是一个选择，
但是直接在生产环境升级是一个需要勇气的选择。比较温和的做法是定制脚本到每台agent节点上
强行删除这些dead容器。&lt;/p&gt;

&lt;p&gt;考虑到这里有多台主机批量执行的场景，所以我们可以选择使用ansible工具，
创建一个工作目录如/opt/agent_upgrade，所有脚本都放到这个目录下。
首先我们要生成一个agent host的列表，这里可以借用Rancher的CLI工具，
具体脚本get_hosts.sh如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
#set -ex
rm -f all_hosts_ip

RANCHER=&amp;quot;/usr/local/bin/rancher&amp;quot;

for env in `$RANCHER env -a --format {{.ID}}`; do
    for host in `$RANCHER --env $env hosts --format json| jq .Host.agentIpAddress| sed &#39;s/\&amp;quot;//g&#39;`; do
        echo &amp;quot;${host}&amp;quot; &amp;gt;&amp;gt; all_hosts_ip
    done
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;定义一个ansible的配置文件，由于所有agent host都是有运维专有访问秘钥的，
所以这里直接定义private_key_file比较方便，如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[defaults]
hostfile=/opt/agent_upgrade/all_hosts_ip
host_key_checking = False
remote_user = ubuntu
private_key_file=/opt/agent_upgrade/aws-prd.pem
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后我们就可以使用ansible来做批量操作了，删除dead容器的脚本非常简单，
clean_dead_containers.sh脚本如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash

#set -ex
ansible all --sudo -m shell -a &amp;quot;docker ps -aq --filter status=dead &amp;gt; ~/dead&amp;quot; -vvvv
ansible all --sudo -m shell -a &amp;quot;cat ~/dead | xargs docker rm -f&amp;quot; -vvvv
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于Rancher集群在运行时仍然会产生这些dead containers，所以我们可以借用crontab来做定时清理，
添加一条crontab如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;*/30 * * * * cd /opt/agent_upgrade &amp;amp;&amp;amp; sh get_hosts.sh &amp;amp;&amp;amp; sh clean_dead_containers.sh &amp;gt; /tmp/dead_containers.log 2&amp;gt;&amp;amp;1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如此，每隔30分钟定期清理这些dead containers，Rancher Server就不会受其影响。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Rancher v1.2 Swarmkit的实现</title>
      <link>http://niusmallnan.com/2016/12/14/rancher12-swarmkit-architect</link>
      <pubDate>Wed, 14 Dec 2016 21:53:27 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/12/14/rancher12-swarmkit-architect</guid>
      <description>&lt;p&gt;Rancher v1.2系列文章，已独家授权Rancher Labs，转载请联系Rancher China相关人员。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;Rancher v1.2更新了之前对Swarm的支持，与Docker一样抛弃了就有的Swarm，
选择支持Swarmkit。Swarmkit引擎非常轻量级，由于其内置早Docker Engine中，
所以部署起来会非常方便。虽然目前Swarmkit引擎还在不断发展，而且bug也很多，
但是它也有其擅长的使用场景，比如简单的CI/CD场景，它会非常灵活简洁。
本文将带大家体验一下，Rancher v1.2对Swarmkit的支持。&lt;/p&gt;

&lt;h3 id=&#34;部署与使用&#34;&gt;部署与使用&lt;/h3&gt;

&lt;p&gt;部署方面秉承Rancher一贯的原则，非常简单，只需要在创建Env时选择Sawrm即可。&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tKfTcjw1favae6x30jj30nn09odgr.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
Env创建完毕后，会看到多个Infra Service需要创建，这时候和其他引擎一样，
我们需要向Env中添加Host。我们知道Swarm的node有两种：Manager和Worker。
Rancher创建的Swarm集群默认是3个Manager，多个Manager内有一个是Leader，
另外两个备用。这样单个Host出问题，新的Leader会很快选举出来，保证集群的稳定性。
比如我添加了两个Host，默认是先添加Manager角色，
所以2个Host都会以Manager方式添加，如下图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tKfTcjw1favaeq4hrhj30hc074aav.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
进入其中一台Host内，查看swarm集群状态，可以看到一个是Leader，另外一个Reachable做备用。&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tKfTcjw1favaey8g8qj30fc01wq3g.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
尝试创建一个简单的程序，查看与UI上的联动效果，如图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tKfTcjw1favaf8yytqj30eb05vdgv.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
如果使用Swarmkit的自定义网络方式，情况如何？虽然在UI上显示无IP，
但是进入容器内部可以看到overlay对应的网卡，如图所示：
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tKfTcjw1favafscwv8j30ii07habk.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;实现原理&#34;&gt;实现原理&lt;/h3&gt;

&lt;p&gt;那么Rancher是如何来完成Swarmkit的部署和联动呢？Rancher中Swarmkit也是基于Cattle来部署的，
根据之前的文章分析，我们可以知道Rancher的基础设施编排的定义都是通过catalog中的infra-templates实现的，
Swarmkit比较特殊它是在community-catalog中定义的，如果一直在rancher-catalog中寻找肯定找不到。
compose文件中定义了一个service swarmkit-mon，如图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tKfTcjw1favaggy2snj30iz0at40g.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
如果探究原理，我们就需要知道swarmkit-mon对应的镜像是如何定义的。
rancher/swarmkit这个dockerfile并没有在&lt;a href=&#34;https://github.com/rancher&#34;&gt;https://github.com/rancher&lt;/a&gt;下面的项目中，
这个需要顺藤摸瓜，找到该Dockerfile的维护者（其实也是Rancher的一名员工），
最终地址是&lt;a href=&#34;https://github.com/LLParse/swarmkit-catalog&#34;&gt;https://github.com/LLParse/swarmkit-catalog&lt;/a&gt;。如图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tKfTcjw1favahpg2pzj30kc0aiq57.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
swarmkit-mon中内置了docker，并映射了Host上的docker.sock，
这样可以在swarmkit-mon容器中控制docker创建swarmkit集群。swarmkit-mon的实现比较简单，
主要包括两个shell脚本：run.sh负责swarmkit集群的管理和Rancher的联动，
agent节点信息需要通过rancher-metadata读取，设置Host Label则直接调用Rancher API；
health.sh负责监控swarmkit节点的状态（通过与docker.sock通信读取Swarm.LocalNodeState的状态），
并与giddyup协作暴露健康检查端口，这样可以利用Rancher Cattle的healthcheck来保证swarmkit-mon服务的高可用性，
每个Host的swarmkit-mon出问题时可以进行自动重建恢复。原理如图：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tKfTcjw1favaidbyfgj30bh08qmyh.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;目前来看，由于Kubernetes的发展的确迅猛，所以Rancher的更多精力都放在K8s上。
针对Swarmkit的支持显得略显单薄，但是Swarmkit本身的问题也很多，目前也难以应对复杂场景，
所以目前的支持力度应该是足够了。后续对docker1.13版本的Swarmkit支持也在持续迭代中。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>为什么Rancher v1.2中netstat看不到开放端口</title>
      <link>http://niusmallnan.com/2016/12/09/rancher-netstat-miss-port</link>
      <pubDate>Fri, 09 Dec 2016 15:07:14 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/12/09/rancher-netstat-miss-port</guid>
      <description>&lt;p&gt;拯救一脸懵逼。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;随着Rancher v1.2的发布，越来越多的伙伴参与到新特性的尝鲜中。有时不免会碰到一些问题，
比如网络不通之类的，这时通常我们都会下意识的使用netstat命令查看端口是否正确开启，
可是出现的结果却让伙伴们一脸疑问。比如我们使用了ipsec网络，
netstat命令却看不到UDP 500和4500端口的开放，这是为什么呢？本文将释疑这个问题。&lt;/p&gt;

&lt;h3 id=&#34;问题现象&#34;&gt;问题现象&lt;/h3&gt;

&lt;p&gt;启动之前的Rancher v1.1版本并对比v1.2版本，可以看到如下现象，
ipsec网络正常，但在新版中netstat却看不到端口开放：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tKfTcjw1fakk2e84hcj30d509z76g.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
用LB暴露一个服务访问，UI上明明显示了端口，但是在netstat中却看不到：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tKfTcjw1fakk3aasx6j30j7075my6.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
这些现象是为什么？&lt;/p&gt;

&lt;h3 id=&#34;原生docker和k8s可以看到端口&#34;&gt;原生docker和k8s可以看到端口&lt;/h3&gt;

&lt;p&gt;原先的docker，一般run一个容器加入-p参数，就可以看到端口。
这其实是由当前tcp/ip栈内的docker-proxy开放的，如图：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tKfTcjw1fakk45w280j30ju06t0v2.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
k8s是可以通过netstat看到端口，举个例子，创建一个service暴露NodePort，如下图：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tKfTcjw1fakk4l1qplj308z069wer.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
而通过netstat命令却能看到对应的端口，这是因为这个端口是由kube-proxy开放的，
就是说在这个tcp/ip栈内有应用程序bind了这个端口：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tKfTcjw1fakk4z18gzj30j605jtah.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;本质原因&#34;&gt;本质原因&lt;/h3&gt;

&lt;p&gt;端口的本质其实是应用程序，就是说需要有process bind port，
而netstat要能显示出开放端口，则必须在当前的namespace里有应用程序监听了端口才行。
Rancher中因为是使用CNI模型，虽然也是使用docker0，但是这个docker0是CNI Bridge，
并不和docker-proxy有协作，所以端口并没有通过docker-proxy暴露，
只是通过iptables dnat转发到其他的namespace上。在当前的namespace里使用netstat看不到开放的端口，
iptables dnat规则如下：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tKfTcjw1fakk5qzfr6j30jg03idh7.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;有时候惯性思维让我们忽略了事物的本质，让人匪夷所思的问题往往是很简单的道理。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>go-machine-service访问Rancher API的授权机制分析</title>
      <link>http://niusmallnan.com/2016/12/05/go-machine-service-access-api-key</link>
      <pubDate>Mon, 05 Dec 2016 11:14:19 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/12/05/go-machine-service-access-api-key</guid>
      <description>&lt;p&gt;一段分析调用机制的旅程。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;深更半夜一位道上的朋友突然微信问我，Rancher Server内Cattle之外的服务如何调用Rancher API。
调用API本身没问题，但是Rancher API是需要授权访问的，
也就是说Rancher Server内其他服务如何授权访问API，这个机制是什么样的？
Rancher Server内有很多服务，如catalog、go-machine-service、websocket-proxy等等，
本文就以go-machine-service为例说明其授权机制。&lt;/p&gt;

&lt;h3 id=&#34;以go-machine-service为例&#34;&gt;以go-machine-service为例&lt;/h3&gt;

&lt;p&gt;首先我们要了解go-machine-service在做什么事，官方文档里很清楚的描述到，
它实现了physicalhost相关事件的handler，主要包括：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;physicalhost.create - Calls machine create &amp;hellip;.&lt;/li&gt;
&lt;li&gt;physicalhost.activate - Runs docker run rancher/agent &amp;hellip;&lt;/li&gt;
&lt;li&gt;physicalhost.delete|purge - Calls machine delete &amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;而若实现这些handler，需要和rancher events打交道，
我们认为rancher events也是rancher api的一部分，
所以访问rancher events也需要和API服务一样的鉴权信息。
go-machine-service在启动之时，很明确的读取了CATTLE相关变量：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006y8lVajw1fafqwxo6pyj30ir0ccdhx.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;而且rancher还给go-machine-service专门创建了相应的API Key，查看DB可以得知：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006y8lVajw1fafqxbv1wbj30f90iwgnb.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;进入rancher-server容器内部，查看go-machine-service进程的环境变量，
可以看到API Key的授权信息就在其中：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006y8lVajw1fafqxofpl1j30m805dmz0.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;那么go-machine-service的环境变量是如何设置的呢？查看Cattle源码，找到MachineLancher，
它是启动go-machine-service服务的具体实现，我们可以看到设置环境变量这一步：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006y8lVajw1fafqy3ywfbj30gg03tt9w.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;Cattle本身就是一个框架，Rancher Server内的其他服务都是通过Cattle的Proxy机制来转发请求的，
授权机制则是由Cattle统一提供出来，其他服务自身无需单独实现授权机制。
如果我们想对Rancher Server扩展，再增加一些其他服务，利用本文所描述的机制，就会非常方便。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>实践指南-快速解锁Rancher v1.2</title>
      <link>http://niusmallnan.com/2016/12/02/rancher12-run-on-laptop</link>
      <pubDate>Fri, 02 Dec 2016 21:31:53 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/12/02/rancher12-run-on-laptop</guid>
      <description>&lt;p&gt;Rancher v1.2系列文章，已独家授权Rancher Labs，转载请联系Rancher China相关人员。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;Rancher v1.2已经发布，相信众多容器江湖的伙伴们正魔拳擦准备好好体验一番。
由于Docker能够落地的操作系统众多，各种Docker版本不同的Graph driver，
所以通常大版本的第一个release都会在兼容性上有一些小问题。
为了更好的体验Rancher v1.2的完整特性，我们选取了Rancher测试比较严格的运行环境。
手握众多服务器资源的devops们可以飘过此文，身背MBP或Windows笔记本的Sales/Pre-Sales们可以品读一番。&lt;/p&gt;

&lt;h3 id=&#34;基础软件安装&#34;&gt;基础软件安装&lt;/h3&gt;

&lt;p&gt;首先需要安装基础软件，由于Rancher v1.2已经支持Docker v1.2，
所以可以直接使用Docker的Mac或Windows版（以下以Mac为例），
下载地址：&lt;a href=&#34;https://www.docker.com/&#34;&gt;https://www.docker.com/&lt;/a&gt;。在Mac上，
Docker会使用xhyve轻量级虚拟化来保证一个Linux环境，所以可以把Rancher Server直接运行起来。&lt;/p&gt;

&lt;p&gt;因为要在MBP上添加多个Host组成小集群，所以需要用虚拟化扩展多个节点添加到Rancher集群中。
这里可以使用docker-machine控制VirtualBox来添加节点，
VirtualBox下载地址：&lt;a href=&#34;https://www.virtualbox.org/wiki/Downloads&#34;&gt;https://www.virtualbox.org/wiki/Downloads&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;在Host节点的操作系统上，可以选取RancherOS，我们的目标是快速体验新特性，
而Rancher Labs在Rancher和RancherOS的相互兼容性上是做了大量测试的，
这样可以避免我们少进坑，直接体验新特性。
RancherOS下载地址：&lt;a href=&#34;https://github.com/rancher/os&#34;&gt;https://github.com/rancher/os&lt;/a&gt;，推荐使用最新release版本。&lt;/p&gt;

&lt;p&gt;在用docker-machine驱动VirtualBox来创建Host时，可以指定操作系统ISO的URL路径，
由于我们使用RancherOS，所以最好把RancherOS放到本机HTTP服务器内。
MBP内自带Apache HTTPD，将Apache的vhosts模块开启，并添加配置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 开启vhost /etc/apache2/httpd.conf
# 以下两行的默认注释去掉
LoadModule vhost_alias_module libexec/apache2/mod_vhost_alias.so
Include /private/etc/apache2/extra/httpd-vhosts.conf

# vhost的配置 /etc/apache2/extra/httpd-vhosts.conf
# DocumentRoot目录就是在用户根目录下创建Sites
# 如用户名niusmallnan，则DocumentRoot就是/Users/niusmallnan/Sites
&amp;lt;VirtualHost *:80&amp;gt;
    DocumentRoot &amp;quot;/Users/niusmallnan/Sites&amp;quot;
    ServerName localhost
    ErrorLog &amp;quot;/private/var/log/apache2/sites-error_log&amp;quot;
    CustomLog &amp;quot;/private/var/log/apache2/sites-access_log&amp;quot;
    common
    &amp;lt;Directory /&amp;gt;
        Options Indexes FollowSymLinks MultiViews
        AllowOverride None
        Order allow,deny
        Allow from all
        Require all granted
    &amp;lt;/Directory&amp;gt;
&amp;lt;/VirtualHost&amp;gt;

# 重启 Apache
$ sudo apachectl restart

# 拷贝 RancherOS的ISO 到 DocumentRoot
$ cp rancheros.iso /Users/niusmallnan/Sites/
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;rancher安装&#34;&gt;Rancher安装&lt;/h3&gt;

&lt;p&gt;首先打开Docker，并配置registry mirror，配置完成后重启Docker。
mirror的服务可以去各个公用云厂商申请一个，比如我这里使用的是阿里云的registry mirror，
如图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tKfTcjw1fav9vw30d1j30jh082403.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;打开terminal，安装Rancher Server：&lt;br /&gt;
&lt;code&gt;$ docker run -d --restart=unless-stopped -p 8080:8080 rancher/server:stable&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;若要添加Host节点，则需要通过docker-machine创建Host，
这里使用的规格是2核2G（具体可根据自身MBP的性能调整），脚本（add_ros_host.sh）参考如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/usr/bin/env bash
ROS_ISO_URL=&#39;http://127.0.0.1/rancheros.iso&#39;
ROS_CPU_COUNT=2
ROS_MEMORY=2048
docker-machine create -d virtualbox \
        --virtualbox-boot2docker-url $ROS_ISO_URL \
        --virtualbox-cpu-count $ROS_CPU_COUNT \
        --virtualbox-memory $ROS_MEMORY \
        $1
docker-machine ls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;添加节点则需执行：&lt;br /&gt;
&lt;code&gt;$ ./add_ros_host.sh ros-1&lt;/code&gt;&lt;br /&gt;
添加完成后，可以进入虚机内进行设置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker-machine ls
NAME  ACTIVE DRIVER     STATE     URL                       SWARM DOCKER ERRORS 
ros-1 -      virtualbox Running   tcp://192.168.99.100:2376       v1.12.3

# 进入VM中
$ docker-machine ssh ros-1
# RancherOS内设置registry mirror
$ sudo ros config set rancher.docker.extra_args \
        &amp;quot;[&#39;--registry-mirror&#39;,&#39;https://s06nkgus.mirror.aliyuncs.com&#39;]&amp;quot;
$ sudo system-docker restart docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于我们要使用VirtualBox的虚机组成一个小集群，所以建议把Rancher的Host Registration URL
设置为&lt;a href=&#34;http://192.168.99.1:8080&#34;&gt;http://192.168.99.1:8080&lt;/a&gt;，如下图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tKfTcjw1fav9z6dih7j30eo072gmf.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
添加Rancher agent的时候也要注意，CATTLE_AGENT_IP参数要设置成虚机内192.168.99.0/24网段的IP，
如下图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tKfTcjw1fav9zo3g5zj30hp06tmyk.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
如此就可以基本完全解锁Rancher v1.2的各种功能了，完整演示各种特性。&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;Docker目前版本分支众多，虽然最新的v1.13即将发布，但是各个公司的使用版本应该说涵盖了v1.9到v1.12，
而且Docker graph driver也有很多，再加上很多的LinuxOS，可以说使用Docker而产生组合有很多种，
这就会带来各种各样的兼容性问题，因此导致的生产环境故障会让人头疼不已。
当然如果纯粹基于演示和调研新功能，我们可以优先兼容性较好的选择。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Rancher v1.2网络架构解读</title>
      <link>http://niusmallnan.com/2016/12/01/rancher12-networking</link>
      <pubDate>Thu, 01 Dec 2016 21:18:36 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/12/01/rancher12-networking</guid>
      <description>&lt;p&gt;Rancher v1.2系列文章，已独家授权Rancher Labs，转载请联系Rancher China相关人员。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;在之前的Rancher版本上，用户经常诟病Rancher的网络只有IPsec，没有其他选择。
而容器社区的发展是十分迅猛的，各种容器网络插件风起云涌，欲在江湖中一争高下。
Rancher v1.2版本中与时俱进，对之前的网络实现进行了改造，支持了CNI标准，
除IPsec之外又实现了呼声比较高的VXLAN网络，同时增加了CNI插件管理机制，
让我们可以hacking接入其他第三方CNI插件。本文将和大家一起解读一下Rancher v1.2中网络的实现。&lt;/p&gt;

&lt;h3 id=&#34;rancher-net-cni化&#34;&gt;Rancher-net CNI化&lt;/h3&gt;

&lt;p&gt;以最简单最快速方式部署Rancher并添加Host，以默认的IPsec网络部署一个简单的应用后，
进入应用容器内部看一看网络情况，对比一下之前的Rancher版本：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tKfTcjw1fav9di74b7j30iv06tgoo.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
我们最直观的感受便是，网卡名从eth0到eth0@if8有了变化，原先网卡多IP的实现也去掉了，
变成了单纯的IPsec网络IP。这其实就引来了我们要探讨的内容，虽然网络实现还是IPsec，
但是rancher-net组件实际上是已经基于CNI标准了。最直接的证明就是看一下，
rancher-net镜像的Dockerfile：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tKfTcjw1fav9e5komyj30ju06o40i.jpg&#34; alt=&#34;&#34; /&gt;
熟悉CNI规范的伙伴都知道/opt/cni/bin目录是CNI的插件目录，bridge和loopback也是CNI的默认插件，
这里的rancher-bridge实际上和CNI原生的bridge没有太大差别，只是在幂等性健壮性上做了增强。
而在IPAM也就是IP地址管理上，Rancher实现了一个自己的rancher-cni-ipam，它的实现非常简单，
就是通过访问rancher-metadata来获取系统给容器分配的IP。
Rancher实际上Fork了CNI的代码并做了这些修改，&lt;a href=&#34;https://github.com/rancher/cni&#34;&gt;https://github.com/rancher/cni&lt;/a&gt;。
这样看来实际上，rancher-net的IPsec和Vxlan网络其实就是基于CNI的bridge基础上实现的。&lt;/p&gt;

&lt;p&gt;在解释rancher-net怎么和CNI融合之前，我们需要了解一下CNI bridge模式是怎么工作的。
举个例子，假设有两个容器nginx和mysql，每个容器都有自己的eth0，
由于每个容器都是在各自的namespace里面，所以互相之间是无法通信的，
这就需要在外部构建一个bridge来做二层转发，
容器内的eth0和外部连接在容器上的虚拟网卡构建成对的veth设备，这样容器之间就可以通信了。
其实无论是docker的bridge还是cni的bridge，这部分工作原理是差不多的，如图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tKfTcjw1fav9ffzi1sj30ch08q750.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;那么我们都知道CNI网络在创建时需要有一个配置，这个配置用来定义CNI网络模式，
读取哪个CNI插件。在这个场景下也就是cni bridge的信息，
这个信息rancher是通过rancher-compose传入metadata来控制的。
查看ipsec服务的rancher-compose.yml可以看到，type使用rancher-bridge，
ipam使用rancher-cni-ipam，bridge网桥则复用了docker0，
有了这个配置我们甚至可以随意定义ipsec网络的CIDR，如下图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tKfTcjw1fav9g15ofmj30c80anab3.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;ipsec服务实际上有两个容器：一个是ipsec主容器，内部包含rancher-net服务和ipsec需要的charon服务；
另一个sidekick容器是cni-driver，它来控制cni bridge的构建。两端主机通过IPsec隧道网络通信时，
数据包到达物理网卡时，需要通过Host内的Iptables规则转发到ipsec容器内，
这个Iptables规则管理则是由network-manager组件来完成的，
&lt;a href=&#34;https://github.com/rancher/plugin-manager&#34;&gt;https://github.com/rancher/plugin-manager&lt;/a&gt;。其原理如下图所示（以IPsec为例）：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tKfTcjw1fav9gp2vkwj30hj06pacf.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
整体上看cni ipsec的实现比之前的ipsec精进了不少，而且也做了大量的解耦工作，
不单纯是走向社区的标准，之前大量的Iptables规则也有了很大的减少，性能上其实也有了很大提升。&lt;/p&gt;

&lt;h3 id=&#34;rancher-net-vxlan的实现&#34;&gt;Rancher-net vxlan的实现&lt;/h3&gt;

&lt;p&gt;那么rancher-net的另外一个backend vxlan又是如何实现的呢？
我们需要创建一套VXLAN网络环境来一探究竟，默认的Cattle引擎网络是IPsec，
如果修改成VXLAN有很多种方式，可以参考我下面使用的方式。&lt;/p&gt;

&lt;p&gt;首先，创建一个新的Environment Template，把Rancher IPsec禁用，同时开启Rancher VXLAN，如下图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tKfTcjw1fav9hpxm04j30ft09wmya.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
然后，我们创建一个新的ENV，并使用刚才创建的模版Cattle-VXLAN，创建完成后，
添加Host即可使用。如下图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tKfTcjw1fav9i7um3fj30d7083js5.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
以分析IPsec网络实现方式来分析VXLAN，基本上会发现其原理大致相同。
同样是基于CNI bridge，使用rancher提供的rancher-cni-bridge、rancher-cni-ipam，
网络配置信息以metadata方式注入。区别就在于rancher-net容器内部，
rancher-net激活的是vxlan driver，它会生成一个vtep1042设备，并开启udp 4789端口，
这个设备基于udp 4789构建vxlan overlay的两端通信，对于本机的容器通过eth0走bridge通信，对于
其他Host的容器，则是通过路由规则转发到vtep1042设备上，再通过overlay到对端主机，
由对端主机的bridge转发到相应的容器上。整个过程如图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tKfTcjw1fav9j4nhblj30e607z0um.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
vxlan实现的原理可更多参考：&lt;a href=&#34;https://www.kernel.org/doc/Documentation/networking/vxlan.txt&#34;&gt;https://www.kernel.org/doc/Documentation/networking/vxlan.txt&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;容器网络是容器云平台中很重要的一环，对于不通规模不通的安全要求会有不同的选型。
Rancher的默认网络改造成了CNI标准，同时也会支持其他第三方CNI插件，
结合Rancher独有的Environment Template功能，用户可以在一个大集群中的每个隔离环境内，
创建不同的网络模式，以满足各种业务场景需求，这种管理的灵活性是其他平台没有的。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Rancher v1.2基础设施引擎整体架构</title>
      <link>http://niusmallnan.com/2016/11/28/rancher12-infra-architect</link>
      <pubDate>Mon, 28 Nov 2016 20:53:44 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/11/28/rancher12-infra-architect</guid>
      <description>&lt;p&gt;Rancher v1.2系列文章，已独家授权Rancher Labs，转载请联系Rancher China相关人员。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;Rancher v1.2可以说是一个里程碑版本，发布时间虽屡次跳票让大家心有不爽，
但是只要体会其新版功能，会发现这个等待绝对是值得的。从架构角度看，
用两个字来概括就是“解耦”，基础设施引擎的分离，agent节点的服务粒度更细；
从产品角度看，给了用户更多定制的空间，Rancher依然秉持着全部OpenSource的理念；
在开发语言上，之前遗留的通过shell脚本方式的粗糙实现也都基于Golang重写，
解耦的新服务也几乎使用Golang开发，agent节点全线基于Golang这也为后续便利地支持ARM埋下伏笔。
在市场选择上，Rancher依然在kubernetes下面投入了大量精力，引入了万众期待的CNI plugin管理机制，
坚持要做最好用的Kubernetes发行版。本文就带着大家从架构角度总览Rancher v1.2版本的特性。&lt;/p&gt;

&lt;h3 id=&#34;总览&#34;&gt;总览&lt;/h3&gt;

&lt;p&gt;在v1.2版本的整体架构图（如下图所示）上，Cattle引擎向下深入演化成了基础设施引擎，
这一点上在v1.1时代也早有体现。Cattle更多得作为基础设施的管理工具，
可以用它来部署其他服务和编排引擎，当然它本身编排能力还是可以使用的，
习惯了stack-service的朋友仍然可以继续使用它，同时rancher scheduler的引入也大大增强了其调度能力。
Rancher仍然支持Kubernetes、Mesos、Swarm三大编排引擎，Kubernetes可以支持到较新的v1.4.6版本，
由于所有的部署过程的代码都是开放的，用户依然可以自己定制部署版本。值得一提的是，
Rancher支持了新版的Swarm Mode也就是Swarmkit引擎，这也意味着Rancher可以在Docker1.12上部署，
不小心装错Docker版本的朋友这回可以放心了。&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tKfTcjw1fav8rinilzj30hj09p0ur.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
在存储方面，Rancher引入了Kubernetes社区的flexvol来做存储插件的管理，
同时也支持Docker原生的volume plugin机制，并实现了对AWS的EFS&amp;amp;EBS以及标准NFS的支持，
先前的Convoy应该会被抛弃，Rancher最终还是选择参与社区标准。在网络方面，
除了CNI插件机制的引入，用户还可以使用rancher-net组件提供的vxlan网络替代先前的ipsec网络。
在可定制性方面，还体现在Rancher提供了用户可以自定义rancher-lb的机制，
如果特殊场景下默认的Haproxy不是很给力时，用户可以自定义使用nginx、openresty或者traefik等等。
下面便做一下详细分解。&lt;/p&gt;

&lt;h3 id=&#34;基础设施引擎&#34;&gt;基础设施引擎&lt;/h3&gt;

&lt;p&gt;初次安装v1.2版本，会发现多了Infrastructure（如下图所示）的明显标识，
默认的Cattle引擎需要安装healthcheck、ipsec、network-services、scheduler等服务。
这个是有rancher-catalog来定义的，&lt;a href=&#34;https://github.com/rancher/rancher-catalog&#34;&gt;https://github.com/rancher/rancher-catalog&lt;/a&gt;，
新分离出来了infra-templates和project-templates：infra-templates就是Rancher定义的各种基础设施服务，
包括基础服务和编排引擎；project-templates对应的是Env初始化时默认安装的服务，
它可以针对不同的编排引擎进行配置。&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tKfTcjw1fav8t4d321j30dl0b275b.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
以Cattle引擎为例，可以在project-templates的Cattle目录中找到相应的配置文件，
当ENV创建初始化时会创建这里面定义的服务，这样一个机制就可以让我们可以做更深入的定制，
让ENV初始化时创建我们需要的服务：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;name: Cattle
description: Default Cattle template
stacks:
- name: network-services
templateId: &#39;library:infra*network-services&#39;
- name: ipsec
templateId: &#39;library:infra*ipsec&#39;
- name: scheduler
templateId: &#39;library:infra*scheduler&#39;
- name: healthcheck
templateId: &#39;library:infra*healthcheck&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在Cattle引擎调度方面，Rancher实现了rancher-scheduler，&lt;a href=&#34;https://github.com/rancher/scheduler&#34;&gt;https://github.com/rancher/scheduler&lt;/a&gt;。
它实现了允许用户按计算资源调度，目前支持memory、cpu的Reservation。其实现原理是，
内部有一个resource watcher，通过监听rancher metadata的获取Host的使用资源数据变化，
进而得到ENV内所有Host资源汇总信息。与此同时，
监听rancher events的scheduler.prioritize、scheduler.reserve、scheduler.release等各种事件，
通过排序过滤可用主机后发送回执信息，Rancher Server就有了可以选择的Host列表。如下图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tKfTcjw1fav8vafyqaj30h005xwfk.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
需要注意的是，rancher-scheduler并没有和rancher-server部署在一起，
而是在你添加Host时候部署在agent节点上，当然rancher-scheduler在一个ENV内只会部署一个。&lt;/p&gt;

&lt;h3 id=&#34;agent节点服务解耦&#34;&gt;Agent节点服务解耦&lt;/h3&gt;

&lt;p&gt;agent节点一个最大的变化就是，agent-instance容器没有了，它被拆分成多个容器服务，
包括rancher-metadata、network-manager、rancher-net、rancher-dns、healthcheck等。&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tKfTcjw1fav8w3brm9j30ea06labt.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;metadata服务是老朋友了，它在每个agent节点上保存了host、stack、service、container等的信息，
可以非常方便的在本地调用取得，&lt;a href=&#34;https://github.com/rancher/rancher-metadata&#34;&gt;https://github.com/rancher/rancher-metadata&lt;/a&gt;，
在新的体系中它扮演了重要角色，几乎所有agent节点上的服务均依赖它。
在先前的体系中，metadata的answer file的更新是通过事件驱动shell脚本来执行下载，
比较简单粗暴。v1.2开始使用监听rancher events方式来reload metadata的answer file，
但是answer file还需要到rancher server端下载，总的来说效率还是有一定提升的。&lt;/p&gt;

&lt;p&gt;dns在新的体系中仍然承担着服务发现的功能，&lt;a href=&#34;https://github.com/rancher/rancher-dns&#34;&gt;https://github.com/rancher/rancher-dns&lt;/a&gt;。
除了拆分成单独容器之外，它也在效率上做了改进，它与rancher-metadata容器共享网络，
以metadata的结果生成dns的answer file。与之前的架构相比，
省去了dns answer file下载的过程。需要注意的是，rancher-dns的TTL默认是600秒，
如果出于各种原因觉得dns作为服务发现不是很可靠，那么可以使用etc-host-updater和rancher-metadata的组合，
&lt;a href=&#34;https://github.com/rancher/etc-host-updater&#34;&gt;etc-host-updater&lt;/a&gt;
会根据metadata数据动态生成hosts文件并写入容器内，这样通过服务名访问时，
其实已经在本地转换成了IP，无需经过dns，如下图所示：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tKfTcjw1fav8z58gm6j30bc04mmxg.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;rancher-net作出了比较重大的革新，&lt;a href=&#34;https://github.com/rancher/rancher-net&#34;&gt;https://github.com/rancher/rancher-net&lt;/a&gt;，
除了继续支持原有的ipsec外，还支持了vxlan。这个支持是原生支持，
只要内核有vxlan的支持模块就可以。vxlan并不是Cattle的默认网络，
使用时可以在infra-catalog中重新选择它来部署，其实现以及部署方式后续会在专门的文章中进行探讨：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tKfTcjw1fav8zxs951j30ds0al753.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;network-manager的引入为Rancher v1.2带来了一个重要特性就是CNI插件管理，
在之前的版本中很多用户都提到rancher-net本身的网络无法满足业务需求。
容器网络之争，无非就是CNM与CNI，Rancher选择站队CNI，这也是为了更好与Kubernetes融合。
而CNI的插件很多种，Calico、Weave等之流，每个插件的部署方式都不一样。
Rancher为了简化管理提出了network-manager，&lt;a href=&#34;https://github.com/rancher/plugin-manager&#34;&gt;https://github.com/rancher/plugin-manager&lt;/a&gt;，
它可以做到兼容主流的CNI插件，它实际上定义了一个部署框架，让CNI插件在框架内部署。
network-manager是以容器方式部署，由于每种插件在初始化时可能需要暴露端口或加入一些NAT规则，
所以network-manager能够动态设置不同插件的初始化规则，
它的做法是以metadata作为 host port和host nat规则的数据源，
然后获取数据后生成相应的Iptables规则加入的Host中。而对于真正的CNI插件，
需要在network-manager容器内/opt/cni目录下部署对应cni插件的执行程序（calico/weave），
/etc/cni目录下部署cni插件的配置，这两个目录映射了docker卷rancher-cni-driver，
也就是Host上的/var/lib/docker/volumes/rancher-cni-driver目录下。&lt;/p&gt;

&lt;p&gt;关于healthcheck，先前是通过agent-instance镜像实现，里面内置了Haproxy，
事件驱动shell脚本来下载healchcheck配置并reload。新的架构中，
Rancher实现了单独的healthcheck，&lt;a href=&#34;https://github.com/rancher/healthcheck&#34;&gt;https://github.com/rancher/healthcheck&lt;/a&gt;，
采用Golang微服务的方式，数据源是metadata。
当然healthcheck的最终检查仍然是通过与Haproxy sock通信来查看相应member的健康状态（原理如下图），
healthcheck的实现主要是为了将其从agent-instance中解耦出来。&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tKfTcjw1fav928q6bwj30hu06mad4.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;Rancher v1.2的新特性还是非常多的，基础设施引擎的变化时一切特性的基础，
这篇文章算是开篇之作。后续会持续为大家带来，Kubernetes、Swarmkit的支持，自定义rancher-lb，
vxlan的支持，各种CNI插件的集成，以及各种存储接入的实践操作指南等等。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>如何hack一下rancher k8s</title>
      <link>http://niusmallnan.com/2016/10/08/rancher-k8s-hacking</link>
      <pubDate>Sat, 08 Oct 2016 16:09:24 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/10/08/rancher-k8s-hacking</guid>
      <description>&lt;p&gt;Rancher可以轻松实现Kubernetes的部署，尽管默认的部署已经完全可用，
但是如果我们想修改部署的K8s版本，这时应当如何应对？
&lt;/p&gt;

&lt;h3 id=&#34;原理分析与执行&#34;&gt;原理分析与执行&lt;/h3&gt;

&lt;p&gt;在Rancher中，由于K8s是基于Cattle引擎来部署，所以在K8s在部署完成之后，
我们可以通过Link Graph来很清晰的看到整体的部署情况。&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tNc79jw1fa0ybyprq4j30mi07tdh0.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;既然基于Cattle引擎部署，也就是说需要两个compose文件，
k8s引擎的compose文件放在&lt;a href=&#34;https://github.com/rancher/rancher-catalog/tree/master/templates&#34;&gt;https://github.com/rancher/rancher-catalog/tree/master/templates&lt;/a&gt;下面，
这里面有两个相关目录kubernetes与k8s，k8s是Rancher1.2开始使用的，
而kubernetes则是Rancher1.2之后开始使用的。&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tNc79jw1fa0ydzd1dgj30gt07qmyg.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;为了我们可以自己hack一下rancher k8s的部署，我们可以在github上fork一下rancher-catalog，
同时还需要修改一下Rancher中默认的catalog的repo地址，
这个可以在&lt;a href=&#34;http://rancher-server/v1/settings&#34;&gt;http://rancher-server/v1/settings&lt;/a&gt;页面下，
寻找名为 catalog.url 的配置项，然后进入编辑修改。比如我这里将library库的地址换成了自己的：
&lt;a href=&#34;https://github.com/niusmallnan/rancher-catalog.git&#34;&gt;https://github.com/niusmallnan/rancher-catalog.git&lt;/a&gt;&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tNc79jw1fa0yff7sbzj30h206imyy.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;此时，我们就可以修改了，找一个比较实用的场景。我们都知道k8s的pod都会依赖一个基础镜像，
这个镜像默认的地址是被GFW挡在墙外了，一般我们会把kubelet的启动参数调整一下，
以便重新指定这个镜像地址，比如指定到国内的镜像源上。&lt;br /&gt;
&lt;strong&gt;&amp;ndash;pod-infra-container-image=index.tenxcloud.com/google_containers/pause:2.0&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;如果我们要让rancher k8s部署时自动加上该参数，
可以直接修改私有rancher-catalog中的k8s compose文件。&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tNc79jw1fa0ygscal6j30gv0asjtp.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;修改之后稍等片刻（主要是为了让rancher-server更新到新的catalog compose文件），
添加一个k8s env并在其中添加host，k8s引擎就开始自动部署，
部署完毕后，我们可以看到Kubernetes Stack的compose文件，
已经有了&amp;ndash;pod-infra-container-image这个启动参数。&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tNc79jw1fa0yhg3alsj30gn09rtav.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
如此我们在添加pod时再也不用手动导入pod基础镜像了。&lt;/p&gt;

&lt;p&gt;在compose file中，部署k8s的基础镜像是rancher/k8s，这个镜像的Dockerfile在rancher维护的k8s分支中，
如在rancher-k8s 1.2.4分支中可以看到：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tNc79jw1fa0yi5rmplj30e007ogmr.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这样如果想对rancher-k8s发行版进行深度定制，就可以重新build相关镜像，通过rancher-compose来部署自己的发行版。&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;本文写于Rancher1.2行将发布之际，1.2版本是非常重大的更新，Rancher会支持部署原生的K8s版本，
同时CNI网络和Cloud Provider等都会以插件方式，用户可以自己定义，并且在UI上都会有很好的体现。
只要了解Rancher部署K8s的原理和过程，我们就可以定制非常适合自身使用的k8s，
通过Rancher来部署自定义的k8s，我们就可以很容易的扩展了k8s不擅长的UI、Catalog、
用户管理、审计日志维护等功能，这也是本文的目的。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Rancher HA部署实践</title>
      <link>http://niusmallnan.com/2016/09/23/rancher-ha-practice</link>
      <pubDate>Fri, 23 Sep 2016 17:12:27 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/09/23/rancher-ha-practice</guid>
      <description>&lt;p&gt;Rancher1.1版本HA结构部署实践记录
&lt;/p&gt;

&lt;h3 id=&#34;过程记录&#34;&gt;过程记录&lt;/h3&gt;

&lt;p&gt;以三节点为例，节点信息：
ubuntu(aufs)+docker1.11.2+rancher1.1.x
Haproxy+Mysql 放在同一节点 使用VM
Rancher HA Node 三个节点 使用baremetal（也可以使用VM，建议使用4核8G以上flavor）&lt;/p&gt;

&lt;p&gt;先部署Haproxy：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d --name rancher-haproxy \
           -v /opt/conf/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro \
           -p 80:80 \
           haproxy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;文件 /opt/conf/haproxy.cfg参考：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;global
        log 127.0.0.1 local0
        log 127.0.0.1 local1 notice
        maxconn 4096
        maxpipes 1024
        daemon

defaults
        log     global
        mode    tcp
        option  tcplog
        option  dontlognull
        option  redispatch
        option http-server-close
        option forwardfor
        retries 3
        timeout connect 5000
        timeout client 50000
        timeout server 50000

frontend default_frontend
        bind *:80
        mode http

        default_backend rancher-ha-node

backend rancher-ha-node
        mode http
        server r-ha-1 xx.xx.xx.xx
        server r-ha-2 xx.xx.xx.xx
        server r-ha-3 xx.xx.xx.xx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建容器化的mysql：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run --name rancher-mysql \
    -v /opt/data/mysql:/var/lib/mysql \
    -e MYSQL_ROOT_PASSWORD=root \
    -p 3306:3306 \
    -d mysql:5.5

mysql -uroot -proot
CREATE DATABASE IF NOT EXISTS cattle COLLATE = &#39;utf8_general_ci&#39; CHARACTER SET = &#39;utf8&#39;;
GRANT ALL ON cattle.* TO &#39;cattle&#39;@&#39;%&#39; IDENTIFIED BY &#39;cattle&#39;;
GRANT ALL ON cattle.* TO &#39;cattle&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;cattle&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;正常运行rancher，配置并下载HA脚本，如果不想用HTTPS，
记得把&lt;strong&gt;CATTLE_HA_HOST_REGISTRATION_URL&lt;/strong&gt;的值换成HTTP的，再执行脚本。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>