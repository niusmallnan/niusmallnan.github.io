<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Post-rsses on Niusmallnan</title>
    <link>http://niusmallnan.com/post/index.xml</link>
    <description>Recent content in Post-rsses on Niusmallnan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <copyright>Niusmallnan</copyright>
    <lastBuildDate>Thu, 14 Feb 2019 14:06:57 +0800</lastBuildDate>
    <atom:link href="http://niusmallnan.com/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>[后记] runc cve-2019-5736</title>
      <link>http://niusmallnan.com/2019/02/14/runc-cve-2019-5736-afterword</link>
      <pubDate>Thu, 14 Feb 2019 14:06:57 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2019/02/14/runc-cve-2019-5736-afterword</guid>
      <description>&lt;p&gt;有些话不知当讲不当讲
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;runc是一个根据OCI(Open Container Initiative)标准创建并运行容器的CLI tool，目前docker引擎内部也是基于runc构建的。
2019年2月11日，研究人员通过oss-security邮件列表披露了runc容器逃逸漏洞的详情，根据OpenWall的规定EXP会在7天后也就是2019年2月18日公开。&lt;/p&gt;

&lt;p&gt;经过几天的咨询内容轰炸，相信大家已经对漏洞已经有了初步的了解，甚至在Github上已经有人提交了EXP代码。Rancher在第一时间完成了补丁修复，并向企业用户推送的修复方案。
同时在我们也收到了大量来自社区用户在后台的提问，为了疏解种种谜团，我们特意再发布一篇文章，选择一些比较热点的问题进行回复。&lt;/p&gt;

&lt;h3 id=&#34;热点问题&#34;&gt;热点问题&lt;/h3&gt;

&lt;h4 id=&#34;非特权容器也能发起攻击吗&#34;&gt;非特权容器也能发起攻击吗？&lt;/h4&gt;

&lt;p&gt;答案是肯定的，Rancher安全团队做了一些测试，即使运行容器时不使用privileged参数，一样可以发起攻击。因为这个漏洞核心要素在于，容器内的用户是否对runc有访问权限，
容器内默认是root用户，只是这个root是受限制的root，但是它是具有对runc的访问权限，所以一定可以发起攻击。&lt;/p&gt;

&lt;h4 id=&#34;主机上不用root用户启动容器可以避免攻击吗&#34;&gt;主机上不用root用户启动容器可以避免攻击吗？&lt;/h4&gt;

&lt;p&gt;答案是无法避免，如上一个问题分析，它和容器内的用户有关，至于在主机上以什么用户启动无关。Rancher安全团队在Ubuntu系统上做了测试，即使使用ubuntu用户启动容器，
依然可以完成对runc的替换。&lt;/p&gt;

&lt;h4 id=&#34;更新官方docker的注意事项&#34;&gt;更新官方Docker的注意事项&lt;/h4&gt;

&lt;p&gt;Docker也在第一时间发布了两个版本18.06.2和18.09.2，这两个版本都可以修复runc漏洞，但是你需要注意的是他们都只兼容4.x内核，如果你的系统依然使用的3.x内核，
请谨慎使用，因为它基本不会起作用，甚至可能导致额外的问题。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Ubuntu 14.04 customers using a 3.13 kernel will need to upgrade to a supported Ubuntu 4.x kernel
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参考两个版本的RN：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/engine/release-notes/#18062&#34;&gt;https://docs.docker.com/engine/release-notes/#18062&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/engine/release-notes/#18092&#34;&gt;https://docs.docker.com/engine/release-notes/#18092&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;kubernetes用户怎么办&#34;&gt;Kubernetes用户怎么办？&lt;/h4&gt;

&lt;p&gt;使用K8s的用户都很清楚，K8s并不能兼容太高的Docker版本，所以更新官方Docker版本是很难的一件事，为此K8s官方特意发表了一篇Blog：&lt;a href=&#34;https://kubernetes.io/blog/2019/02/11/runc-and-cve-2019-5736/&#34;&gt;https://kubernetes.io/blog/2019/02/11/runc-and-cve-2019-5736/&lt;/a&gt; 。
主要思想就是，不要在容器中使用root，它推荐的方案是使用PodSecurityPolicy。当然很多用户修改PodSecurityPolicy后可能会引发各种问题，所以它也推荐用户更新Docker。
同时它也提到，不能更新Docker的用户，可以使用Rancher提供的方案，Rancher为每个版本都移植了补丁：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;If you are unable to upgrade Docker, the Rancher team has provided backports of the fix for many older versions at github.com/rancher/runc-cve.
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;如何使用rancher提供的补丁&#34;&gt;如何使用Rancher提供的补丁？&lt;/h4&gt;

&lt;p&gt;如上一个问题提到的，用户可以直接访问 &lt;a href=&#34;https://github.com/rancher/runc-cve&#34;&gt;https://github.com/rancher/runc-cve&lt;/a&gt; 来获取方案，值得一提的是Rancher为3.x和4.x内核用户都提供了补丁版本。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;To install, find the runc for you docker version, for example Docker 17.06.2 for amd64 will be runc-v17.06.2-amd64. 
For Linux 3.x kernels use the binaries that end with no-memfd_create. Then replace the docker-runc on your host with the patched one.
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;如何正确使用exp&#34;&gt;如何正确使用EXP？&lt;/h4&gt;

&lt;p&gt;首先不建议大家广泛传播EXP，因为它每暴露一次，就为整体环境增加了一丝风险，我们可以研究学习但是不要恶意传播。
我们在后台看到有些人问到，他们使用了某些EXP代码，攻击没有成功，想知道是不是自己的系统是安全的，不用考虑升级。
Rancher安全团队也查看了一些外部公开的EXP，有些EXP是不完整的，它可能只能在某些环境上起作用。
比如利用libseccomp的EXP，就无法在静态编译的runc上起作用，我们使用了一些公开的EXP就无法在RancherOS上完成攻击。
虽然不同版本的Docker都使用runc，但是不同的操作系统使用runc的方式不同，有的使用static runc，有的使用dynamic runc。
所以不能以某些公开的EXP的执行结果为标准，来判断自己系统是否存在漏洞。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Runc CVE-2019-5736以及RancherOS v1.5.1发布</title>
      <link>http://niusmallnan.com/2019/02/12/rancheros-v151-release</link>
      <pubDate>Tue, 12 Feb 2019 14:24:44 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2019/02/12/rancheros-v151-release</guid>
      <description>&lt;p&gt;一言不合就发新版本
&lt;/p&gt;

&lt;p&gt;runc是一个根据OCI(Open Container Initiative)标准创建并运行容器的CLI tool，目前docker引擎内部也是基于runc构建的。
2019年2月11日，研究人员通过oss-security邮件列表披露了runc容器逃逸漏洞的详情，根据OpenWall的规定EXP会在7天后也就是2019年2月18日公开。&lt;/p&gt;

&lt;p&gt;此漏洞允许以root身份运行的容器以特权用户身份在主机上执行任意代码。实际上，这意味着容器可能会破坏Docker主机(覆盖runc CLI)。
所需要的只是容器中允许使用root。攻击者可以使用受感染的Docker镜像或对未受感染的正在运行的容器运行exec命令。针对此问题的已知缓解措施包括：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用只读主机文件系统运行&lt;/li&gt;
&lt;li&gt;运行用户命名空间&lt;/li&gt;
&lt;li&gt;不在容器中运行root&lt;/li&gt;
&lt;li&gt;正确配置的AppArmor / SELinux策略（当前的默认策略不够）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;收到披露邮件后，RancherOS团队尝试编写了攻击脚本，在一个普通容器中运行一个非常简单的脚本就完成了对主机的攻击，将主机上的runc替换成了其他程序。&lt;/p&gt;

&lt;p&gt;Docker在第一时间发布了&lt;a href=&#34;https://github.com/docker/docker-ce/releases/tag/v18.09.2&#34;&gt;18.09.2&lt;/a&gt;，用户可升级到此版本以修复该漏洞。
但是通常由于各种因素，用户的生产环境并不容易升级太新的Docker版本，Rancher Labs已经将修复程序反向移植到所有版本的Docker。
相关修补程序以及安装说明，请参考&lt;a href=&#34;https://github.com/rancher/runc-cve&#34;&gt;rancher/run-cve&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;RancherOS作为一款容器化操作系统，其中很多组件依赖runc，我们也在第一时间更新了补丁并发布了v1.5.1和v1.4.3两个版本。&lt;/p&gt;

&lt;h3 id=&#34;rancheros的更新&#34;&gt;RancherOS的更新&lt;/h3&gt;

&lt;p&gt;RancherOS的核心部件system-docker和user-docker都依赖runc，所以v1.5.1和v1.4.3都对他们进行了更新。而针对user-docker，RancherOS可以切换各种版本的docker engine，
所以我们对一下docker engine都进行了反向移植：&lt;strong&gt;v1.12.6/v1.13.1/v17.03.2/v17.06.2/v17.09.1/v17.12.1/v18.03.1/v18.06.1&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;默认安装v1.5.1和v1.4.3，补丁程序已经是内置的，你无需任何操作就可以避免该漏洞。如果你希望使用早起的docker版本，那么切换user-docker时，请使用上面提到的补丁修复版本：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@ip-172-31-2-241:~# ros engine list
disabled docker-1.12.6
disabled docker-1.13.1
disabled docker-17.03.1-ce
disabled docker-17.03.2-ce
disabled docker-17.06.1-ce
disabled docker-17.06.2-ce
disabled docker-17.09.0-ce
disabled docker-17.09.1-ce
disabled docker-17.12.0-ce
disabled docker-17.12.1-ce
disabled docker-18.03.0-ce
disabled docker-18.03.1-ce
disabled docker-18.06.0-ce
enabled  docker-18.06.1-ce
disabled docker-18.09.0
disabled docker-18.09.1
disabled docker-18.09.2

root@ip-172-31-2-241:~# ros engine switch docker-17.03.2-ce
...

root@ip-172-31-2-241:~# docker info | grep Server
Server Version: 17.03.2-ce
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同时v1.5.1版本也是支持docker 18.09.2，你可以切换到该版本，如果你考虑使用Docker官方的修复版本。只需简单运行: &lt;code&gt;ros engine switch docker-18.09.2&lt;/code&gt;。&lt;/p&gt;

&lt;h3 id=&#34;v1-5-1-和-v1-4-3&#34;&gt;v1.5.1 和 v1.4.3&lt;/h3&gt;

&lt;p&gt;推荐您使用最新的v1.5.1版本，除了修复CVE-2019-5736漏洞外还支持其他新特性以及一些Bug Fix。当然，仍然有很多用户在使用1.4.x版本，所以我们也发布了v1.4.3，
它只修复了runc漏洞，没有其他额外的更新。&lt;/p&gt;

&lt;p&gt;AWS相关镜像已经上传到各个region中，可以直接搜索查找并使用，包括AWS中国区。其他主要镜像列表参考：&lt;a href=&#34;https://github.com/rancher/os/blob/v1.5.x/README.md#release&#34;&gt;https://github.com/rancher/os/blob/v1.5.x/README.md#release&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;更多新特性和Bug Fix请参考v1.5.1的&lt;a href=&#34;https://github.com/rancher/os/releases/tag/v1.5.1&#34;&gt;Release Notes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;文档说明：&lt;a href=&#34;https://rancher.com/docs/os/v1.x/en/&#34;&gt;https://rancher.com/docs/os/v1.x/en/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;最后，RancherOS还是一个小众的开源项目，我们专注Docker在Linux上的精简体验，如果喜欢RancherOS，请下载使用，非常期待收到您的反馈。
同时Github上的star，也是鼓励我们继续前行的精神动力。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>RancherOS v1.5.0发布</title>
      <link>http://niusmallnan.com/2019/01/06/rancheros-v150-release</link>
      <pubDate>Sun, 06 Jan 2019 17:00:46 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2019/01/06/rancheros-v150-release</guid>
      <description>&lt;p&gt;一言不合就发新版本
&lt;/p&gt;

&lt;p&gt;年关将至，寒意习习，落叶萧萧下，阳光日日稀。RancherOS团队历时两个来月的开发，正式发布RancherOS v1.5.0版本。
在此期间同为Container Linux阵营的CoreOS已经从红帽再入IBM，潮流之变，业界态势，让我们无不更加努力去争得一席之地。
无论是商业用户的积累，还是业界变化带来的社区用户增长，都在催促我们不断革新，应该说1.5.0版本是用户的需求推着我们走出来的。&lt;/p&gt;

&lt;h3 id=&#34;重大特性更新&#34;&gt;重大特性更新&lt;/h3&gt;

&lt;p&gt;本版本的新特征众多，无法一次性全部说明，以下只表述一些用户关注度比较高的特性。个别特性详细说明，我们会不断推出文章一一展开。&lt;/p&gt;

&lt;h4 id=&#34;启动性能提升&#34;&gt;启动性能提升&lt;/h4&gt;

&lt;p&gt;一直以来RancherOS的initrd一直采用xz格式压缩，随着RancherOS的体积不断增大，xz压缩越来越影响系统启动速度。虽然xz格式能够带来比较小的initrd和ISO，
但是我们也需要兼顾启动速度。v1.5.0版本的initrd已经采用了gzip格式，文件体积有所增大，但是启动速度有了质的飞跃。
同时我们也优化了system-docker的镜像加载和cloud-init的启动，对启动速度进行了深度优化。&lt;/p&gt;

&lt;h4 id=&#34;luks磁盘加密支持&#34;&gt;LUKS磁盘加密支持&lt;/h4&gt;

&lt;p&gt;支持LUKS，允许用户对跟磁盘分区进行加密，在一些特殊场景下增强了RancherOS的安全性。运行效果参考下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/4208881/47795869-ee87a780-dd2b-11e8-91be-4ac8f685d8fa.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;wifi和4g支持&#34;&gt;WiFi和4G支持&lt;/h4&gt;

&lt;p&gt;Intel正在micro PC领域不断发力，RancherOS被纳入其生态体系，我们支持了WiFi和4G网络，用户可以通过简单的cloud-config配置就可以开启，
带来了十分简洁的用户体验，这部分我们会在后续其他文章中详细介绍。&lt;/p&gt;

&lt;h4 id=&#34;hyper-v支持&#34;&gt;Hyper-V支持&lt;/h4&gt;

&lt;p&gt;很多社区用户一直希望能在Hyper-V使用RancherOS，先前我们一直提供给用户一些custom build的方式来实现它，现在我们正式支持了它，并会持续维护。
无论是docker-machine方式还是boot from ISO方式均可以支持。&lt;/p&gt;

&lt;p&gt;下一个版本我们也会带来RancherOS的Azure Cloud支持。&lt;/p&gt;

&lt;h4 id=&#34;多docker-engine支持&#34;&gt;多docker engine支持&lt;/h4&gt;

&lt;p&gt;这是一个很有趣的特性，目前RancherOS中默认拥有一个user docker。在v1.5.0中，用户可以用过ROS CLI来创建多个user docker engine，
并且每个docker拥有独立的ROOTFS和网络栈，并且可以在console很容易的切换使用任意一个docker。&lt;/p&gt;

&lt;p&gt;当然我们并不推荐您在生产中使用，我们的某个商业客户把这个特性应用在其CI环境中，极大的提升了资源的利用率，减少了物理机器数量的开销。&lt;/p&gt;

&lt;h4 id=&#34;改善vmware的支持&#34;&gt;改善VMware的支持&lt;/h4&gt;

&lt;p&gt;RancherOS的广大用户中Vmware是占有很大的用户群，之前我们的版本中只针对docker-machine方式做了一些改善，但是很多用户还希望使用boot from ISO方式和VMDK方式，
我们相关的镜像也做了支持，用户可以饿直接下载使用它：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[VMDK] &lt;a href=&#34;https://releases.rancher.com/os/v1.5.0/vmware/rancheros.vmdk&#34;&gt;https://releases.rancher.com/os/v1.5.0/vmware/rancheros.vmdk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[Docker Machine] &lt;a href=&#34;https://releases.rancher.com/os/v1.5.0/rancheros-vmware.iso&#34;&gt;https://releases.rancher.com/os/v1.5.0/rancheros-vmware.iso&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[Boot From ISO] &lt;a href=&#34;https://releases.rancher.com/os/v1.5.0/vmware/rancheros.iso&#34;&gt;https://releases.rancher.com/os/v1.5.0/vmware/rancheros.iso&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;arm的支持&#34;&gt;ARM的支持&lt;/h4&gt;

&lt;p&gt;由于Rancher和ARM已经开始了战略合作，我们会在一起做很多有趣的事。RancherOS的ARM支持也是其中的一部分，原先我们只是对RPi做了支持，
现在我们提供ARM版本的initrd和vmlinuz，用户可以用它们使用iPXE方式启动：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://releases.rancher.com/os/v1.5.0/arm64/initrd&#34;&gt;https://releases.rancher.com/os/v1.5.0/arm64/initrd&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://releases.rancher.com/os/v1.5.0/arm64/vmlinuz&#34;&gt;https://releases.rancher.com/os/v1.5.0/arm64/vmlinuz&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们依然只会对ARM64支持，且v1.5.0的ARM支持只是实验性质的，并不推荐应用在生产中。
我们会和ARM进行合作进行更广泛的测试，后续的版本将会是更稳定的。&lt;/p&gt;

&lt;h3 id=&#34;更加友好的自定义&#34;&gt;更加友好的自定义&lt;/h3&gt;

&lt;p&gt;社区中越来越多的发烧友并不局限使用我们的正式发布版本，他们会根据自己的需求修改RancherOS，构建自己的RancherOS。
我们提供了一些友好的编译选项，用户可以自定义自己的RancherOS。&lt;/p&gt;

&lt;h4 id=&#34;更改默认docker-engine&#34;&gt;更改默认docker engine&lt;/h4&gt;

&lt;p&gt;RancherOS的每个版本都会有自己设定的默认docker engine，而在用户的场景下，可能需要一个内部认可的docker engine，且希望它是RancherOS默认的版本。
那么用户可以在构建时候指定docker engine版本，来构建自己的RancherOS，以docker 17.03.2为例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;USER_DOCKER_VERSION=17.03.2 make release
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;更改默认console&#34;&gt;更改默认console&lt;/h4&gt;

&lt;p&gt;RancherOS支持很多console，比如ubuntu、alpine、centos等，由于我们的default console基于busybox，有些用户并不喜欢它，且不希望每次都去切换console。
那么用户可以使用这种方式构建一个默认console是自己喜欢的版本，以alpine console为例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ OS_CONSOLE=alpine make release
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;其他&#34;&gt;其他&lt;/h3&gt;

&lt;p&gt;AWS相关镜像已经上传到各个region中，可以直接搜索查找并使用，包括AWS中国区。其他主要镜像列表参考：&lt;a href=&#34;https://github.com/rancher/os/blob/v1.5.x/README.md#release&#34;&gt;https://github.com/rancher/os/blob/v1.5.x/README.md#release&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;更多新特性和Bug Fix请参考v1.5.0的&lt;a href=&#34;https://github.com/rancher/os/releases/tag/v1.5.0&#34;&gt;Release Notes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;文档说明：&lt;a href=&#34;https://rancher.com/docs/os/v1.x/en/&#34;&gt;https://rancher.com/docs/os/v1.x/en/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;最后，RancherOS还是一个小众的开源项目，我们专注Docker在Linux上的精简体验，如果喜欢RancherOS，请在Github上给我们一个star，鼓励我们继续前行。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>RancherOS v1.2.0发布</title>
      <link>http://niusmallnan.com/2018/02/07/rancheros-v120-release</link>
      <pubDate>Wed, 07 Feb 2018 16:52:31 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2018/02/07/rancheros-v120-release</guid>
      <description>&lt;p&gt;一言不合就发新版本
&lt;/p&gt;

&lt;p&gt;RancherOS v1.2.0版本于北京时间2月7日正式发布，从v1.1到v1.2开发周期中，我们收集到了社区用户和商业用户的Bug report和Feature request，
感谢大家为此作出的贡献。这个周期内，Meltdown和Spectre漏洞曝出给OS界造成了沉重的打击，我
们也时刻紧跟业界动向，第一时间把漏洞补丁更新到RancherOS中。&lt;/p&gt;

&lt;h3 id=&#34;spectre-var-2-漏洞修复&#34;&gt;Spectre Var.2 漏洞修复&lt;/h3&gt;

&lt;p&gt;对于Spectre变种2，我们采用了新的GCC编译器开启Retpoline指令重新编译了内核。而Intel的微码补丁，我们并没有采用，因为业界对这个补丁诟病很深，
已经造成了很多云厂商的Crash问题。&lt;/p&gt;

&lt;p&gt;基于最新的RancherOS v1.2.0可以使用这种方式检测Meltdown和Spectre的修复状态：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo system-docker run --rm -it -v /:/host niusmallnan/spectre-meltdown-checker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://ws4.sinaimg.cn/large/006tKfTcgy1fo80fauf1fj319s0v8gqy.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;不再支持arm32&#34;&gt;不再支持ARM32&lt;/h3&gt;

&lt;p&gt;考虑到32位操作系统发展情况，以及其他Linux发行版的支持情况，不再对ARM 32位进行硬件支持。
ARM 64位依然会继续支持，你依然可以继续在合适的树莓派型号上使用它。&lt;/p&gt;

&lt;p&gt;树莓派镜像：&lt;a href=&#34;https://github.com/rancher/os/releases/download/v1.2.0/rancheros-raspberry-pi64.zip&#34;&gt;https://github.com/rancher/os/releases/download/v1.2.0/rancheros-raspberry-pi64.zip&lt;/a&gt;
安装方式参考：&lt;a href=&#34;https://www.raspberrypi.org/documentation/installation/installing-images/&#34;&gt;https://www.raspberrypi.org/documentation/installation/installing-images/&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;各个console更新&#34;&gt;各个Console更新&lt;/h3&gt;

&lt;p&gt;RancherOS支持多种console，比如Ubuntu、Debian、Alpine等等，默认使用busybox console。
Busybox console基于&lt;a href=&#34;https://github.com/buildroot/buildroot&#34;&gt;Buildroot&lt;/a&gt;，本次发布更新了较新的稳定版本。&lt;/p&gt;

&lt;p&gt;其他console均更新成LTS版本，版本如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Alpine: 3.7&lt;/li&gt;
&lt;li&gt;CentOS: 7.4.1708&lt;/li&gt;
&lt;li&gt;Debian: stretch&lt;/li&gt;
&lt;li&gt;Fedora: 27&lt;/li&gt;
&lt;li&gt;Ubuntu: xenial&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;其他&#34;&gt;其他&lt;/h3&gt;

&lt;p&gt;AWS相关镜像已经上传到各个region中，可以直接搜索查找并使用，AWS中国区目前还不支持。其他主要镜像列表：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;阿里云镜像：&lt;a href=&#34;https://github.com/rancher/os/releases/download/v1.2.0/rancheros-aliyun.vhd&#34;&gt;https://github.com/rancher/os/releases/download/v1.2.0/rancheros-aliyun.vhd&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GCE镜像：&lt;a href=&#34;https://github.com/rancher/os/releases/download/v1.2.0/rancheros-gce.tar.gz&#34;&gt;https://github.com/rancher/os/releases/download/v1.2.0/rancheros-gce.tar.gz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;OpenStack镜像：&lt;a href=&#34;https://github.com/rancher/os/releases/download/v1.2.0/rancheros-openstack.img&#34;&gt;https://github.com/rancher/os/releases/download/v1.2.0/rancheros-openstack.img&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;更多新特性和Bug Fix请参考v1.2.0的&lt;a href=&#34;https://github.com/rancher/os/releases/tag/v1.2.0&#34;&gt;Release Notes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;文档说明：&lt;a href=&#34;http://rancher.com/docs/os/v1.2/en/&#34;&gt;http://rancher.com/docs/os/v1.2/en/&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>如何在RancherOS上更新microcode</title>
      <link>http://niusmallnan.com/2018/01/22/update-microcode-on-rancheros</link>
      <pubDate>Mon, 22 Jan 2018 17:16:17 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2018/01/22/update-microcode-on-rancheros</guid>
      <description>&lt;p&gt;RancherOS上更新microcode
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;进来公布Google的Project Zero团队公布的Spectre和Meltdown漏洞，真可谓一石激起千层浪，
业内各个硬件软件厂商开源社区都在积极参与漏洞修复。Meltdown是最早被修复的，在Kernel中开启KPTI就可以缓解该问题，
Spectre则依然在火热进行中。而作为漏洞的始作俑者Intel，Spectre漏洞的Intel官方缓解方案也于近日放出。
当然目前很多更新并非真正的修复，而是在最大程度抵御和缓解相关攻击。&lt;/p&gt;

&lt;h3 id=&#34;更新microcode&#34;&gt;更新microcode&lt;/h3&gt;

&lt;p&gt;通过更新microcode和系统补丁可以缓解Spectre Var. 2漏洞，也就是CVE-2017-5715 branch target injection。
除了在BIOS中可以更新microcode之外，Kernel也提供了更新microcode的机制。本文只关注在如何在RancherOS上更新microcode。&lt;/p&gt;

&lt;p&gt;下载最新的Intel microcode版本，注意microcode一般并不是适用所有的CPU，比如&lt;a href=&#34;https://downloadcenter.intel.com/download/27431/Linux-Processor-Microcode-Data-File&#34;&gt;20180108&lt;/a&gt;这个版本，主要是以下CPU才会起作用：&lt;br /&gt;
&lt;img src=&#34;https://ws1.sinaimg.cn/large/006tNc79ly1fnpsvdzc3rj30wu1byn1l.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;下载并解压缩之后，你会发现一个是intel-ucode目录下一些文件，另外一个单独的microcode.dat文件。
前者是支持热加载方式，也是现在比较推荐的方式；后者是传统更新方式，需要在initrd中加入microcode加载。
对于RancherOS，前者比较合适，使用起来相对简单。&lt;/p&gt;

&lt;p&gt;若要在内核支持更新microcode，需要在编译内核时加入以下配置（RancherOS已经开启）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CONFIG_MICROCODE=y
CONFIG_MICROCODE_INTEL=y
CONFIG_MICROCODE_AMD=y
CONFIG_MICROCODE_OLD_INTERFACE=y #支持传统方式，开启此项
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装过程比较简单，几乎每个版本的microcode都有相应的releasnote，大致如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Make sure /sys/devices/system/cpu/microcode/reload exits:
$ ls -l /sys/devices/system/cpu/microcode/reload

# You must copy all files from intel-ucode to /lib/firmware/intel-ucode/ using the cp command
$ sudo cp -v intel-ucode/* /lib/firmware/intel-ucode/

# You just copied intel-ucode directory to /lib/firmware/. Write the reload interface to 1 to reload the microcode files:
$ echo 1 &amp;gt; /sys/devices/system/cpu/microcode/reload
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;无论更新成功与否，在dmesg中都会查看到相关信息，比如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ dmesg | grep microcode
[   13.659429] microcode: sig=0x306f2, pf=0x1, revision=0x36
[   13.665981] microcode: Microcode Update Driver: v2.01 &amp;lt;tigran@aivazian.fsnet.co.uk&amp;gt;, Peter Oruba
[  510.899733] microcode: updated to revision 0x3b, date = 2017-11-17  # 这条msg很重要
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看cpuinfo，再次确认microcode版本，不同的CPU型号，升级后对应的microcode版本是不同的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat /proc/cpuinfo |grep &amp;quot;model\|microcode\|stepping\|family&amp;quot; |head -n 5
cpu family   : 6
model        : 63
model name   : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping     : 2
microcode    : 0x3b #之前是0x36
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后在cloud-config中在runcmd添加脚本，保证每次启动都加载最新版本的microcode：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;runcmd:
- echo 1 &amp;gt; /sys/devices/system/cpu/microcode/reload
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;关于Spectre Var. 2，我们依然在持续关注，
直接在内核编译中使用Retpoline指令替换技术，可以更简单方便的缓解branch target injection，
现在内核已经支持了Retpoline指令替换的设置，但是也需要最新版本的GCC编译器的特性支持，
而带有GCC的新补丁的正式版本还没有发布，一旦GCC新版本发布，我们会马上更新内核并发布新的RancherOS版本。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>RancherOS上运行RKE</title>
      <link>http://niusmallnan.com/2018/01/21/rke-on-rancheros</link>
      <pubDate>Sun, 21 Jan 2018 16:23:54 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2018/01/21/rke-on-rancheros</guid>
      <description>&lt;p&gt;RancherOS上运行RKE的注意事项
&lt;/p&gt;

&lt;h3 id=&#34;注意事项&#34;&gt;注意事项&lt;/h3&gt;

&lt;p&gt;RancherOS是一个非常精简的操作系统，功能上肯定不如Ubuntu/CentOS之流全面，所以在其上面使用RKE部署Kubernetes时候需要注意一些问题。&lt;/p&gt;

&lt;h4 id=&#34;切换到合适的docker-engine上&#34;&gt;切换到合适的docker engine上&lt;/h4&gt;

&lt;p&gt;一般来说K8s并不会适配所有docker engine，但是RancherOS提供docker engine自由切换的功能，可以先列出RancherOS的支持的docker engine版本:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@ip-172-31-5-104 rancher]# ros engine list
disabled docker-1.12.6
disabled docker-1.13.1
disabled docker-17.03.1-ce
current  docker-17.03.2-ce
disabled docker-17.04.0-ce
disabled docker-17.05.0-ce
disabled docker-17.06.1-ce
disabled docker-17.06.2-ce
disabled docker-17.09.1-ce
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据RKE的文档，选择合适的docker engine，切换docker engine很简单，使用&lt;code&gt;ros engine switch xxx&lt;/code&gt;即可。&lt;/p&gt;

&lt;h4 id=&#34;持久化相关目录&#34;&gt;持久化相关目录&lt;/h4&gt;

&lt;p&gt;RancherOS的默认console中只有部分目录是持久化的，这意味着你重启主机后，非持久化目录的数据会自动清理。
K8s会有一些数据需要持久化的磁盘上，所以针对K8s这些持久化目录，我们需要提前给RancherOS配置额外的持久化目录。&lt;/p&gt;

&lt;p&gt;RancherOS中目录的定义，请参考：&lt;a href=&#34;http://rancher.com/docs/os/v1.1/en/system-services/system-docker-volumes/&#34;&gt;http://rancher.com/docs/os/v1.1/en/system-services/system-docker-volumes/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;RancherOS默认包括三个持久化目录(user-volumes)，&lt;code&gt;/home/&lt;/code&gt; &lt;code&gt;/opt/&lt;/code&gt; &lt;code&gt;/var/lib/kubelet&lt;/code&gt;，如果要添加其他目录，可以使用下面的方式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 增加了 /etc/kubernetes 和 /etc/cni
ros config set rancher.services.user-volumes.volumes  [/home:/home,/opt:/opt,/var/lib/kubelet:/var/lib/kubelet,/etc/kubernetes:/etc/kubernetes,/etc/cni:/etc/cni]

system-docker rm all-volumes

reboot
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;具体那些目录需要持久化，还要看自己的部署需求，相关目录请参考：&lt;a href=&#34;https://github.com/rancher/rke#cluster-remove&#34;&gt;https://github.com/rancher/rke#cluster-remove&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>在阿里云上运行RancherOS</title>
      <link>http://niusmallnan.com/2017/12/05/rancheros-on-aliyun</link>
      <pubDate>Tue, 05 Dec 2017 16:32:52 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2017/12/05/rancheros-on-aliyun</guid>
      <description>&lt;p&gt;阿里云上也可以玩转RancherOS
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;RancherLabs创立之初就立志于做一家纯粹的云产品公司，秉承一切开源的理念，为用户带来方便快捷的体验。我们推出的Rancher深受全球用户的喜爱，
为了构建完整的用户体验丰富我们的产品线，我们考虑把传统的操作系统也引入容器的体验，于是我们大概在2015年正式推出了&lt;a href=&#34;https://github.com/rancher/os&#34;&gt;RancherOS&lt;/a&gt;这个项目，
该项目一经推出凭借其极具创新性的理念，荣登2015年Github10大开源项目，我们致力于打造轻量可靠，一切皆容器的操作系统。今年早些时候Docker发布的Linuxkit其实也从侧面印证了，
我们选择的这条路是顺应发展趋势的，只不过RancherOS先行了一步。&lt;/p&gt;

&lt;p&gt;现在我们在各个公有云平台上给用户免费使用，国外的厂商DigitalOcean甚至把RancherOS放到了Container distributions的UI选项中，越来越多的用户开始使用RancherOS，
参与到RancherOS的社区建设中，给我们反馈问题，协助我们不断改善产品。
而非常遗憾的是，中国用户始终无法在公有云中方便地使用它，其中原因诸多，国内的公有云也是这一两年才真正发展起来，
最初我们尝试在国内引入RancherOS时，很多平台无法提供有效的途径。&lt;/p&gt;

&lt;p&gt;现在，一切已经开始走向成熟，这次可以很开心得向大家宣布，RancherOS登陆阿里云，欢迎大家使用。
作为开源软件，不断积累改进是我们的目标，文章最后给大家提供了各种反馈使用问题的渠道。&lt;/p&gt;

&lt;h3 id=&#34;如何使用&#34;&gt;如何使用&lt;/h3&gt;

&lt;p&gt;首先声明一下，作为支持阿里云的首个版本仍然有很多不成熟之处，还有一些使用限制，如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;需使用阿里云VPC网络&lt;/li&gt;
&lt;li&gt;创建VM时，只支持密钥，不支持设置密码&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;首先下载RancherOS专属阿里云镜像，比如：&lt;a href=&#34;https://releases.rancher.com/os/latest/rancheros-aliyun.vhd&#34;&gt;https://releases.rancher.com/os/latest/rancheros-aliyun.vhd&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;将镜像上传到阿里云对象存储中，因为自定义镜像是需要从对象存储中获取。&lt;/p&gt;

&lt;p&gt;创建自定义镜像，按如下方式填写：&lt;br /&gt;
&lt;img src=&#34;https://ws4.sinaimg.cn/large/006tKfTcly1fmm2l7gfbhj312a0rewfl.jpg&#34; alt=&#34;&#34; /&gt;
注意事项包括：系统盘大小大于10GB，系统架构为x86_64，系统平台为Others Linux，镜像格式为VHD。&lt;/p&gt;

&lt;p&gt;创建VM时，按以下方式选择镜像：&lt;br /&gt;
&lt;img src=&#34;https://ws1.sinaimg.cn/large/006tKfTcly1fmwjd489e6j30wi09odg1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;创建成功后，使用ssh，访问用户是rancher，当然不要忘记指定密钥：&lt;br /&gt;
&lt;img src=&#34;https://ws2.sinaimg.cn/large/006tKfTcly1fm77ckj7ywj31ek0aq3z5.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
&lt;img src=&#34;https://ws4.sinaimg.cn/large/006tKfTcly1fm76t2hvs7j316u12641q.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;由于现在ROS镜像还没有发布到阿里云镜像市场中，当你创建完自定义镜像后，你可以按照如下方式共享给你的伙伴使用：&lt;br /&gt;
&lt;img src=&#34;https://ws1.sinaimg.cn/large/006tKfTcly1fm76xwulxxj30vy0jw74v.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;反馈渠道&#34;&gt;反馈渠道&lt;/h3&gt;

&lt;p&gt;各种问题欢迎随时提交issue:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;这里建议使用英文 &lt;a href=&#34;https://github.com/rancher/os/issues&#34;&gt;https://github.com/rancher/os/issues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;中文issue可以提交到Maintainer的个人Repo上: &lt;a href=&#34;https://github.com/niusmallnan/os/issues&#34;&gt;https://github.com/niusmallnan/os/issues&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;同时也欢迎大家发送邮件(niusmallnan@gmail.com)给开发者。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Rancher下实现Calico/Flat/Macvlan网络</title>
      <link>http://niusmallnan.com/2017/05/16/rancher-extend-network</link>
      <pubDate>Tue, 16 May 2017 16:31:50 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2017/05/16/rancher-extend-network</guid>
      <description>&lt;p&gt;Rancher下的高性能网络方案
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;目前Rancher的开源版本中提供了IPSec和VXLAN两种网络方案：IPSec更多面向的是安全加密的场景，
但是网络性能就会差很多；Vxlan与IPSec同属Overlay模型，但是内核层面的支持和没有加密环节的损耗，
相对来说Vxlan的性能要远高于IPSec，在AWS上测试的Vxlan带宽性能是逼近于两台VM之间带宽，
当然由于AWS本身的网络质量就很好，用户在自己的环境测试时并不会有这么好的性能。
当面向私有数据中心部署时，Overlay的模型并不一定满足用户的需求，而且当前容器网络发展非常迅猛，
出现了很多独树一帜的网络插件，所以这里我选择了三种插件Calico、Flat、Macvlan对接Rancher，
希望可以为更多用户提供帮助。&lt;/p&gt;

&lt;h3 id=&#34;rancher对接第三方网络插件&#34;&gt;Rancher对接第三方网络插件&lt;/h3&gt;

&lt;p&gt;Rancher在v1.2版本重新架构后，实际上已经为第三方网络插件的支持留下了空间。Rancher中的network-manager
组件是管理网络插件的核心，其管理原理本文不赘述，后面会放一个PPT参考链接，有兴趣可以仔细品味。
但是当真正对接第三方插件时，还有一些小问题需要解决，这个问题就是多个ENV时，每个ENV都有独立的网络插件，
那么network-manager如何匹配自己当前ENV的CNI config文件，关于此问题，
我建了一个issue&lt;a href=&#34;https://github.com/rancher/rancher/issues/8535&#34;&gt;#8535&lt;/a&gt;，并提交了相关PR。
在Rancher v1.6.0+版本中，已经包含了这些修复。&lt;/p&gt;

&lt;p&gt;此外，还需要注意的是，在Rancher中集成第三方网络插件需要满足以下三个条件，否则兼容性和体验会大打折扣：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;容器可以分配IP，并能互相联通，IP与MAC设定尽量与Rancher同步&lt;/li&gt;
&lt;li&gt;容器网络可以访问metadata服务，即容器内可以ping rancher-metadata&lt;/li&gt;
&lt;li&gt;LB与metadata等组件兼容性&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;calico的支持&#34;&gt;Calico的支持&lt;/h3&gt;

&lt;p&gt;Calico本身的设计思路很独到，利用BGP技术学习路由，通过路由构建高性能的网络，但是Calico本身在用户端的落地阻力还是相当大的，
很多用户纯粹把Calico的支持当作一个Poc的要点，因为如果用户真的决定使用Calico网络，那么来自用户内部网管的阻力是相当大的。
BGP路由动态学习对于很多行业，这种网络几乎认为是没有管理的状态。&lt;/p&gt;

&lt;p&gt;出于对Poc支持的考虑和一些付费用户的定制需求，大家如有兴趣可以参考一个demo版本的Rancher Calico的实现：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Issue描述&lt;a href=&#34;https://github.com/rancher/rancher/issues/8603&#34;&gt;#rancher-8603&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Demo Catalog &lt;a href=&#34;https://github.com/niusmallnan/test-calico-catalog-item&#34;&gt;#test-calico-catalog-item&lt;/a&gt;，请使用test分支&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;flat的支持&#34;&gt;Flat的支持&lt;/h3&gt;

&lt;p&gt;Flat网络，中国用户喜欢称它为扁平网络，其目的是构建一种不带有任何路由或者overlay技术的网络，基本上容器的网络就是在一张大网中。
通常容器的Flat网络是在交换机中配置好的，主机和容器其实使用的是同一个网段，这时候需要规划好网络的设置，比如主机的ip范围和容器的ip范围不能冲突。&lt;/p&gt;

&lt;p&gt;Flat网络的实现非常简单，原理上用户也很容易了解，在用户自身的网络部门推广起来几乎没有阻力，很多商业用户依赖这个网络。
所以Rancher已经开源了Flat网络的实现，用户可以在v1.6.11版本上使用Flat网络，在Catalog中就可以找到。&lt;/p&gt;

&lt;h3 id=&#34;macvlan的支持&#34;&gt;Macvlan的支持&lt;/h3&gt;

&lt;p&gt;Macvlan网络更多是在Docker的CNM下被提及，其实CNI下也有Macvlan的实现，通常我们利用Macvlan可以隔离不同的容器子网。
但是实际上这个特性Rancher并不是十分需要，Rancher的设计中一个ENV下就是一张网，这样可以很方便用户使用。
如果不需要Macvlan的隔离特性，实际上这里其实更多就是利用Macvlan构建了一个类似Flat的网络，对于大部分Rancher用户来说Macvlan并没有很大的使用价值。&lt;/p&gt;

&lt;p&gt;出于对Poc支持的考虑和一些付费用户的定制需求，大家如有兴趣可以参考一个demo版本的Rancher Macvlan的实现：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Issue描述&lt;a href=&#34;https://github.com/rancher/rancher/issues/8686&#34;&gt;#8686&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Demo Catalog App &lt;a href=&#34;https://github.com/niusmallnan/flatnet-catalog&#34;&gt;macvlan&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;更多内容可以参考我之前对外分享的PPT，请自行搭梯子，&lt;a href=&#34;https://docs.google.com/presentation/d/1U4YZpsXnFg6l7YNIcaq0SJ5DsUlSr14mtWnxQ-rvFwQ/edit?usp=sharing&#34;&gt;Rancher network management&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Rancher体系下容器日志采集</title>
      <link>http://niusmallnan.com/2017/04/11/rancher-logging</link>
      <pubDate>Tue, 11 Apr 2017 11:56:14 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2017/04/11/rancher-logging</guid>
      <description>&lt;p&gt;在Rancher体系内，实现容器日志采集。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;一个完整的容器平台，容器日志都是很重要的一环。尤其在微服务架构大行其道状况下，程序的访问监控健康状态很多都依赖日志信息的收集，
由于Docker的存在，让容器平台中的日志收集和传统方式很多不一样，日志的输出和采集点收集和以前大有不同。
本文就探讨一下，Rancher平台内如何做容器日志收集。&lt;/p&gt;

&lt;h3 id=&#34;当前现状&#34;&gt;当前现状&lt;/h3&gt;

&lt;p&gt;纵览当前容器日志收集的场景，无非就是两种方式：一是直接采集Docker标准输出，容器内的服务将日志信息写到标准输出，
这样通过Docker的log driver可以发送到相应的收集程序中；二是延续传统的日志写入方式，容器内的服务将日志直接写到普通文件中，
通过Docker volume将日志文件映射到Host上，日志采集程序就可以收集它。&lt;/p&gt;

&lt;p&gt;第一种方式足够简单，直接配置相关的Log Driver就可以，但是这种方式也有些劣势：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;当主机的容器密度比较高的时候，对Docker Engine的压力比较大，毕竟容器标准输出都要通过Docker Engine来处理。&lt;/li&gt;
&lt;li&gt;尽管原则上，我们希望遵循一容器部署一个服务的原则，但是有时候特殊情况不可避免容器内有多个业务服务，
这时候很难做到所有服务都向标准输出写日志，这就需要用到前面所说的第二种场景模式。&lt;/li&gt;
&lt;li&gt;虽然我们可以先选择很多种Log Driver，但是有些Log Driver会破坏Docker原生的体验，比如docker logs无法直接看到容器日志。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;基于以上考虑，一个完整的日志方案必须要同时满足标准输出和日志卷两种模式才可以。当然完整的日志体系中，并不仅仅是采集，
还需要有日志存储和UI展现。日志存储有很多种开源的实现，这个一般用户都会有自己钟情的选择，而UI展现更是各家有各家的需求，
很难形成比较好的标准，一般都是通过定制化方式解决。所以此文主要展现的方案是日志采集方案，当然在存储和UI展现上会对接开源实现，
没有特殊需求的情况下，也可以拥有一个完整的体验。&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tNc79ly1feiptl1cx3j31js0v6401.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;rancher下的解决方案&#34;&gt;Rancher下的解决方案&lt;/h3&gt;

&lt;p&gt;如上面图中所示，日志存储和UI展现可以直接使用ElasticSearch &amp;amp; Kibana，日志采集方面如之前所分析，需要对接两种采集模式，
所以这部分采用Fluentd &amp;amp; Logging Helper的组合，Fluentd是很通用的日志采集程序，拥有优异的性能，相对Logstash来说同等压力下，
其内存消耗要少很多。Logging Helper是可以理解微Fluentd的助手，它可以识别容器内的日志卷文件，通知Fluentd进行采集。
此外，由于要保证Dokcer和Rancher体验的完整性，在Docker Log Driver的选型上支持json-file和journald，其原因：
一是json-file和journald相对来说比较常用；二是这两种驱动下，docker logs依然可以有内容输出，保证了体验的完整性。&lt;/p&gt;

&lt;p&gt;下面开始说明，整个方案的部署过程。先用一张图来描述整体的部署结构，如下：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tNc79ly1feirbhlr0ij31700j4aaj.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
总共划分三个ENV，其中一个ENV部署ES &amp;amp; Kibana，另外两个ENV分别添加json-file驱动的主机和journald驱动的主机。
详细部署过程如下：&lt;/p&gt;

&lt;p&gt;创建三个ENV，使用Cattle引擎。设置Logging Catalog方便部署，
在Admin&amp;ndash;Settings页面中添加Catalog，地址为：&lt;a href=&#34;https://github.com/niusmallnan/rancher-logging-catalog.git&#34;&gt;https://github.com/niusmallnan/rancher-logging-catalog.git&lt;/a&gt;&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tNc79ly1feirqbal91j31du0b2q3d.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;进入ES-Kibana ENV中，部署ElasticSearch &amp;amp; Kibana，这两个应用在Community Catalog中均可以找到，部署非常简单，
需要注意的是，建议选择Elasticsearch 2.x，Kibana中的Elasicsearch Source选择elasticseach-clients：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tNc79ly1feirvmhq6qj30sc0mimxt.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;分配两台主机并安装Docker，其中Log Driver分别选择json-file和journald，并将主机添加到各自的ENV中。
在这两个ENV中添加External Link指向之前部署的Elasticsearch地址：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tNc79ly1feis5geevwj319e0gmaas.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
然后在Jsonfile &amp;amp; Journald ENV中添加Rancher Logging应用，打开对应的catalog，ES link指向刚才设定的External link，
其中Volume Pattern是日志卷的匹配模式，File Pattern是日志卷内的日志文件匹配模式：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tNc79ly1feis130yinj31ke0scwfk.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;以上部署完成之后，部署一些应用并产生一些访问日志，就可以在Kibana的界面中看到：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tNc79ly1feistxqn3ej31a60x0jto.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;若要使用日志卷方式，则需要在service启动的时候配置volume，volume name需要匹配之前设定的Volume Pattern：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tNc79ly1feisvr5tlkj317m0ju3zd.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;秉承一切开源的原则，相关实现可以查看一下链接：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/niusmallnan/rancher-fluentd-package&#34;&gt;https://github.com/niusmallnan/rancher-fluentd-package&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/niusmallnan/logging-helper&#34;&gt;https://github.com/niusmallnan/logging-helper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/niusmallnan/rancher-logging-catalog&#34;&gt;https://github.com/niusmallnan/rancher-logging-catalog&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;通过Fluentd我们可以对接很多第三方日志存储体系，但是Fluentd自身并不能完成日志采集的所有场景，所以非常需要Logging Helper的帮助。
通过Logging Helper可以定制出一些额外采集规则，比如可以过滤某些容器日志等等。当然真正大规模生产环境日志平台，其实是对整个运维体系的考验，
单纯靠开源程序的组合并不能真正解决问题。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Kubelet无法访问rancher-metadata问题分析</title>
      <link>http://niusmallnan.com/2017/03/09/analysis-of-kubelet-start-failure</link>
      <pubDate>Thu, 09 Mar 2017 15:45:41 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2017/03/09/analysis-of-kubelet-start-failure</guid>
      <description>&lt;p&gt;拯救一脸懵逼
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;Rancher能够支持Kubernetes，可以快速几乎无障碍的拉起一套K8s环境，这对刚入门K8s的小白来说简直是一大利器。
当然由于系统特性五花八门，系统内置软件也相互影响，所以有时候伙伴们会碰到比较难缠的问题。
本文就分析一下关于kubelet无法访问rancher-metadata问题，当然这个问题并不是必现，
这里要感谢一位Rancher社区网友的帮助，他的环境中发现了这个问题，并慷慨得将访问权限共享给了我。&lt;/p&gt;

&lt;h3 id=&#34;问题现象&#34;&gt;问题现象&lt;/h3&gt;

&lt;p&gt;使用Rancher部署K8s后，发现一切服务状态均正常，这时候打开K8s dashboard却无法访问，
细心得查看会发现，dashboard服务并没有部署起来，这时下意识的行为是查看kubelet的日志，
此时会发现一个异常：&lt;br /&gt;
&lt;img src=&#34;https://ww1.sinaimg.cn/large/006tNbRwly1fdgnj5h1bjj30g503omxw.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
你会发现kubelet容器内部一直无法访问rancher-metadata，查看rancher-k8s-package源码，
kubelet服务启动之前需要通过访问rancher-metadata做一些初始化动作，由于访问不了，
便一直处于sleep状态，也就是出现了上面提到的那些异常日志的现象：&lt;br /&gt;
&lt;img src=&#34;https://ww2.sinaimg.cn/large/006tNbRwly1fdgnn1tbzzj30k2071q41.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;同样，在github上也能看到类似的issue：&lt;a href=&#34;https://github.com/rancher/rancher/issues/7160&#34;&gt;https://github.com/rancher/rancher/issues/7160&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;排查分析&#34;&gt;排查分析&lt;/h3&gt;

&lt;p&gt;进入kubelet容器一探究竟，分别用ping和dig测试对rancher-metadata访问情况如下：&lt;br /&gt;
&lt;img src=&#34;https://ww1.sinaimg.cn/large/006tNbRwly1fdgnq66xs1j30ey0abmyh.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
dig明显可以解析，但是ping无法解析，因此基本排除了容器内dns nameserver或者网络链路情况的问题。&lt;/p&gt;

&lt;p&gt;既然dig没有问题，ping有问题，那么我们就直接采取使用strace（&lt;code&gt;strace ping rancher-metadata -c 1&lt;/code&gt;）来调试，
这样可以打印系统内部调用的情况，可以更深层次找到问题根源：&lt;br /&gt;
&lt;img src=&#34;https://ww1.sinaimg.cn/large/006tNbRwly1fdgnsufow0j30k7090tb2.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;之前提到这个问题并不是必现的，所以我们找一个正常的环境，同样用strace调试，如下：&lt;br /&gt;
&lt;img src=&#34;https://ww2.sinaimg.cn/large/006tNbRwly1fdgnu63kl2j30m0077dhj.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;对这两张图，其实已经能够很明显的看出区别，有问题的kubelet在解析rancher-metadata之前，
向nscd请求的解析结果，nscd返回了unkown host，所以就没有进行dns解析。
而正常的kubelet节点并没有找到nscd.socket，而后直接请求dns进行解析rancher-metadata地址。&lt;/p&gt;

&lt;p&gt;经过以上的分析，基本上断定问题出在nscd上，那么为什么同样版本的rancher-k8s，
一个有nscd socket，而另一个却没有，仔细看一下kubelet的compose定义：&lt;br /&gt;
&lt;img src=&#34;https://ww4.sinaimg.cn/large/006tNbRwly1fdgny3wpihj30c5087wf5.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
kubelet启动时候映射了主机目录/var/run，那么基本可以得知nscd来自于系统。
检查一下有问题的kubelet节点的系统，果然会发现安装了nscd服务（服务名为unscd）。&lt;/p&gt;

&lt;p&gt;用比较暴力的方案证明一下分析过程，直接删除nscd socket文件，这时候你会发现kubelet服务正常启动了，
rancher-metadata也可以访问了。&lt;/p&gt;

&lt;p&gt;回过头来思考一下原理，为什么ping/curl这种会先去nscd中寻找解析结果呢，而dig/nslookup则不受影响。
ping/curl这种在解析地址前都会先读取/etc/nsswitch.conf，这是由于其底层均引用了glibc，
由nsswitch调度，最终指引ping/curl先去找nscd服务。nscd服务是一个name services cache服务，
很多解析结果他会缓存，而我们知道这个nscd是运行在Host上的，Host上是不能直接访问rancher-metadata这个服务名，
所以kubelet容器中就无法访问rancher-metadata。&lt;/p&gt;

&lt;h3 id=&#34;其他解决方案&#34;&gt;其他解决方案&lt;/h3&gt;

&lt;p&gt;其实我们也未必要如此暴力删除nscd，nscd也有一些配置，我们可以修改一下以避免这种情况，
可以disable hosts cache，这样nscd中便不会有相应内容的缓存，所以解析rancher-metadata并不会出现unknown host，
而是继续向dns nameserver申请解析地址，这样也不会有问题。&lt;br /&gt;
&lt;img src=&#34;https://ww4.sinaimg.cn/large/006tNbRwly1fdgo48mxf8j30ej06lab0.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;遇到问题不能慌，关键是要沉得住气，很多看似非常复杂的问题，其实往往都是一个小配置引发的血案。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Rancher如何按计算资源调度</title>
      <link>http://niusmallnan.com/2017/02/23/rancher-scheduler-based-on-resource</link>
      <pubDate>Thu, 23 Feb 2017 18:58:19 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2017/02/23/rancher-scheduler-based-on-resource</guid>
      <description>&lt;p&gt;非常简单的说明一下，Rancher如何做按计算资源调度。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;按计算资源调度基本上是各大编排引擎的标配，Rancher在v1.2版本后也推出了这个功能，
很多朋友并不知道，主要是因为当前的实现还并不是那么智能，故不才欲写下此文以助视听。&lt;/p&gt;

&lt;h3 id=&#34;实现机制&#34;&gt;实现机制&lt;/h3&gt;

&lt;p&gt;Rancher的实现比较简单，其主要是通过Infra services中的scheduler服务来实现，整体的逻辑架构如下：&lt;br /&gt;
&lt;img src=&#34;https://ww3.sinaimg.cn/large/006tNc79ly1fd0lxplni2j30m90a7q49.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
scheduler会订阅Rancher Events，主要是scheduler相关事件，当有调度需求时候，scheduler就会收到消息，
通过计算将合适的调度目标返回给cattle。比如说现在支持memory和cpu为基准，
那么scheduler会不断根据metadata的数据变化来计算资源的使用量，最后可根据资源剩余量为调度目标排序，
这样就可以完成按计算资源调度的目标。&lt;/p&gt;

&lt;p&gt;之前有说，Rancher的实现并不智能，这在于在计算资源使用量的时候，Rancher并不是通过一套复杂数据采集机制来计算，
而是通过用户在创建service的时候标注reservation的方式，这个地方很多朋友并没有注意到：&lt;br /&gt;
&lt;img src=&#34;https://ww4.sinaimg.cn/large/006tNc79ly1fd0m3wzn0dj30lf08f3ze.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;除此之外，在每个节点的资源总量上也是可配置的，我们完全可以进行一个整体预留的设置，比如：&lt;br /&gt;
&lt;img src=&#34;https://ww4.sinaimg.cn/large/006tNc79ly1fd0m5p3spuj30f90amaao.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;这个实现看似简单，其实这是提供了一个很好的扩展能力。如果我们有自己的监控采集体系，
完全可以在scheduler的时候调用我们自身监控接口来计算资源，这样就能达到我们所认可的“智能”了。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>关于Subscribe Rancher Events的思考</title>
      <link>http://niusmallnan.com/2017/02/23/thinking-about-subcribe-rancher-events</link>
      <pubDate>Thu, 23 Feb 2017 17:33:56 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2017/02/23/thinking-about-subcribe-rancher-events</guid>
      <description>&lt;p&gt;路漫漫其修远兮，吾将上下而求索
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;几乎每个大型的分布式的集群软件，都离不开一样东西，就是所谓的message bus（消息总线），
它就如同人体的血管一样，连通着各个组件，相互协调，一起工作。
与很多同类软件不同的是，Rancher使用的基于websocket协议实现的消息总线，
Rancher不会依赖任何MQ，基于websocket的实现十分轻量级，
同时在各种语言库的支持上，也毫无压力，毕竟websocket是HTTP的标准规范之一。&lt;/p&gt;

&lt;h3 id=&#34;初识rancher-events&#34;&gt;初识Rancher Events&lt;/h3&gt;

&lt;p&gt;基于websocket的消息总线可以很好的与前端兼容，让消息的传递不再是后端的专利。
在Rancher UI上，很容易就能捕获到rancher events，比如：&lt;br /&gt;
&lt;img src=&#34;https://ww4.sinaimg.cn/large/006tNc79ly1fd0k5tuin9j30it07at9x.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
这里面监听的事件名称是resource.change，这个resource.change在前端UI上有很大的用处，
其实我们都知道，很多POST形式的create请求并不是同步返回结果的，因为调度引擎需要处理，
这个等待的过程中，当然不能前端一直wait，所以做法都是发起create后直接返回HTTP 202，
转入后台执行后，Rancher的后端会把创建的执行过程中间状态不断发送给消息总线，
那么前端通过监听resource.change就会获得这些中间状态，这样在UI上就可以给用户一个很好的反馈体验。&lt;/p&gt;

&lt;p&gt;当然Rancher Events并不是只有resource.change，比如在Iaas Events集合中就有如下这些：&lt;br /&gt;
&lt;img src=&#34;https://ww1.sinaimg.cn/large/006tNc79ly1fd0kccvpwxj30gk0ac40k.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;除了Events的事件定义，当然还有如何去subscribe 这些events，这部分内容我之前的文章中有所涉猎，
便不赘言。&lt;a href=&#34;http://niusmallnan.com/2016/08/25/rancher-envent/&#34;&gt;http://niusmallnan.com/2016/08/25/rancher-envent/&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;subscribe-rancher-events的架构模式&#34;&gt;Subscribe Rancher Events的架构模式&lt;/h3&gt;

&lt;p&gt;Rancher的体系内，很多微服务的组件都是基于Subscribe Rancher Events这种架构，举个例子来看，
以rancher-metadata组件为例：&lt;br /&gt;
&lt;img src=&#34;https://ww4.sinaimg.cn/large/006tNc79ly1fd0kmwbmu4j30iy07wq3n.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
metadata服务可以提供当前host的元数据查询，我们可以很容器的知道env内的stack/service/container的情况，
这些数据其实由rancher-server也就是cattle引擎生成的，那么生成之后怎么发送给各个agent呢？
其实就是metadata进行了subscribe rancher events，当然它只监听了config.update事件，
只要这个事件有通知，metadata服务便会下载新的元数据，这样就达到了不断更新元数据的目的。&lt;/p&gt;

&lt;p&gt;随着深入的使用Rancher，肯定会有一些伙伴需要对Rancher进行扩展，那就需要自行研发了，
毕竟常见的方式就是监听一些事件做一些内部处理逻辑，并在DB中存入一些数据，
同时暴露API服务，架构如下：&lt;br /&gt;
&lt;img src=&#34;https://ww3.sinaimg.cn/large/006tNc79ly1fd0kt6rttwj309o072jrn.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;如果需要做HA，可能需要scale多个这样的服务，那么架构就变成这样：&lt;br /&gt;
&lt;img src=&#34;https://ww2.sinaimg.cn/large/006tNc79ly1fd0ku7zaafj30g30avdgq.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
这里其实会有一个问题，如果你监听了一些广播事件，那么实际上每个实例都会收到同样的事件，
那么你的处理逻辑就要注意了，尤其是在处理向DB中写数据时，一定要考虑到这样的情况。&lt;/p&gt;

&lt;p&gt;比如，可以只有其中一个实例来监听广播事件，这样不会导致事件重复收取：&lt;br /&gt;
&lt;img src=&#34;https://ww4.sinaimg.cn/large/006tNc79ly1fd0kxdd6vmj30g30b1dgm.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
Event Handler要考虑一定failover机制，这样事件收取不会长时间中断。&lt;/p&gt;

&lt;p&gt;Rancher Events有一些非广播事件，那么就需要在subscribe的时候指定一些特殊参数，
这样事件就会只发送给注册方，不会发送给每个节点，比如：&lt;br /&gt;
&lt;img src=&#34;https://ww3.sinaimg.cn/large/006tNc79ly1fd0kztt3k6j30fx0audgr.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;此文算是这段时间做Rancher服务扩展的心得，深度参与一个开源软件最终肯定会希望去改动它扩展它。
这也是客观需求所致，开源软件可以拿来即用，但是真正可用实用，必须加以改造，适应自身需求。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Rancher K8s Helm使用体验</title>
      <link>http://niusmallnan.com/2017/02/05/rancher-k8s-helm-practice</link>
      <pubDate>Sun, 05 Feb 2017 14:08:38 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2017/02/05/rancher-k8s-helm-practice</guid>
      <description>&lt;p&gt;潇潇洒洒玩一玩
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;之前的文章已经阐述了Rancher K8s中国区的使用优化，
&lt;a href=&#34;http://niusmallnan.com/2017/01/19/optimize-rancher-k8s-in-china/&#34;&gt;http://niusmallnan.com/2017/01/19/optimize-rancher-k8s-in-china/&lt;/a&gt;，
本文则是选取一个特殊的组件Helm，深入一些体验使用过程。Helm是K8s中类似管理catalog的组件，
在较新的Rancher K8s版本中，Helm已经默认集成可直接使用。&lt;/p&gt;

&lt;h3 id=&#34;初始化cli环境&#34;&gt;初始化CLI环境&lt;/h3&gt;

&lt;p&gt;如果已经习惯使用Rancher UI上自带的kubectl tab，那么可以跳过此步。
大部分玩家还是更喜欢在自己的机器上使用kubectl和helm CLI来做管理的。
在自己的机器上部署kubectl和helm命令行工具，有一个比较偷懒的方法，
就是直接到kubectld容器拷贝出来，主要过程如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# kubectrld容器ID为 42291346c064
$ docker cp 42291346c064:/usr/bin/kubectl /usr/local/bin/kubectl
$ docker cp 42291346c064:/usr/bin/helm /usr/local/bin/helm
$ docker cp 42291346c064:/tmp/.helm ~/.helm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然也不要忘记kubectl的配置文件，在Rancher UI上生成，然后拷贝到对应的本地目录上。&lt;br /&gt;
&lt;img src=&#34;https://ww1.sinaimg.cn/large/006tNc79ly1fcfkc0a5yjj30dx07edgb.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;基于helm部署mysql&#34;&gt;基于Helm部署Mysql&lt;/h3&gt;

&lt;p&gt;K8s官方的Charts中已经有了mysql这个应用，
这里&lt;a href=&#34;https://github.com/kubernetes/charts/tree/master/stable&#34;&gt;https://github.com/kubernetes/charts/tree/master/stable&lt;/a&gt;可以找到，
几乎所有的Chart都需要依赖PersistentVolumeClaim提供卷，所以在一切开始之前，
我们需要先创建一个PersistentVolume，来提供一个数据卷池。这里可以选择部署比较方便的NFS。&lt;/p&gt;

&lt;p&gt;选择一台主机，安装nfs-kernel-server，并做相应配置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ apt-get install nfs-kernel-server

# 配置卷
# 修改 /etc/exports，添加如下内容
/nfsexports *(rw,async,no_root_squash,no_subtree_check)

# 重启nfs-server
service nfs-kernel-server restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用kubectl创建PV，文件内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0003
  annotations:
    volume.beta.kubernetes.io/storage-class: &amp;quot;slow&amp;quot;
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  nfs:
    path: /nfsexports #NFS mount path
    server: 172.31.17.169 #NFS Server IP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装mysql之前，需要准备一份mysql chart的配置文件，也就是对应的values.yaml，
其他内容&lt;a href=&#34;https://github.com/kubernetes/charts/blob/master/stable/mysql/values.yaml&#34;&gt;https://github.com/kubernetes/charts/blob/master/stable/mysql/values.yaml&lt;/a&gt;基本不变，
主要修改persistence部分，这样所依赖的PVC才能bound到对应的PV上，如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;persistence:
  enabled: true
  storageClass: slow
  accessMode: ReadWriteMany
  size: 1Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一切准备妥当，就可以进行Mysql Chart的安装，执行过程如下：&lt;code&gt;helm install --name ddb -f values.yaml stable/mysql&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;安装完毕后，在Rancher UI和K8s Dashboard上都可以看到。&lt;br /&gt;
&lt;img src=&#34;https://ww1.sinaimg.cn/large/006tNc79ly1fcfkfju2ayj30k407q750.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
&lt;img src=&#34;https://ww3.sinaimg.cn/large/006tNc79ly1fcfkfsuoayj30ed05zaae.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>优化Rancher k8s中国区的使用体验</title>
      <link>http://niusmallnan.com/2017/01/19/optimize-rancher-k8s-in-china</link>
      <pubDate>Thu, 19 Jan 2017 17:32:56 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2017/01/19/optimize-rancher-k8s-in-china</guid>
      <description>&lt;p&gt;拯救一脸懵逼
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;Kubernetes（以下简称K8s）是Rancher平台重点支持的一个编排引擎，Rancher K8s具有部署灵活使用方便的特点，
而且Rancher基本是同步更新支持K8s的新版本新组件，用户也可以选择部署指定的K8s版本。
但是这一切的便利，身在中国的我们这些贱民无法深刻体验，万恶的GFW把很多部署的依赖挡在之外，
而服务全球开发者的Rancher平台亦不可能为中国用户单独定制，所以我只好自己动手丰衣足食。&lt;/p&gt;

&lt;h3 id=&#34;部署要点&#34;&gt;部署要点&lt;/h3&gt;

&lt;p&gt;部署之前的操作系统选型上，相对来说我比较推荐ubuntu+docker的组合，
毕竟这个组合在国外使用的用户比较多，相对来说bug fix的速度也是比较快的，
如果你是一个docker重度用户，应该深知docker本身的bug并不少。&lt;/p&gt;

&lt;p&gt;如果是部署一个新的Rancher环境，我推荐用下面的脚本来启动，通过设置DEFAULT_CATTLE_CATALOG_URL，
这样可以直接指定我定制过的Rancher K8s：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d --restart=unless-stopped \
     -e DEFAULT_CATTLE_CATALOG_URL=&#39;{&amp;quot;catalogs&amp;quot;:{&amp;quot;community&amp;quot;:{&amp;quot;url&amp;quot;:&amp;quot;https://github.com/rancher/community-catalog.git&amp;quot;,&amp;quot;branch&amp;quot;:&amp;quot;master&amp;quot;},&amp;quot;library&amp;quot;:{&amp;quot;url&amp;quot;:&amp;quot;https://github.com/niusmallnan/rancher-catalog.git&amp;quot;,&amp;quot;branch&amp;quot;:&amp;quot;k8s-cn&amp;quot;}}}&#39; \
     --name rancher-server \
     -p 8082:8080 rancher/server:stable
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然如果是已经部署的Rancher环境，那就需要在Rancher UI上，做一下简单的修改，
Disable已有的library catalog repo，指向我定制过的即可，注意branch的设置，网络状况不好的需要耐心等待重新拉取repo内容：
&lt;img src=&#34;https://ww3.sinaimg.cn/large/006y8lValy1fbw2toyl38j30s30a50u6.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在部署agent节点前，如果是一个干净的环境最好，但是如果是曾经做过agent节点，
尤其是之前部署过rancher k8s的，我强烈建议你执行一次大扫除，否则会出现各种意想不到的状况，
大扫除的脚本可以参考执行我的这个，具体都做了什么事可自行阅读：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash

docker rm -f $(docker ps -qa)

rm -rf /var/etcd/

for m in $(tac /proc/mounts | awk &#39;{print $2}&#39; | grep /var/lib/kubelet); do
    umount $m || true
done
rm -rf /var/lib/kubelet/

for m in $(tac /proc/mounts | awk &#39;{print $2}&#39; | grep /var/lib/rancher); do
    umount $m || true
done
rm -rf /var/lib/rancher/

rm -rf /run/kubernetes/

docker volume rm $(docker volume ls -q)

docker ps -a
docker volume ls
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;一切opensource&#34;&gt;一切OpenSource&lt;/h3&gt;

&lt;p&gt;如果你对我在其中的改动颇有疑虑，亦大可放心。我主要是改动两个地方：
fork了rancher-catalog建立了k8s-cn的分支，只要将Rancher的library catalog repo指向我的工程分支即可；
fork了kubernetes-package，每次Rancher K8s发布新版本，
我都会基于该版本建立一个CN分支（如：v1.5.1-rancher1-7-cn），
一切对于中国区的优化修改都会在这个分支上。最终我会更新出中国区的使用镜像，并push到镜像仓库上，
目前使用的是阿里云的镜像仓库（招牌比较大短时间内不会倒。。。）。&lt;/p&gt;

&lt;p&gt;参考链接：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/niusmallnan/rancher-catalog&#34;&gt;https://github.com/niusmallnan/rancher-catalog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/niusmallnan/kubernetes-package&#34;&gt;https://github.com/niusmallnan/kubernetes-package&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;后续支持计划&#34;&gt;后续支持计划&lt;/h3&gt;

&lt;p&gt;截止到本文写作之时，我刚开始支持rancher-k8s v1.5.1-rancher1-7版本，并在Rancher v1.3.1版本上做了测试。
后续Rancher官方发布新版本，我会进行同步更新，并做一些简单的测试。
后续考虑加入离线安装，可以指定本地镜像仓库，依赖镜像一键导入等方便的功能。
当然如果在使用中发现各种疑难杂症，可以发邮件给我niusmallnan@gmail.com。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Rancher-LB开启访问日志</title>
      <link>http://niusmallnan.com/2017/01/06/rancher-lb-logs</link>
      <pubDate>Fri, 06 Jan 2017 15:12:10 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2017/01/06/rancher-lb-logs</guid>
      <description>&lt;p&gt;拯救一脸懵逼
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;在Rancher v1.2之前的版本，Rancher LB也就是Haproxy都是放在agent/instance内的，由于这个镜像本身要支持很多功能，
所以要开启Haproxy的访问日志比较麻烦。因为Harpoxy的日志和其他Nginx之类略有不同，它是通过syslog协议讲日志发送出去的，
要想展现日志还要开启syslog进行收集。Rancher v1.2版本开始，
Rancher LB的功能已经解耦成独立的镜像rancher/lb-service-haproxy，如此日志收集的工作就可以做了。&lt;/p&gt;

&lt;h3 id=&#34;开启访问日志&#34;&gt;开启访问日志&lt;/h3&gt;

&lt;p&gt;lb-service-haproxy的实现可以仔细阅读源码&lt;a href=&#34;https://github.com/rancher/lb-controller&#34;&gt;https://github.com/rancher/lb-controller&lt;/a&gt;，
简单的说，其内置了rsyslog来收集haproxy的日志，在容器内部查看配置文件&lt;strong&gt;/etc/haproxy/rsyslogd.conf&lt;/strong&gt;，
可以知道其开启了udp 8514端口，不同的级别的日志发送到不同的文件中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ModLoad imudp.so
$UDPServerRun 8514
$template CustomFormat,&amp;quot;%timegenerated% %HOSTNAME% %syslogtag%%msg%\n&amp;quot;
$ActionFileDefaultTemplate CustomFormat

/* info */
if $programname contains &#39;haproxy&#39; and $syslogseverity == 6 then (
    action(type=&amp;quot;omfile&amp;quot; file=&amp;quot;/var/log/haproxy/traffic&amp;quot;)
)
if $programname contains &#39;haproxy&#39; and $syslogseverity-text == &#39;err&#39; then (
    action(type=&amp;quot;omfile&amp;quot; file=&amp;quot;/var/log/haproxy/errors&amp;quot;)
)
/* notice */
if $programname contains &#39;haproxy&#39; and $syslogseverity &amp;lt;= 5 then (
    action(type=&amp;quot;omfile&amp;quot; file=&amp;quot;/var/log/haproxy/events&amp;quot;)
)

*.* /var/log/messages
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关于syslog的Severity level可以参考这里：&lt;a href=&#34;https://en.wikipedia.org/wiki/Syslog&#34;&gt;https://en.wikipedia.org/wiki/Syslog&lt;/a&gt;，
所以我们要查看的访问日志，应该就是/var/log/haproxy/traffic文件。
同时还添加了logrotate的配置进行日志分割和压缩，可查看容器内的&lt;strong&gt;/etc/logrotate.d/haproxy&lt;/strong&gt;文件。&lt;/p&gt;

&lt;p&gt;当前lb-service-haproxy版本默认是不开启日志收集的，需要自己手动开启，
我们都知道Haproxy开启日志需要在其global和defaults配置中添加log的配置。
Rancher LB是支持custom haproxy.cfg的，所以在Rancher中添加这两块配置就可以这样：&lt;br /&gt;
&lt;img src=&#34;https://ww2.sinaimg.cn/large/006tKfTcjw1fbgy5m62l3j30i80ecgn1.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
Rancher会自动合并这几条配置，最终在lb-service-haproxy容器内生成的配置就是这样：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;global
    log 127.0.0.1:8514 local0
    log 127.0.0.1:8514 local1 notice
    chroot /var/lib/haproxy
    daemon
    group haproxy
    maxconn 4096
.....
.....
defaults
    log global
    option tcplog
    errorfile 400 /etc/haproxy/errors/400.http
    errorfile 403 /etc/haproxy/errors/403.http
.....
.....
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;那么查看之前分析的日志目录文件，可以看到响应的访问日志信息：&lt;br /&gt;
&lt;img src=&#34;https://ww1.sinaimg.cn/large/006tKfTcjw1fbgya5nb4vj30l309vn1r.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;注意事项&#34;&gt;注意事项&lt;/h3&gt;

&lt;p&gt;日志虽然可以这样灵活的开启，但是使用时需要注意：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;日志文件放在容器内会不断增大，虽然有logrotate，但是扛不住日积月累。
临时测试用本地可以，但是长远看还是发送到外部syslog服务上比较好。&lt;/li&gt;
&lt;li&gt;lb-service-haproxy的早期版本是默认就开启了访问日志，v0.4.5版本后取消了这个默认配置，
所以你就得使用我上面所说的方式，早期的lb-service-haproxy会积存很多访问日志，
所以尽早升级到v0.4.5以上的版本。&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    
  </channel>
</rss>