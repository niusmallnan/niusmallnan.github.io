<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Post-rsses on Niusmallnan</title>
    <link>http://niusmallnan.com/post/index.xml</link>
    <description>Recent content in Post-rsses on Niusmallnan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <copyright>Niusmallnan</copyright>
    <lastBuildDate>Mon, 05 Dec 2016 11:14:19 +0800</lastBuildDate>
    <atom:link href="http://niusmallnan.com/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>go-machine-service访问Rancher API的授权机制分析</title>
      <link>http://niusmallnan.com/2016/12/05/go-machine-service-access-api-key</link>
      <pubDate>Mon, 05 Dec 2016 11:14:19 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/12/05/go-machine-service-access-api-key</guid>
      <description>&lt;p&gt;一段分析调用机制的旅程。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;深更半夜一位道上的朋友突然微信问我，Rancher Server内Cattle之外的服务如何调用Rancher API。
调用API本身没问题，但是Rancher API是需要授权访问的，
也就是说Rancher Server内其他服务如何授权访问API，这个机制是什么样的？
Rancher Server内有很多服务，如catalog、go-machine-service、websocket-proxy等等，
本文就以go-machine-service为例说明其授权机制。&lt;/p&gt;

&lt;h3 id=&#34;以go-machine-service为例&#34;&gt;以go-machine-service为例&lt;/h3&gt;

&lt;p&gt;首先我们要了解go-machine-service在做什么事，官方文档里很清楚的描述到，
它实现了physicalhost相关事件的handler，主要包括：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;physicalhost.create - Calls machine create &amp;hellip;.&lt;/li&gt;
&lt;li&gt;physicalhost.activate - Runs docker run rancher/agent &amp;hellip;&lt;/li&gt;
&lt;li&gt;physicalhost.delete|purge - Calls machine delete &amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;而若实现这些handler，需要和rancher events打交道，
我们认为rancher events也是rancher api的一部分，
所以访问rancher events也需要和API服务一样的鉴权信息。
go-machine-service在启动之时，很明确的读取了CATTLE相关变量：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006y8lVajw1fafqwxo6pyj30ir0ccdhx.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;而且rancher还给go-machine-service专门创建了相应的API Key，查看DB可以得知：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006y8lVajw1fafqxbv1wbj30f90iwgnb.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;进入rancher-server容器内部，查看go-machine-service进程的环境变量，
可以看到API Key的授权信息就在其中：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006y8lVajw1fafqxofpl1j30m805dmz0.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;那么go-machine-service的环境变量是如何设置的呢？查看Cattle源码，找到MachineLancher，
它是启动go-machine-service服务的具体实现，我们可以看到设置环境变量这一步：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006y8lVajw1fafqy3ywfbj30gg03tt9w.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;Cattle本身就是一个框架，Rancher Server内的其他服务都是通过Cattle的Proxy机制来转发请求的，
授权机制则是由Cattle统一提供出来，其他服务自身无需单独实现授权机制。
如果我们想对Rancher Server扩展，再增加一些其他服务，利用本文所描述的机制，就会非常方便。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>基于容器实现树莓派的动态域名绑定</title>
      <link>http://niusmallnan.com/2016/11/22/rpi-noip-with-docker</link>
      <pubDate>Tue, 22 Nov 2016 13:59:30 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/11/22/rpi-noip-with-docker</guid>
      <description>&lt;p&gt;家庭使用树莓派场景中，如何给树莓派绑定一个域名，让我们一扫运营商动态IP的困扰，
可以轻轻松松在外网使用域名访问。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;我们在家里使用树莓派时，如果在上面搭建了一些服务，有时会期待能在外网可以访问。
家庭宽带会给我们分配一个外网IP，可以通过这个IP在公网上访问树莓派上的服务。
但是运营商提供的这个IP是非静态的，很可能在凌晨的时候会被重新分配，
而且IP也不是很好记，所以通常我们都希望能有一个域名可以直接访问。
动态域名解析是个老话题，国内最早花生壳就做过，具体原理不必多说。&lt;/p&gt;

&lt;h3 id=&#34;我们的需求清单如下&#34;&gt;我们的需求清单如下：&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;一个动态域名解析的软件，最好是一定程度Free。&lt;/li&gt;
&lt;li&gt;域名解析的client端一定要是开源的（谁也不想被当肉鸡&amp;hellip;）。&lt;/li&gt;
&lt;li&gt;部署控制要非常简单。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;最终选型如下&#34;&gt;最终选型如下：&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;使用国外&lt;a href=&#34;https://www.noip.com/&#34;&gt;noip&lt;/a&gt;的服务，免费账户可以绑定三个免费域名，
每个域名30天有效，失效后需要手动添加回来。这种程度对于我这种玩家已经完全足够了。&lt;/li&gt;
&lt;li&gt;noip的client端支持各种平台，支持&lt;a href=&#34;https://www.noip.com/download?page=linux&#34;&gt;Linux&lt;/a&gt;，同时是开源。&lt;/li&gt;
&lt;li&gt;为了让部署变得更加简洁，决定使用Docker容器来部署。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;执行过程&#34;&gt;执行过程&lt;/h3&gt;

&lt;p&gt;首先，需要到&lt;a href=&#34;https://www.noip.com/&#34;&gt;noip&lt;/a&gt;上注册账户，并填写自己的域名，注册过程请自行体验，比如：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tNc79jw1fa0utbt10oj30ja04maap.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后，我们就需要在下载客户端，并在树莓派上进行编译，生成适合ARM运行的版本，编译只要执行make即可：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tNc79jw1fa0uxw14ulj30fk0a4wgz.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;初次执行需要先在client端注册账号信息，其实就是生成一下相关配置文件，再次执行就可以运行起来了。&lt;/p&gt;

&lt;p&gt;这么看来，对于很多用户来说还是太复杂，所以我们决定使用容器来简化这个过程。
可以把上面繁琐操作，全部放在容器中，为了精简程序的编译环境，
我们使用alpine-linux来进行编译，容器的Dockerfile如下：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tNc79jw1fa0uz7rv7oj30i304pmyk.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;接着，我们需要对树莓派的系统进行容器化，以便我们可以运行noip容器程序，此时可以有三种选择：
1. RancherOS
2. HypriotOS
3. Rasbian 基础上安装Docker&lt;/p&gt;

&lt;p&gt;前两个都是内置Docker Engine的，用起来比较方便，Rasbian需要自行安装Docker。我选择的OS是RancherOS。&lt;/p&gt;

&lt;h3 id=&#34;最简方式部署noip-只需三步&#34;&gt;最简方式部署noip，只需三步&lt;/h3&gt;

&lt;p&gt;在RancherOS上修改registry mirror后，可以加速镜像下载，然后拉取前面Dockerfile编译的镜像：&lt;br /&gt;
&lt;code&gt;$ docker pull hypriot/rpi-noip&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;注册noip client，按照提示添加用户名密码等信息：&lt;br /&gt;
&lt;code&gt;$ docker run -ti -v noip:/usr/local/etc/ hypriot/rpi-noip noip2 -C&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;运行noip clent：&lt;br /&gt;
&lt;code&gt;$ docker run -v noip:/usr/local/etc/ --restart=always hypriot/rpi-noip&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;登录No-IP的&lt;a href=&#34;https://my.noip.com/#!/dynamic-dns&#34;&gt;控制台&lt;/a&gt;可以看到 dns绑定情况：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tNc79jw1fa0v2h5umhj30km05rjs0.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在本地使用dig确认一下解析情况：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tNc79jw1fa0v2po6wfj30gy08yjt4.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;以上所有源码可以参考：&lt;a href=&#34;https://github.com/hypriot/rpi-noip&#34;&gt;https://github.com/hypriot/rpi-noip&lt;/a&gt;。
这样，我们就在树莓派上非常简单的实现了动态域名绑定。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>树莓派上的Docker集群管理</title>
      <link>http://niusmallnan.com/2016/11/07/docker-on-rpi</link>
      <pubDate>Mon, 07 Nov 2016 17:36:37 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/11/07/docker-on-rpi</guid>
      <description>&lt;p&gt;随着IOT市场的火热发展，Docker天然的轻量级以及帮助业务快速重构的特性，
将会在IOT领域迎来巨大发展潜力，甚至有可能会比它在云端的潜力更大。
本文将致力于构建一个利用RancherOS来管理运行在树莓派上的容器集群。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;目前业界主流基本都是在x86架构上使用Docker，除了因为Intel在服务器领域的绝对领导地位之外，
x86 CPU的确在性能上有着卓越的表现。但是近些年来，随着云计算的迅猛发展，
引来了数据中心的大规模建设，慢慢地大家对数据中心PUE尤其是CPU功耗有了更高的要求。
ARM CPU虽然性能不如x86，但是在功耗上绝对有着无法比拟的优势，
同时我们知道并不是所有的服务都有高性能的CPU需要。很多厂商在都对ARM服务器投入了研发资源，
但是效果上目前来看并不是太好，ARM处理器在服务器领域并没有如在移动端那样被快速接受，
主要是因为市场接受度及服务器市场的性能要求所致。&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/7853084cjw1fa26h172hoj20ib09dgmz.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;但是在物联网（IOT）领域，ARM处理器却是霸主级的地位，毕竟在这个领域功耗胜过一切。
那么我们可以想象，未来会占大市场的IOT设备中，会出现各种尺寸各种架构，
内置操作系统也不统一，没有通用程序打包标准，几乎每种设备程序的开发框架均不同，
IOT设备中程序部署升级回滚等操作不够灵活，等等这样那样的问题。&lt;/p&gt;

&lt;h3 id=&#34;分析与实践&#34;&gt;分析与实践&lt;/h3&gt;

&lt;p&gt;这些问题，我们可以借鉴X86时代的经验，用Docker容器来解决它们。
Docker能降低IOT应用管理的负载度，但是在物理设备和Docker之间，
我们还需要一个轻量级的操作系统。这个OS需要是完全可以定制的，
可以针对不同设备需求，裁剪或增加对应的程序模块，更小体积更少进程意味着更低的功耗。&lt;/p&gt;

&lt;p&gt;根据以上判断和需求，我经过了一番探索，最终选择了RancherOS。它本身的特性是：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;真正容器化的Linux操作系统极致精简，所有服务（包括系统服务）均运行在容器中，
可以以容器方式对其进行任意定制&lt;/li&gt;
&lt;li&gt;内置了Docker Engine，无需在安装系统后再进行Docker安装&lt;/li&gt;
&lt;li&gt;完全开源&lt;a href=&#34;https://github.com/rancher/os&#34;&gt;https://github.com/rancher/os&lt;/a&gt;，我们可以进行各种深度定制&lt;/li&gt;
&lt;li&gt;最最重要的，支持ARM&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在RancherOS的整体架构中，最底层毋庸置疑是Linux kernel，系统启动后的PID 1用system-docker代替，
由它来把udev、dhcp、console等系统服务启动，同时会启动user-docker，
用户运行的应用程序均跑在user-docker下。&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/7853084cjw1fa26kta7rkj20l109s3zt.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我们以树莓派为例，将RancherOS部署在其之上。这里需要提示的是，RancherOS每个版本release之时，
都会放出树莓派的支持版本，
比如本次分享使用的&lt;a href=&#34;https://github.com/rancher/os/releases/
download/v0.7.0/rancheros-raspberry-pi.zip&#34;&gt;v0.7.0版本&lt;/a&gt;。通过dd命令将RancherOS写到树莓派的SD卡上，
通电点亮树莓派。&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/7853084cjw1fa26m5gaz0j20aq0aawg4.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;查看PID 1是system-docker：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/7853084cjw1fa26mgfy51j20fn03ldh0.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;通过system-docker ps 查看启动的系统服务：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/7853084cjw1fa26mp3zphj20s603u0uw.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;正常来说，我们都得设置一下docker registry mirror，这样方便pull镜像。
RancherOS的配置，都是通过ros config命令来配置，比如设置user-docker的mirror：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo ros config set rancher.docker.extra_args [--registry-mirror,https://xxxxxxx]
$ sudo system-docker restart docker # 重启user-docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最终，可以看到：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/7853084cjw1fa26nqlvixj20wf02ydi4.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;RancherOS有一个我认为比较好的特性，就是支持很方便的对Docker Engine版本进行切换。
目前Docker迭代的速度并不慢，实际上很多程序不一定会兼容比较新的Engine，
Docker Engine版本的管理变得越来越重要。尤其是在测试环境中，
我们有时确实需要变换Docker Engine版本，来构建测试场景：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo ros engine list  #查看当前版本支持的engine有哪些
disabled docker-1.10.3
disabled docker-1.11.2
current  docker-1.12.1
$ sudo ros engine switch docker-1.11.2 #切换docker-1.11版本
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此外，如果对docker engine有更特殊的需求，还可以定制自己的版本，然后让system-docker来加载它。
只需将编译好的docker engine放到scrach镜像中即可：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/7853084cjw1fa26p7gcphj20fx06fmy1.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
这部分可以参考：&lt;a href=&#34;https://github.com/rancher/os-engines&#34;&gt;https://github.com/rancher/os-engines&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;另外，如果习惯了使用相应Linux发行版的命令行，
那么也可以加载对应的console镜像（当然如果考虑精简系统也可不必加载）：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/7853084cjw1fa26pvzryej20by03pdg5.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
此部分需要进行深度定制，可以参考：&lt;a href=&#34;https://github.com/rancher/os-images&#34;&gt;https://github.com/rancher/os-images&lt;/a&gt;&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/7853084cjw1fa26qd06b7j20ej0bm410.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;RancherOS更多酷炫的功能，可以访问官方的文档：&lt;a href=&#34;http://docs.rancher.com/os&#34;&gt;http://docs.rancher.com/os&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;RancherOS介绍完毕后，我们可以在单机树莓派上做容器管理了，喜欢命令行的当然最好，
喜欢UI管理的，推荐两款可以在树莓派上运行的管理程序。
&lt;strong&gt;portainer&lt;/strong&gt;&lt;a href=&#34;https://github.com/portainer/portainer&#34;&gt;https://github.com/portainer/portainer&lt;/a&gt;，
其有专门的arm镜像portainer/portainer:arm ，运行后：
&lt;code&gt;$ docker run --restart=always -d -p 9000:9000 --privileged 
    -v /var/run/docker.sock:/var/run/docker.sock 
    portainer/portainer:arm&lt;/code&gt;&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/7853084cjw1fa26s6xjj2j20oz0e2jtp.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;更加简约的 ui-for-docker &lt;a href=&#34;https://github.com/kevana/ui-for-docker&#34;&gt;https://github.com/kevana/ui-for-docker&lt;/a&gt; ，运行如下：
&lt;code&gt;$ docker run --restart=always -d -p 9000:9000 
    -v /var/run/docker.sock:/var/run/docker.sock 
    hypriot/rpi-dockerui&lt;/code&gt;&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/7853084cjw1fa26sxv8ofj20ot0g0abm.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;单机树莓派之后，我们就要考虑如何将多个树莓派组成Docker集群。
一提到Docker集群，我们就会考虑需要编排引擎的支持，
无非就是主流的Mesos、Kubernetes、Swarm，还有非主流的Cattle、Nomad之流。
那么在IOT场景下，我们最需要考虑的就是精简，所以我选择了新版的Swarm。
将RancherOS的Engine切换到1.12.3，然后构建Swarm集群。&lt;/p&gt;

&lt;p&gt;简单得执行swarm init和join后，我们就得到了一个树莓派Docker集群：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/7853084cjw1fa26tjxxlyj20fi01twey.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后我们可以快速执行一个小demo：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#swarmkit demo
$ docker service create --replicas 1 -p 80 --name app armhf/httpd
$ docker service scale app=2
$ docker service ps app
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://ww3.sinaimg.cn/large/7853084cjw1fa26u6875lj20jk01y751.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;更多ARM相关的Docker镜像，可以到这两个地方查找：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://hub.docker.com/r/armhf&#34;&gt;https://hub.docker.com/r/armhf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hub.docker.com/u/aarch64&#34;&gt;https://hub.docker.com/u/aarch64&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;RancherOS设计之初是为了构建一个运行Rancher的轻量级操作系统，
那么Rancher本身在ARM的支持上也在不断推进中，
相应的&lt;a href=&#34;https://github.com/rancher/rancher/pull/4704&#34;&gt;PR&lt;/a&gt;也有提交。
不过目前来看，对rancher-server的ARM化还是比较麻烦，对agent的节点支持ARM相对简单一些，
也就是说rancher-server仍然运行在x86架构上，而agent节点可以支持ARM和x86。&lt;/p&gt;

&lt;p&gt;Kubernetes的ARM支持在社区中也有很多人在做，比如：
&lt;a href=&#34;https://github.com/luxas/kubernetes-on-arm&#34;&gt;https://github.com/luxas/kubernetes-on-arm&lt;/a&gt;，来自社区的分享：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/7853084cjw1fa26w8ifrrj20i00h577r.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;秀一下，我的“家庭树莓派数据中心”：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/7853084cjw1fa26wjj274j20c50chmzj.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;最后，我要特别感谢RancherOS的开发者们，他们帮助我解决了很多问题；
另外还要特别感谢MBH树莓派社区的伙伴，提供了硬件设备，支持我的技术探索，
并提供了很多帮助。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>如何hack一下rancher k8s</title>
      <link>http://niusmallnan.com/2016/10/08/rancher-k8s-hacking</link>
      <pubDate>Sat, 08 Oct 2016 16:09:24 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/10/08/rancher-k8s-hacking</guid>
      <description>&lt;p&gt;Rancher可以轻松实现Kubernetes的部署，尽管默认的部署已经完全可用，
但是如果我们想修改部署的K8s版本，这时应当如何应对？
&lt;/p&gt;

&lt;h3 id=&#34;原理分析与执行&#34;&gt;原理分析与执行&lt;/h3&gt;

&lt;p&gt;在Rancher中，由于K8s是基于Cattle引擎来部署，所以在K8s在部署完成之后，
我们可以通过Link Graph来很清晰的看到整体的部署情况。&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tNc79jw1fa0ybyprq4j30mi07tdh0.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;既然基于Cattle引擎部署，也就是说需要两个compose文件，
k8s引擎的compose文件放在&lt;a href=&#34;https://github.com/rancher/rancher-catalog/tree/master/templates&#34;&gt;https://github.com/rancher/rancher-catalog/tree/master/templates&lt;/a&gt;下面，
这里面有两个相关目录kubernetes与k8s，k8s是Rancher1.2开始使用的，
而kubernetes则是Rancher1.2之后开始使用的。&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006tNc79jw1fa0ydzd1dgj30gt07qmyg.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;为了我们可以自己hack一下rancher k8s的部署，我们可以在github上fork一下rancher-catalog，
同时还需要修改一下Rancher中默认的catalog的repo地址，
这个可以在&lt;a href=&#34;http://rancher-server/v1/settings&#34;&gt;http://rancher-server/v1/settings&lt;/a&gt;页面下，
寻找名为 catalog.url 的配置项，然后进入编辑修改。比如我这里将library库的地址换成了自己的：
&lt;a href=&#34;https://github.com/niusmallnan/rancher-catalog.git&#34;&gt;https://github.com/niusmallnan/rancher-catalog.git&lt;/a&gt;&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/006tNc79jw1fa0yff7sbzj30h206imyy.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;此时，我们就可以修改了，找一个比较实用的场景。我们都知道k8s的pod都会依赖一个基础镜像，
这个镜像默认的地址是被GFW挡在墙外了，一般我们会把kubelet的启动参数调整一下，
以便重新指定这个镜像地址，比如指定到国内的镜像源上。&lt;br /&gt;
&lt;strong&gt;&amp;ndash;pod-infra-container-image=index.tenxcloud.com/google_containers/pause:2.0&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;如果我们要让rancher k8s部署时自动加上该参数，
可以直接修改私有rancher-catalog中的k8s compose文件。&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tNc79jw1fa0ygscal6j30gv0asjtp.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;修改之后稍等片刻（主要是为了让rancher-server更新到新的catalog compose文件），
添加一个k8s env并在其中添加host，k8s引擎就开始自动部署，
部署完毕后，我们可以看到Kubernetes Stack的compose文件，
已经有了&amp;ndash;pod-infra-container-image这个启动参数。&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/006tNc79jw1fa0yhg3alsj30gn09rtav.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
如此我们在添加pod时再也不用手动导入pod基础镜像了。&lt;/p&gt;

&lt;p&gt;在compose file中，部署k8s的基础镜像是rancher/k8s，这个镜像的Dockerfile在rancher维护的k8s分支中，
如在rancher-k8s 1.2.4分支中可以看到：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/006tNc79jw1fa0yi5rmplj30e007ogmr.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这样如果想对rancher-k8s发行版进行深度定制，就可以重新build相关镜像，通过rancher-compose来部署自己的发行版。&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;本文写于Rancher1.2行将发布之际，1.2版本是非常重大的更新，Rancher会支持部署原生的K8s版本，
同时CNI网络和Cloud Provider等都会以插件方式，用户可以自己定义，并且在UI上都会有很好的体现。
只要了解Rancher部署K8s的原理和过程，我们就可以定制非常适合自身使用的k8s，
通过Rancher来部署自定义的k8s，我们就可以很容易的扩展了k8s不擅长的UI、Catalog、
用户管理、审计日志维护等功能，这也是本文的目的。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Rancher HA部署实践</title>
      <link>http://niusmallnan.com/2016/09/23/rancher-ha-practice</link>
      <pubDate>Fri, 23 Sep 2016 17:12:27 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/09/23/rancher-ha-practice</guid>
      <description>&lt;p&gt;Rancher1.1版本HA结构部署实践记录
&lt;/p&gt;

&lt;h3 id=&#34;过程记录&#34;&gt;过程记录&lt;/h3&gt;

&lt;p&gt;以三节点为例，节点信息：
ubuntu(aufs)+docker1.11.2+rancher1.1.x
Haproxy+Mysql 放在同一节点 使用VM
Rancher HA Node 三个节点 使用baremetal（也可以使用VM，建议使用4核8G以上flavor）&lt;/p&gt;

&lt;p&gt;先部署Haproxy：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -d --name rancher-haproxy \
           -v /opt/conf/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro \
           -p 80:80 \
           haproxy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;文件 /opt/conf/haproxy.cfg参考：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;global
        log 127.0.0.1 local0
        log 127.0.0.1 local1 notice
        maxconn 4096
        maxpipes 1024
        daemon

defaults
        log     global
        mode    tcp
        option  tcplog
        option  dontlognull
        option  redispatch
        option http-server-close
        option forwardfor
        retries 3
        timeout connect 5000
        timeout client 50000
        timeout server 50000

frontend default_frontend
        bind *:80
        mode http

        default_backend rancher-ha-node

backend rancher-ha-node
        mode http
        server r-ha-1 xx.xx.xx.xx
        server r-ha-2 xx.xx.xx.xx
        server r-ha-3 xx.xx.xx.xx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建容器化的mysql：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run --name rancher-mysql \
    -v /opt/data/mysql:/var/lib/mysql \
    -e MYSQL_ROOT_PASSWORD=root \
    -p 3306:3306 \
    -d mysql:5.5

mysql -uroot -proot
CREATE DATABASE IF NOT EXISTS cattle COLLATE = &#39;utf8_general_ci&#39; CHARACTER SET = &#39;utf8&#39;;
GRANT ALL ON cattle.* TO &#39;cattle&#39;@&#39;%&#39; IDENTIFIED BY &#39;cattle&#39;;
GRANT ALL ON cattle.* TO &#39;cattle&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;cattle&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;正常运行rancher，配置并下载HA脚本，如果不想用HTTPS，
记得把&lt;strong&gt;CATTLE_HA_HOST_REGISTRATION_URL&lt;/strong&gt;的值换成HTTP的，再执行脚本。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>CNM macvlan实践</title>
      <link>http://niusmallnan.com/2016/09/19/docker-cnm-practice</link>
      <pubDate>Mon, 19 Sep 2016 17:22:00 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/09/19/docker-cnm-practice</guid>
      <description>&lt;p&gt;针对Dcoker原生网络模型CNM的一次实践记录，网络模式macvlan。&lt;/p&gt;

&lt;p&gt;
首先测试一下系统对bridge和namespace的支持情况：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# add the namespaces
ip netns add ns1
ip netns add ns2

# create the macvlan link attaching it to the parent host eno1
ip link add mv1 link eno1 type macvlan mode bridge
ip link add mv2 link eno1 type macvlan mode bridge

# move the new interface mv1/mv2 to the new namespace
ip link set mv1 netns ns1
ip link set mv2 netns ns2

# bring the two interfaces up
ip netns exec ns1 ip link set dev mv1 up
ip netns exec ns2 ip link set dev mv2 up

# set ip addresses
ip netns exec ns1 ifconfig mv1 192.168.1.50/24 up
ip netns exec ns2 ifconfig mv2 192.168.1.60/24 up

# show interface detail
ip netns exec ns1 ip a
ip netns exec ns2 ip a

# ping from one ns to another
ip netns exec ns1 ping -c 4 192.168.1.60

# cleanup
ip netns del ns1
ip netns del ns2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;给网卡创建一个vlan设备：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# create a new subinterface tied to dot1q vlan 1045
ip link add link eno1 name eno1.1045 type vlan id 1045

# assign an IP addr
ip a add 192.168.120.24/24 dev eno1.1045

# enable the new sub-interface
ip link set eno1.1045 up

# remove sub-interface
#ip link del eno1.1045
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建docker network：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker network create -d macvlan \
    --subnet=192.168.225.0/24 \
    --gateway=192.168.225.1 \
    -o parent=eno1.1045 macvlan1045
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建容器，测试网络连通性：
&lt;code&gt;docker run --net=macvlan1045 --rm --ip=192.168.225.24 -it alpine /bin/sh&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Rancher event机制及其实践指南</title>
      <link>http://niusmallnan.com/2016/08/25/rancher-envent</link>
      <pubDate>Thu, 25 Aug 2016 13:44:04 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/08/25/rancher-envent</guid>
      <description>&lt;p&gt;Rancher在v1.0版本为了Kubernetes的集成，开始扩展并逐步完善了Rancher event机制，
event机制可以完成技术架构上的解耦。那么Rancher都提供了哪些event？
Rancher内部是如何使用的？以及我们如何用event来增强一些定制服务？
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;我们的Rancher官方技术社区已经创立些许时日了，相信通过我们的线下meetup和线上布道工作，
很多朋友对Rancher的使用已经掌握得很纯熟了。一些高级用户开始真正把自己的业务进行微服务化并向Rancher迁移，
在迁移的过程中，由于业务本身的复杂性特殊性，
可能需要利用Rancher的一些高级特性甚至要对Rancher进行一定的扩展，
这就需要对Rancher的一些组件的实现机制有些许了解。&lt;/p&gt;

&lt;p&gt;本次分享就介绍一下Rancher的event机制，由于相关内容文档极其欠缺，本人也只是通过实践和代码阅读分析其原理，
如有谬误欢迎交流指正，小腩双手红包奉上！同时为保证收视率，本次分享会以原理+实践的方式进行。&lt;/p&gt;

&lt;h3 id=&#34;原理分析&#34;&gt;原理分析&lt;/h3&gt;

&lt;p&gt;在大规模系统架构中，event机制通常采用消息驱动 ，它对提升分布式架构的容错性灵活性有很大帮助，
同时也是各个组件之间解耦的利器。Rancher能够管理N多的agent同时又拆分出各种服务组件，
event机制是必不可少的。为实现event机制，通常我们会采用RabbitMQ、ActiveMQ、ZeroMQ等中间件来实现。
而Rancher则采用了基于websocket协议的一种非常轻量级的实现方式，
它的好处就是极大程度的精简了Rancher的部署，Rancher无需额外维护一个MQ集群，
毕竟websocket的消息收发实现是非常简单的，各种语言库均可以支持。&lt;/p&gt;

&lt;p&gt;这里我们会考虑一个问题，websocket毕竟不是真正工业级MQ的实现，消息不能持久化，
一旦某个event的处理出现问题，或者发生消息丢失，Rancher如何保证各个资源的原子性一致性？
Rancher中有一个processpool的概念，它可以看做一个所有event的执行池，
当API/UI/CLI有操作时，Rancher会把操作分解成多个event并放入processpool中。
比如删除一个容器时会把 compute.instance.remove 放入processpool中，
这个event会发送到对应的host agent上，agent处理完成后会发送reply给rancher-server。
如果在这个过程中，由于网络问题消息丢失，或者agent上执行出现问题，
rancher-server没有收到reply信息，cattle会把这个event重新放到processpool中再次重复上面的过程，
直到 compute.instance.remove 完成操作，这个容器的状态才会在DB中更新，
否则该容器状态会一直处于lock不能被其他服务更新。当然cattle不会把这些event不停的重复执行下去，
通常会设置一下TIMEOUT超出后便不再执行（有些资源没有TIMEOUT机制）。&lt;/p&gt;

&lt;p&gt;上面的表述，我们其实可以在UI上看到这个过程，RancherUI上的Processes的Running Tab页上就能实时得看到这些信息，
Processes 在排查一些Rancher的相关问题是非常有用的，大家可以养成 ”查问题先查Processes“的好习惯：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/7853084cjw1fa1zux9y7bj20ly091dh6.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
那么监听event的URL怎么设定呢？非常简单：&lt;br /&gt;
&lt;code&gt;ws://&amp;lt;rancher-server-ip&amp;gt;:8080/v1/projects/&amp;lt;projectId&amp;gt;/subscribe?eventNames=xxxx&lt;/code&gt;&lt;br /&gt;
除此之外还需要加上basic-auth的header信息：&lt;br /&gt;
&lt;code&gt;Authorization: Basic +base64encode(&amp;lt;cattle-access-key&amp;gt;:&amp;lt;cattle-secret-key&amp;gt;)&lt;/code&gt;&lt;br /&gt;
如果是Host上的agent组件，除此之外还需要添加agentId参数：&lt;br /&gt;
&lt;code&gt;ws://&amp;lt;rancher-server-ip&amp;gt;:8080/v1/projects/&amp;lt;projectId&amp;gt;/subscribe?eventNames=xxxx&amp;amp;agentId=xxxx&lt;/code&gt;&lt;br /&gt;
agentId 是注册Host时生成的，如果没有agentId参数，
任何有关无关的event都会发送到所有的Host agent上，这样就会发生类似“广播风暴”的效果。&lt;/p&gt;

&lt;p&gt;Host agent上运行很多组件，其中python-agent是负责接收和回执event信息的，
其运行日志可以在Host上的/var/log/rancher/agent.log文件中查看。
细心的朋友可能会有疑问，我们在添加Host时执行agent容器时并没有指定cattle-access-key和cattle-secret-key，
也就是说python-agent运行时如何获取这两个秘钥信息呢？&lt;/p&gt;

&lt;p&gt;其实Rancher有两种apikey：一种是我们熟知的在UI上手动创建的apikey；
另外一种就是agentApikey，它是系统级的，专门为agent设定，
添加Host时会先把agentApikey发送到Host上。在cattle的credential表中可以查询到相关信息：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/7853084cjw1fa1zxokn7dj20ea05774u.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;eventNames都定义了哪些呢？下面两个文件可以参考：&lt;br /&gt;
* &lt;a href=&#34;https://github.com/rancher/cattle/blob/master/code/iaas/events/src/main/java/io/cattle/platform/iaas/event/IaasEvents.java&#34;&gt;IaasEvents.java&lt;/a&gt;
* &lt;a href=&#34;https://github.com/rancher/cattle/blob/master/code/packaging/app-config/src/main/resources/META-INF/cattle/process/spring-process-context.xml&#34;&gt;系统级的event定义&lt;/a&gt;，详细到每种资源(host、volume、instance、stack、service等)的event定义。&lt;/p&gt;

&lt;p&gt;此外，我们在UI/CLI/API上的某个操作都会被分解成多个event来执行，
每个event信息都会被保存在mysql中，每个event执行成功后会设置成purged状态，
所以记录并不会真正删除，这就会导致相应的表（container_event表、service_event表、process_instance表）会无限膨胀下去。
Rancher为解决此问题提供了周期性清理机制：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;events.purge.after.seconds 可以清理container_event和service_event，每两周清理一次&lt;/li&gt;
&lt;li&gt;process_instance.purge.after.seconds可以清理process_instance，每天清理一次。&lt;br /&gt;
这两个配置我们都可以在&lt;a href=&#34;http://rancher-server-ip:8080/v1/settings&#34;&gt;http://rancher-server-ip:8080/v1/settings&lt;/a&gt;动态修改。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;实践操作&#34;&gt;实践操作&lt;/h3&gt;

&lt;p&gt;下面我们来实践一下，看看如何在程序中实现对Rancher event的监听。
Rancher提供了resource.change事件，这个事件是不用reply的，就是说不会影响Rancher系统的运行，
它是专门开放给开发者用来实现一些自己的定制功能，所以我们就以resource.change作为例子实践一下。&lt;/p&gt;

&lt;p&gt;Rancher的组件大部分都是基于Golang编写，所以我们也采用相同的语言。
为了能够快速实现这个程序，我们需要了解一些辅助快速开发的小工具。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Trash，Golang package管理的小工具，可以帮助我们定义依赖包的路径和版本，非常轻量且方便；&lt;/li&gt;
&lt;li&gt;Dapper，这个是基于容器实现Golang编译的工具，主要是可以帮助我们统一编译环境；&lt;/li&gt;
&lt;li&gt;Go-skel，它可以帮我们快速创建一个Rancher式的微服务程序，可以为我们省去很多的基本代码，
同时还集成了Trash和Dapper这两个实用的小工具。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;首先我们基于go-skel创建一个工程名为 scale-subscriber （名字很随意），执行过程需要耐心的等待：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/7853084cjw1fa2044jpu9j20im0ce78n.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;执行完毕后，我们把工程放到GOPATH中，开始添加相关的逻辑代码。
在这之前我们可以考虑添加一个healthcheck的服务端口，纵观Rancher所有的微服务组件，
基本上除了主程序之外都会添加 healthcheck port，这个主要是为了与Rancher中的healthcheck功能配合，
通过设定对这个端口的检查机制来保证微服务的可靠性。我们利用Golang的goroutine机制，
分别添加主服务和healthcheck服务：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/7853084cjw1fa2053h2k7j20i908u0uu.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;主服务的核心就是监听resource.change的信息，同时注册handler来获取event的payload信息，
从而可以定制扩展自己的逻辑，这里需要利用Rancher提供的
&lt;a href=&#34;https://github.com/rancher/event-subscriber&#34;&gt;event-subsriber&lt;/a&gt;库来快速实现。
如下图，在&lt;strong&gt;reventhandlers.NewResourceChangeHandler().Handler&lt;/strong&gt;中就可以实现自己的逻辑：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/7853084cjw1fa206sahqtj20ka07habz.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这里我们只是演示监听event的机制，所以我们就不做过多的业务逻辑处理，打印输出event信息即可。
然后在项目根目录下执行make，make会自动调用dapper，在bin目录下生成scale-subscriber，
我们执行scale-subscriber就可以监听resource.change的信息：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/7853084cjw1fa207fymwlj20u904gdhf.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这里我们可以看到分别启动了healthcheck功能和event listener。然后可以在UI上随意删除一个stack，
scale-subsciber就可以获取event信息：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/7853084cjw1fa207xudcej20ni07mgpy.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;附加小技巧&#34;&gt;附加小技巧&lt;/h3&gt;

&lt;p&gt;如要将其部署在Rancher中，那就可以将scale-subsciber执行程序打包放到镜像中，通过compose来启动。
但是我们知道scale-subsciber启动需要指定CATTLE_URL、CATTLE_ACCESS_KEY、CATTLE_SECRET_KEY，
正常的做法就是我们需要先生成好apikey，然后启动service的时候设置对应的环境变量。
这样做就会有弊端，就是apikey这种私密的信息不得不对外暴露，而且还要手动维护这些apikey，非常不方便。&lt;/p&gt;

&lt;p&gt;Rancher提供了一个非常方便的做法，就是在service中添加两个label，
&lt;strong&gt;io.rancher.container.create_agent: true&lt;/strong&gt;和&lt;strong&gt;io.rancher.container.agent.role: environment&lt;/strong&gt;，
设定这两个label后，Rancher引擎会自动创建apikey，并把相应的值设置到容器的ENV中，
只要你的程序通过系统环境变量来读取这些值，就会非常顺利的运行起来。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>扒一扒Rancher社区中的小工具</title>
      <link>http://niusmallnan.com/2016/08/15/rancher-smart-tools</link>
      <pubDate>Mon, 15 Aug 2016 14:16:42 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/08/15/rancher-smart-tools</guid>
      <description>&lt;p&gt;除了我们所熟知的Rancher &amp;amp; RancherOS，Rancher Labs的开发团队在实践中提炼了很多实用的小工具，
这些小工具虽然并不会左右Rancher发展的大局，但是在项目标准化和开发效率上给团队带来巨大的便捷。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;与Linux、OpenStack等成熟的技术社区相比，Rancher社区还是处于初级发展阶段，
一个技术社区的成败并不是单纯的代码贡献，而学习文档的数量和代码管理作业流程也是非常重要的。
如何让怀揣不同需求的工程师都能在社区中快速找到相应的解决方案，这就需要大家协同合作共同促进社区发展与完善。
除了我们所熟知的Rancher &amp;amp; RancherOS，Rancher Labs的开发团队在实践中提炼了很多实用的小工具，
这些小工具虽然并不会左右Rancher发展的大局，但是在项目标准化和开发效率上给团队带来巨大的便捷。
这次主要是带着大家一起来认识一下这些小工具。&lt;/p&gt;

&lt;h5 id=&#34;golang包管理工具-trash&#34;&gt;Golang包管理工具-Trash&lt;/h5&gt;

&lt;p&gt;项目地址：&lt;a href=&#34;https://github.com/rancher/trash&#34;&gt;https://github.com/rancher/trash&lt;/a&gt;&lt;br /&gt;
目前主流的编程语言 Python、Ruby、Java、Php 等已经把包管理的流程设计的犹如行云流水般流畅，
一般情况下开发者是不需要操心类库包依赖管理以及升级、备份、团队协作的。Golang在1.5版本开始，
官方开始引入包管理的设计，加了 vendor目录来支持本地包管理依赖，
但是需要特殊设置 GO15VENDOREXPERIMENT=1，在1.6时代这个特性已经是默认的了。
可是vendor并没有统一的版本号管理功能，只是额外提供了project内包的依赖路径。
于是Trash这个工具就应运而生了，Trash的使用非常简单，只需要有一份依赖包的描述文件即可。&lt;/p&gt;

&lt;p&gt;描述文件 trash.conf 支持两种格式，普通方式和YAML方式，
可以直接在其中描述依赖库的远程地址、版本号等，一个简单的例子（我这里使用普通格式）：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/7853084cjw1fa20qdpd40j20c302t0t0.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后在根目录执行trash，即可获得相关版本的依赖包，非常轻量级，非常简洁：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/7853084cjw1fa20qtj6kcj20av06cjrr.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;golang编译工具-dapper&#34;&gt;Golang编译工具-Dapper&lt;/h5&gt;

&lt;p&gt;项目地址：&lt;a href=&#34;https://github.com/rancher/dapper&#34;&gt;https://github.com/rancher/dapper&lt;/a&gt;&lt;br /&gt;
我们在编译golang执行程序的时候，因为涉及到协作开发，这就会碰到一个问题，
就是如何保证编译环境的一致性。环境一致性最好的办法就是使用容器技术，
Dapper就是利用Docker build镜像的过程中可以执行各种命令生成容器的原理。
只需在项目的根目录下创建 Dockerfile.dapper 文件，这是一个参考Dockerfile标准的文件，
执行dapper命令即可按照约定规则生成最终的执行程序，通过这种方式统一的编译环境。&lt;/p&gt;

&lt;p&gt;几乎所有的Rancher项目都是基于Dapper来编译的，随意打开一个项目比如
&lt;a href=&#34;https://github.com/rancher/rancher-dns&#34;&gt;rancher-dns&lt;/a&gt;就可以看到Dockerfile.dapper文件：&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/7853084cjw1fa20thy5rhj20j809ojtr.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;DAPPER_SOURCE 指定了容器内的源码路径&lt;br /&gt;
DAPPER_OUTPUT 指定了编译后的输出路径，bin dist 目录会自动在项目根目录下创建&lt;br /&gt;
DAPPER_DOCKER_SOCKET 设置为True相当于&lt;code&gt;docker run -v /var/run/docker.sock:/var/run/docker.sock ...&lt;/code&gt;&lt;br /&gt;
DAPPER_ENV 相当于&lt;code&gt;docker run -e TAG -e REPO ...&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;有一点需要注意的是，目前Dapper装载源码有两种方式bind和cp，bind就是直接mount本地的源码路径，
但是如果使用remote docker daemon方式那就得使用cp模式了。&lt;/p&gt;

&lt;h5 id=&#34;golang项目标准化工具-go-skel&#34;&gt;Golang项目标准化工具 Go-skel&lt;/h5&gt;

&lt;p&gt;项目地址：&lt;a href=&#34;https://github.com/rancher/go-skel&#34;&gt;https://github.com/rancher/go-skel&lt;/a&gt;&lt;br /&gt;
介绍了包管理工具和打包编译工具，如果我们在创建一个golang项目时能把这两个工具整合起来一起使用，
那就太赞了。go-skel就是提供了这样一个便利，我们直接来demo一下。&lt;/p&gt;

&lt;p&gt;clone一份go-skel的源码，创建一个rancher-tour（./skel.sh rancher-tour）的项目：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/7853084cjw1fa20wzh1pxj20jt09v0vf.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;执行完毕后，会创建一个标准的项目，包含了dapper和trash这两个工具，
同时定义了一份Makefile，我们可以通过make命令来简化操作：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/7853084cjw1fa20xj5jd3j20cw0art9r.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;比如我们要执行一个ci操作，那就可以直接运行 make ci，自动运行单元测试，
并在bin目录下生成最终的可执行程序：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/7853084cjw1fa20xw28tzj20gz05kmyl.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;标准项目已经创建了一些初始化代码，集成了 github.com/urfave/cli ，所以我们可以执行执行rancher-tour：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/7853084cjw1fa20y6wzdjj20kq08xgmw.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;微服务辅助小工具-giddyup&#34;&gt;微服务辅助小工具 Giddyup&lt;/h5&gt;

&lt;p&gt;项目地址：&lt;a href=&#34;https://github.com/cloudnautique/giddyup&#34;&gt;https://github.com/cloudnautique/giddyup&lt;/a&gt;&lt;br /&gt;
一个传统的服务在容器化的过程中，通常我们会将其拆分成多个微服务，
充分展现每个容器只做好一件事情这样的哲学。那么就会出现我们在Rancher中看到的sidekick容器，
数据卷容器，专门负责更新配置信息的容器等等。实际应用中我们会遇到一些问题，
比如这些微服务容器的启动是无序的，无序启动会导致微服务之间可能出现连接失败，
进而导致整个服务不可用；再比如我们如何来判定依赖的微服务已经正常启动，
这可能需要一个health check的服务端口。giddyup就是简化这种微服务检查工作的利器，它都能做些什么呢：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Get connection strings from DNS or Rancher Metadata.&lt;/li&gt;
&lt;li&gt;Determine if your container is the leader in the service.&lt;/li&gt;
&lt;li&gt;Proxy traffic to the leader.&lt;/li&gt;
&lt;li&gt;Wait for service to have the desired scale.&lt;/li&gt;
&lt;li&gt;Get the scale of the service.&lt;/li&gt;
&lt;li&gt;Get Managed-IP of the container (/self/container/primary_ip).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;我们还是创建stack并在其中创建service来体验一下giddyup的部分功能，
service中包含两个容器分别为主容器main和sidekick容器conf，他们共享网络栈：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/7853084cjw1fa210w5l9pj20c807xt9a.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我们可以在conf内启动giddyup health，会启动一个监听1620端口的http服务，服务路径是/ping：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/7853084cjw1fa2115foy9j20a703h3yl.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;此时我们可以在其他服务的容器内查看这个服务 health port的健康状态，
通过giddyup ip stringify获取服务地址，使用giddyup probe可以查看相关状态：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/7853084cjw1fa211iwkz2j20ql03jgmj.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到probe返回了OK的信息，说明giddy/main的health port是正常的，
如果我们把giddyup health的端口停掉，giddyup probe会如何？&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/7853084cjw1fa211ulol7j20mw06kn1u.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;除了开启health port功能外，还可以sleep等待service内所有容器都启动，
比如刚才的service，我在main上做wait操作，同时在UI上对service扩容，
可以看到为了等待完成扩容差不多sleep了3s：&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/7853084cjw1fa212n0tykj20ep031mxg.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;更多功能可以去看一看giddyup项目的README文档，另外也可以看看catalog中的一些项目是怎么使用giddyup的，
参考：&lt;a href=&#34;https://github.com/rancher/catalog-dockerfiles&#34;&gt;https://github.com/rancher/catalog-dockerfiles&lt;/a&gt;&lt;/p&gt;

&lt;h5 id=&#34;rancher-cli-工具&#34;&gt;Rancher CLI 工具&lt;/h5&gt;

&lt;p&gt;项目地址：&lt;a href=&#34;https://github.com/rancher/cli&#34;&gt;https://github.com/rancher/cli&lt;/a&gt;&lt;br /&gt;
现在我们管理Rancher的方式包括UI、API方式，为了能够和其他工具更好的融合Rancher开发了CLI工具，
可以非常轻量级的嵌入到其他工具中。CLI将会在Rancher 1.2-pre2版本中正式放出，
兼容性上目前来看没有支持老版本的计划，所以在老版本上出现各种问题也是正常的。&lt;/p&gt;

&lt;p&gt;直接在&lt;a href=&#34;https://github.com/rancher/cli/releases&#34;&gt;https://github.com/rancher/cli/releases&lt;/a&gt;下载最新版本即可试用，
将rancher cli放到PATH中，正式使用前需要初始化：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/7853084cjw1fa214oqzn5j20hy07a405.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后便可以执行各种犀利的操作，比如针对某个service进行scale操作，
这比之前需要用rancher-compose cli要简洁的多：&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/7853084cjw1fa2155xxeaj20ps05ggng.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;再比如可以实时监听rancher events，可以即时查看Rancher中正在发生的各种任务，
对协助排查问题，分析系统运行情况都非常有用：&lt;br /&gt;
&lt;img src=&#34;http://ww4.sinaimg.cn/large/7853084cjw1fa215gzjcrj20dr077myp.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;后语&#34;&gt;后语&lt;/h4&gt;

&lt;p&gt;小工具中内含着大智慧，工具文化是工程师Team高效协作的重要标志，
高质量的工具能让项目开发有事半功倍之效，其背后也蕴藏着深厚的团队文化理念，
就是不计项目KPI利用个人业余时间为团队做贡献的和谐氛围。
其实国内很多互联网公司都是有专门设立工具开发工程师的岗位，
对工具带来的生产效率提升，其重视程度不言而喻！&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Kubernetes in Rancher1.0 架构分析</title>
      <link>http://niusmallnan.com/2016/07/28/k8s-in-rancher-1-0</link>
      <pubDate>Thu, 28 Jul 2016 14:49:47 +0800</pubDate>
      
      <guid>http://niusmallnan.com/2016/07/28/k8s-in-rancher-1-0</guid>
      <description>&lt;p&gt;该主题是本人于Rancher k8s 北京meetup进行的一次线下分享，应Rancher China官方之邀，
重新梳理成文字版本，便于大家阅读传播，如有问题纰漏或任何不妥之处随时联系牛小腩（niusmallnan），
我会以最快速度更正。
&lt;/p&gt;

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;在Rancher 1.0版本开始，Rancher逐步增加了Kubernetes、Swarm、Mesos等多编排引擎的支持，
很多朋友就此产生了疑惑，诸如Cattle引擎和这几个之间到底什么关系？
每种引擎是如何支持的？自家的业务环境如何选型？我们将逐步揭开这些神秘面纱，
了解基础架构才能在遇到问题时进行有效的分析，进而准确的定位问题并解决问题，
因为没有一种生产环境是完全可靠的。基于这个背景下，这次我们首先向大家介绍kubernetes in Rancher的架构。&lt;/p&gt;

&lt;h3 id=&#34;分析&#34;&gt;分析&lt;/h3&gt;

&lt;p&gt;从现在Rancher的发展节奏来看，Cattle引擎已经被定义成Rancher的基础设施引擎，
而Rancher的基础设施服务都包括哪些呢？如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Networking，rancher的统一网络服务，由rancher-net组件提供&lt;/li&gt;
&lt;li&gt;Load Balancer，rancher的负载均衡服务，目前来看套路基本上是基于Haproxy来构建&lt;/li&gt;
&lt;li&gt;DNS Service，rancher的dns服务，主要是为了提供服务发现能力，由rancher-dns组件来提供&lt;/li&gt;
&lt;li&gt;Metadata Service，rancher的元数据服务，metadata是我们通过compose编排应用时的利器，
可以很灵活的像service中注入特定信息&lt;/li&gt;
&lt;li&gt;Persistent Storage Service，持久化存储服务目前是由convoy来提供，
而对于真正的后端存储的实现rancher还有longhorn没有完全放出&lt;/li&gt;
&lt;li&gt;Audit Logging，审计日志服务是企业场景中比较重要的一个属性，
目前是集成在cattle内部没有被完全分离出来&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所以Rancher在接入任何一种编排引擎，最终都会把基础设施服务整合到该引擎中，
Kubernetes in Rancher的做法正是如此。&lt;/p&gt;

&lt;p&gt;Kubernetes各个组件的角色可以归为三类即Master、Minion、Etcd，
Master主要是kube-apiserver、kube-scheduler、kube-controller-manager，
Minion主要是kubelet和kube-proxy。Rancher为了融合k8s的管控功能，
又在Master中添加了kuberctrld、ingress-controller、kubernetes-agent三个服务来打通Rancher和K8s，
同时每个node上都会依赖Rancher提供的rancher-dns、rancher-metadata、rancher-net这些基础设施服务。&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/7853084cjw1fa21xnsfbrj20p40c0wgm.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;由于K8s是基于Cattle引擎来部署，所以在K8s在部署完成之后，
我们可以通过Link Graph来很清晰的看到整体的部署情况。&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/7853084cjw1fa21y87258j20mi07tdh0.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
整个服务基于Cattle引擎的rancher-compose构建，新增节点后自动添加kubelet和kube-proxy服务
（此处利用了Global Label的特性），各个组件都添加了health-check机制，
保证一定程度的高可用。考虑到Etcd最低1个最多3个节点，
单台agent host就可以部署K8s，三节点agent host则更合理些。&lt;/p&gt;

&lt;p&gt;K8s集群完成部署后，我们就可以在其中添加各种应用服务，
目前Rancher支持管理K8s的service、pod、replication-controller等，
我们可以用一张图来形象得描述一下应用视图结构。&lt;br /&gt;
&lt;img src=&#34;http://ww1.sinaimg.cn/large/7853084cjw1fa21z5ellcj20qj0cuq5u.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
rancher-net组件会给每个pod分配一个ip，rancher-dns则替代了K8s的Skydns来实现服务发现，
在pod的容器内部依然可以访问rancher-metadata服务来获取元数据信息。除了这三个基础服务外，
我们之前提到的kuberctrld、ingress-controller、kubernetes-agent也在其中扮演者重要角色。&lt;/p&gt;

&lt;p&gt;无论是K8s还是Rancher，其中一些抽象对象（如rancher的stack/service，或者K8s的serivice/pod）
在属性更新时都会有events产生，在任何服务入口来更改这些抽象对象都会有events产生，
所以要保证Rancher和k8s能够互相感知各自对象的更新，那么kubernetes-agent就应运而生了。&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/7853084cjw1fa22166xnyj20nt0d60ut.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
诸如K8s的namespaces、services、replicationcontrollers、pods等对象的信息变更会及时通知给Rancher，
而Cattle管理的Host资源出现信息变更（诸如host label的变动）也会通知给K8s。&lt;/p&gt;

&lt;p&gt;简单得说kubernetes-agent是为了维护Rancher和K8s之间的对象一致性，
而真正要通过Rancher来创建K8s中的service或者pod之类的对象，
还需要另外一个服务来实现，它就是kubectrld，直观的讲它就是包装了kubectrl，
实现了其中kubectl create/apply/get 等功能。&lt;br /&gt;
&lt;img src=&#34;http://ww3.sinaimg.cn/large/7853084cjw1fa221z612jj20o80bhq4q.jpg&#34; alt=&#34;&#34; /&gt;&lt;br /&gt;
所有的K8s对象创建请求都会走cattle引擎，cattle会把请求代理到kubectrld启动的一个api服务。
除此之外，还会监听rancher events来辅助实现相关对象的CRUD。&lt;/p&gt;

&lt;p&gt;K8s上创建的service如需对外暴露访问，那么必然会用到LoadBalancer Type和Ingress kind，
注意K8s概念下的LoadBalancer和Ingress略有不同，LoadBalancer的功能主要关注在L4支持http/tcp，
而Ingrees则是要实现L7的负载均衡且只能支持http。K8s 的LoadBalancer需要在K8s中实现一个Cloud Provider，
目前只有GCE，而Rancher则维护了自己的K8s版本在其中提供了Rancher Cloud Provider。
对于Ingress则是提供了Ingress-controller组件，它实现了K8s的ingress框架，
可以获取ingress的创建信息并执行相应的接口。当然最终这两者都会调用cattle api来创建Rancher的负载均衡，
且都是通过Haproxy完成负责均衡功能。&lt;br /&gt;
&lt;img src=&#34;http://ww2.sinaimg.cn/large/7853084cjw1fa22370rzfj20hv0b6gn5.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;后语&#34;&gt;后语&lt;/h3&gt;

&lt;p&gt;以目前K8s社区的火热势头，Rancher应该会持续跟进并不断更新功能优化架构，
待到Rancher1.2发布之后，CNI的支持会是一个里程碑，
到那时Kubernetes in Rancher也会更加成熟，一起向着最好用Kubernetes发行版大踏步的前进。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>